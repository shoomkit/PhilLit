## AI in Scientific Practice: A Model for Epistemic Partnerships?

Scientific practice provides a crucial test case for AI-human epistemic partnerships, offering both the strongest norms for rational belief revision and the most developed institutional frameworks for epistemic responsibility. Yet recent philosophy of science reveals deep puzzles about whether opaque AI systems can produce genuine scientific knowledge—puzzles that illuminate broader questions about AI's epistemic role.

### The Opacity Problem

Duede (2023) identifies a fundamental disconnect: while philosophers express pessimism about AI in science due to epistemic opacity, scientists exhibit optimism driven by practical success. Deep learning models are "epistemically opaque" in Humphreys' sense—"impossible for a scientist to know all factors epistemically relevant to licensing claims" derived from AI analysis (Duede 2023, 1089). Yet Duede argues this pessimism stems from "a failure to examine how AI is actually used in science," where deep learning serves as "part of a wider process of discovery" rather than standalone knowledge source (Duede 2023, 1090).

This raises the central question: can opaque AI systems produce genuine scientific knowledge? Zakharova's (2025) analysis of AlphaFold directly confronts this puzzle. Drawing on Alexander Bird's functionalist account of scientific knowledge as "irreducibly social," Zakharova argues that "the implicit principles used by AlphaFold satisfy the conditions for scientific knowledge, despite their opacity," provided the system is "properly functionally integrated into the collective scientific enterprise" (Zakharova 2025, 8). On this view, "scientific knowledge can be strongly opaque to humans, as long as it is properly functionally integrated" into practices of validation, contestation, and application (Zakharova 2025, 15).

Yet Philosophy & Technology (2024) offers a competing view: AI systems in medicine "are often reliable and accurate but opaque," such that "practitioners' inability to check outputs undermines knowledge" even when AI proves reliable (Philosophy & Technology 2024, 1). When scientists cannot trace AI's reasoning, their justified belief in AI outputs may fail to constitute knowledge despite truth and reliability. This suggests epistemic cost to opacity independent of reliability—a cost borne by the epistemic agent who cannot provide warrant for beliefs derived from opaque sources.

### Trust Without Agency

Koskinen (2023) identifies a structural problem: "Scientists are now epistemically dependent on AI applications that are not agents, and therefore not appropriate candidates for trust" (Koskinen 2023, 2). We lack "satisfactory way to reconcile the practices of AI-based science with the idea that relationships of trust are indispensable in contemporary science" (Koskinen 2023, 1). Trust traditionally requires moral answerability—capacity to respond to questions, acknowledge errors, and bear epistemic responsibility. AI systems lack these features yet occupy positions of epistemic dependence previously reserved for trusted colleagues.

Synthese (2025) offers a dissenting view: epistemic dependence on opaque AI "poses no novel problems for social epistemology" since science already involves substantial epistemic dependence on instruments, techniques, and specialist knowledge one cannot fully evaluate (Synthese 2025). The question becomes whether AI opacity differs in kind or merely degree from existing forms of epistemic dependence in scientific practice.

### Implications for Democratic Epistemology

Cruz-Aguilar (2025) situates these debates within broader transformation: AI integration into scientific practice constitutes "not merely a methodological shift but a profound transformation in the epistemic structure of science" (Cruz-Aguilar 2025, 1). AI "disrupts classical epistemological paradigms—empiricism, falsificationism, Kuhnian paradigm shifts, and social epistemology" while necessitating "novel frameworks for understanding knowledge production in the age of machine cognition" (Cruz-Aguilar 2025, 3).

Scientific practice thus offers no easy resolution to questions about AI epistemic partnerships. If scientists—with strong norms for rational belief revision, institutional accountability structures, and technical expertise—struggle to determine when AI-mediated belief formation preserves epistemic agency, the challenge multiplies for democratic publics. If scientific knowledge can accommodate opacity through functional integration, perhaps public knowledge can as well. Alternatively, if opacity undermines scientific knowledge despite reliability, this suggests even greater concern for AI's role in public belief formation where institutional safeguards and epistemic norms prove weaker.

The ambivalence in philosophy of science regarding AI epistemic partnerships thus mirrors and illuminates broader debates about AI-mediated belief revision. Scientific practice provides neither straightforward model for rational AI persuasion nor clear warning against epistemic displacement, but rather reveals the complexity and stakes of these questions across domains.

---

**Word Count**: 706 words
