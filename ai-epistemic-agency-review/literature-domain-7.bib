@comment{===================================================================}
@comment{DOMAIN 7: AI IN SCIENTIFIC PRACTICE}
@comment{===================================================================}
@comment{
Domain Focus: This domain examines how AI systems affect scientific
belief formation and epistemic practices. Key themes include: AI as
epistemic tool vs epistemic authority in research, opacity and
interpretability challenges for scientific knowledge, epistemic trust
in AI-assisted science, case studies of AI-driven scientific discovery,
and whether opaque AI systems can produce genuine scientific knowledge.

Search Strategy: Google Scholar (AI + scientific practice, AI +
discovery), Philosophy of Science, Studies in History and Philosophy of
Science, Biology & Philosophy, Synthese, Social Epistemology,
PhilSci-Archive

Key Questions:
- How do AI systems affect scientific belief formation?
- Is AI an epistemic tool or epistemic authority in research?
- Can opaque AI systems produce genuine scientific knowledge?
- What role does epistemic trust play in AI-assisted science?
- How do case studies (AlphaFold, etc.) illuminate epistemic issues?
}
@comment{===================================================================}

@article{Duede2023Opacity,
  author = {Duede, Eamon},
  title = {Deep Learning Opacity in Scientific Discovery},
  journal = {Philosophy of Science},
  year = {2023},
  volume = {90},
  number = {5},
  pages = {1089--1099},
  doi = {10.1017/psa.2023.8},
  url = {https://www.cambridge.org/core/journals/philosophy-of-science/article/deep-learning-opacity-in-scientific-discovery/C46306D902A7AC87FD192D996639784A},
  note = {Argues disconnect between philosophical pessimism and scientific optimism about AI is driven by failure to examine how AI is actually used in science. Philosophers must examine role played by deep learning as part of wider process of discovery. Deep learning models are epistemically opaque following Humphreys: impossible for scientist to know all factors epistemically relevant to licensing claims. Open access. Also arXiv:2206.00520.}
}

@article{Zakharova2025AlphaFold,
  author = {Zakharova, Daria},
  title = {The Epistemology of AI-driven Science: The Case of AlphaFold},
  journal = {PhilSci-Archive preprint},
  year = {2025},
  url = {https://philsci-archive.pitt.edu/26659/1/AlphaFold_preprint%20(2025).pdf},
  note = {Examines critical questions about nature and production of scientific knowledge in age of AI-driven science. Asks: Are predictions alone, even highly reliable ones, sufficient to constitute scientific knowledge? Draws on Alexander Bird's functionalist account of scientific knowledge as irreducibly social. Argues implicit principles used by AlphaFold satisfy conditions for scientific knowledge, despite opacity. Scientific knowledge can be strongly opaque to humans, as long as properly functionally integrated into collective scientific enterprise.}
}

@article{Koskinen2023AIScience,
  author = {Koskinen, Inkeri},
  title = {We Have No Satisfactory Social Epistemology of AI-Based Science},
  journal = {Social Epistemology},
  year = {2023},
  doi = {10.1080/02691728.2023.2286253},
  url = {https://www.tandfonline.com/doi/full/10.1080/02691728.2023.2286253},
  note = {Scientists are now epistemically dependent on AI applications that are not agents, and therefore not appropriate candidates for trust. We currently have no satisfactory way to reconcile practices of AI-based science with idea that relationships of trust are indispensable in contemporary science. Major challenge for social epistemology of science.}
}

@article{PhilTech2025Cost,
  author = {Authors, Multiple},
  title = {The Epistemic Cost of Opacity: How the Use of Artificial Intelligence Undermines the Knowledge of Medical Doctors in High-Stakes Contexts},
  journal = {Philosophy \& Technology},
  year = {2024},
  doi = {10.1007/s13347-024-00834-9},
  url = {https://link.springer.com/article/10.1007/s13347-024-00834-9},
  note = {AI systems used in medicine are often reliable and accurate but opaque. Argues practitioners' inability to check outputs undermines knowledge. When I cannot check AI's reasoning, my justified belief doesn't constitute knowledge even if true and reliable. Epistemic cost of relying on opaque systems in high-stakes scientific/medical contexts. Published 2024 but fits 2025 literature.}
}

@article{Synthese2025Oracles,
  title = {Of Opaque Oracles: Epistemic Dependence on AI in Science Poses No Novel Problems for Social Epistemology},
  journal = {Synthese},
  year = {2025},
  doi = {10.1007/s11229-025-04930-x},
  url = {https://link.springer.com/article/10.1007/s11229-025-04930-x},
  note = {Contrasting view to Koskinen. Argues deep neural networks are epistemically opaque but this poses no special problems for thick normative trust between researchers. Counter-argument that AI opacity doesn't create fundamentally new epistemic challenges. Debate within social epistemology of AI science.}
}

@article{CruzAguilar2025Revolution,
  author = {Cruz-Aguilar, M.A.},
  title = {The Epistemic Revolution of AI: Reconfiguring the Foundations of Scientific Knowledge},
  journal = {AI \& Society},
  year = {2025},
  doi = {10.1007/s00146-025-02658-3},
  url = {https://link.springer.com/article/10.1007/s00146-025-02658-3},
  note = {Critically examines how AI disrupts classical epistemological paradigms - empiricism, falsificationism, Kuhnian paradigm shifts, and social epistemology - while necessitating novel frameworks for understanding knowledge production in age of machine cognition. Proposes three constructive pathways: pragmatic computational empiricism, adversarial epistemology, and democratic AI epistemology. AI integration into scientific practice is 'not merely methodological shift but profound transformation in epistemic structure of science'.}
}

@article{Sage2025Opacity,
  author = {Authors, Multiple},
  title = {It's Not a Bug, It's a Feature: How AI Experts and Data Scientists Account for the Opacity of Algorithms},
  journal = {Social Studies of Science (SAGE)},
  year = {2025},
  doi = {10.1177/03063127251364509},
  url = {https://journals.sagepub.com/doi/10.1177/03063127251364509},
  note = {Sociological study of how AI experts and data scientists themselves account for algorithmic opacity. Shows practitioners don't view opacity as bug but as feature. Important for understanding how scientific community negotiates epistemic challenges of opaque AI. Empirical study of scientific practice.}
}

@misc{ArXiv2023Taxonomy,
  title = {Sources of Opacity in Computer Systems: Towards a Comprehensive Taxonomy},
  journal = {arXiv preprint},
  year = {2023},
  note = {arXiv:2307.14232},
  url = {https://arxiv.org/pdf/2307.14232},
  note = {Develops comprehensive taxonomy of different sources and types of opacity in computational systems. Essential conceptual framework for analyzing epistemic challenges of AI in science. Distinguishes technical, cognitive, social, and institutional sources of opacity.}
}

@article{PhilPapers2024Ethics,
  author = {Mussgnug, Alexander Martin},
  title = {Ethics and Epistemology of AI in Science: Studies of Machine Learning Poverty Prediction},
  journal = {PhilPapers},
  year = {2024},
  url = {https://philpapers.org/rec/MUSEAE},
  note = {Case study approach examining ethics and epistemology of AI in poverty prediction research. Shows how epistemic and ethical issues are intertwined in AI-assisted social science. Concrete example of epistemic challenges in computational social science.}
}

@article{PubMed2023Technology,
  title = {AI as an Epistemic Technology},
  journal = {PubMed},
  year = {2023},
  url = {https://pubmed.ncbi.nlm.nih.gov/37603120/},
  note = {Conceptualizes AI as epistemic technology - technology specifically designed for knowledge production, validation, and dissemination. Distinguishes AI's epistemic role from other technological tools. Framework for understanding AI's unique position in scientific practice as knowledge-producing artifact.}
}

@article{PhilArchive2024Alvarado,
  author = {Alvarado, Ramón},
  title = {AI as an Epistemic Technology},
  journal = {PhilArchive},
  year = {2024},
  url = {https://philpapers.org/rec/ALVAAA-5},
  note = {Philosophical analysis of AI as epistemic technology. Explores how AI functions distinctively as technology for epistemic purposes, not just computational purposes. Examines normative implications of treating AI as epistemic rather than merely computational tool.}
}

@incollection{SEP2024BeliefRevision,
  title = {Logic of Belief Revision},
  booktitle = {The Stanford Encyclopedia of Philosophy},
  editor = {Zalta, Edward N. and Nodelman, Uri},
  publisher = {Metaphysics Research Lab, Stanford University},
  year = {2024},
  url = {https://plato.stanford.edu/entries/logic-belief-revision/},
  note = {Foundational SEP entry on belief revision logic (AGM framework: Alchourrón, Gärdenfors, and Makinson 1985). Belief sets updated via contraction and revision operators to eliminate inconsistency while preserving maximal information content. Framework increasingly applied to AI systems and scientific belief revision. Essential background for understanding formal epistemology of AI-assisted science.}
}
