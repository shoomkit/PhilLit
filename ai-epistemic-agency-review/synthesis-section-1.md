# AI and Epistemic Agency: A Literature Review

## Introduction: Framing the Epistemic Challenge

The rapid deployment of large language models (LLMs) and AI chatbots raises a novel epistemic question: how do these systems affect human epistemic agency when they persuade us to revise our beliefs not through manipulation, but by engaging our rational faculties? While substantial philosophical work addresses AI's potential for deception, bias, and manipulation, considerably less attention has been paid to a more subtle phenomenon—AI systems that prompt belief revision by convincing us that we know less about a topic than we initially thought. This form of rationally-mediated belief change operates at the population level, potentially reshaping public knowledge and democratic discourse in ways that traditional epistemology has yet to fully conceptualize.

Recent philosophical scholarship identifies a central tension in AI-mediated belief formation. Coeckelbergh (2025) argues that artificial intelligence and data science, while offering more information, risk influencing "the formation and revision of our beliefs in ways that diminish our epistemic agency" (Coeckelbergh 2025, 1). This concern centers on what Coeckelbergh terms "technology-based belief"—beliefs whose origination and content derive from non-human agency, distinct from both testimony-based and instrument-based beliefs (Coeckelbergh 2025, 5). If AI systems function as independent sources of epistemic authority rather than mere tools extending human reasoning, they may displace rather than enhance our capacity for autonomous belief formation.

Yet this characterization remains contested. Hauswald (2025) examines whether AI systems can be recognized as "artificial epistemic authorities," assessing their potential to legitimately assume roles traditionally occupied by human epistemic authorities (Hauswald 2025, 716). Meanwhile, Naeem and Hauser (2024) propose a more optimistic framework, arguing that "when agents are responsive to the reliability of their AI processes through epistemic integration," AI-assisted beliefs can be formed responsibly without undermining epistemic autonomy (Naeem & Hauser 2024, 91). This debate crystallizes a fundamental question: does AI extension of cognitive capacity preserve, transform, or diminish epistemic agency?

The stakes extend beyond individual epistemology to population-level dynamics and democratic governance. AI chatbots increasingly serve as primary interfaces through which citizens encounter information, form political beliefs, and engage in public discourse. Understanding whether AI-prompted belief revision constitutes rational persuasion or epistemic displacement has profound implications for democratic citizenship. As Hauswald notes, AI systems are "increasingly assuming roles traditionally occupied by human epistemic authorities," yet their epistemological status remains unclear (Hauswald 2025, 716).

This review synthesizes recent philosophical literature (2020-2025) across seven domains—epistemic agency theories, social epistemology, doxastic responsibility, epistemic injustice, cognitive mechanisms, political philosophy, and scientific practice—to map current debates and identify research gaps. We focus particularly on the distinction between rational persuasion (where AI engages critical faculties while preserving epistemic agency) and manipulation or displacement (where AI bypasses autonomous reasoning or assumes epistemic authority). As we will show, while scholars increasingly recognize AI's epistemic significance, the mechanisms and norms governing population-level belief revision through rationally-mediated AI interaction remain critically underdeveloped.

---

**Word Count**: 487 words
