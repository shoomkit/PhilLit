@comment{===================================================================}
@comment{DOMAIN 5: LLMS, REASONING, AND PHILOSOPHY OF MIND}
@comment{===================================================================}
@comment{
Domain Focus: This domain examines cognitive and philosophical aspects
of how LLMs engage human reasoning capacities. Key themes include:
theory of mind in LLMs, cognitive mechanisms underlying AI-facilitated
belief revision, metacognition and epistemic humility triggered by AI,
dual-process theories and AI interaction, and whether LLMs genuinely
reason or merely simulate reasoning.

Search Strategy: Google Scholar (LLM + reasoning, chatbot + cognition),
arXiv (cognitive science + LLM), Nature Human Behaviour, Cognitive
Science, Minds and Machines, Philosophy of Science, PNAS, ACL conferences

Key Questions:
- What cognitive mechanisms underlie AI-facilitated belief revision?
- How do LLMs engage human reasoning capacities?
- Can AI trigger metacognition and epistemic humility?
- Do LLMs possess theory of mind or merely simulate it?
- How do dual-process theories apply to AI interaction?
}
@comment{===================================================================}

@article{Niu2024Comprehensive,
  author = {Niu, Qian and Liu, Junyu and Bi, Ziqian and Feng, Pohsun and others},
  title = {Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges},
  journal = {arXiv preprint},
  year = {2024},
  note = {arXiv:2409.02387v6},
  url = {https://arxiv.org/abs/2409.02387},
  note = {Comprehensive review exploring intersection of LLMs and cognitive science. Examines similarities and differences between LLMs and human cognitive processes, analyzes evaluation methods for LLMs cognitive abilities, covers applications in various cognitive fields, assesses cognitive biases and limitations. Most recent version December 2024. 15 co-authors from Purdue and other institutions.}
}

@article{Strachan2024Theory,
  author = {Strachan, James W. A. and Albergo, Dalila and Borghini, Giulia and Pansardi, Oriana and Scaliti, Eugenio and Gupta, Saurabh and Saxena, Krati and Rufo, Alessandro and Panzeri, Stefano and Manzi, Guido and Graziano, Michael S. A. and Becchio, Cristina},
  title = {Testing Theory of Mind in Large Language Models and Humans},
  journal = {Nature Human Behaviour},
  year = {2024},
  volume = {8},
  number = {7},
  pages = {1285--1295},
  doi = {10.1038/s41562-024-01882-z},
  url = {https://www.nature.com/articles/s41562-024-01882-z},
  note = {Major empirical study comparing GPT and LLaMA2 models against 1,907 human participants on comprehensive battery of ToM tests. Found GPT-4 performed at or above human levels at identifying indirect requests, false beliefs and misdirection, but struggled with detecting faux pas. Authors emphasize not claiming AI 'has' theory of mind, only exhibits behavior indistinguishable from humans on ToM tests. Published May 2024.}
}

@article{OpenMind2024Limitations,
  title = {The Limitations of Large Language Models for Understanding Human Language and Cognition},
  journal = {Open Mind},
  publisher = {MIT Press},
  year = {2024},
  doi = {10.1162/opmi_a_00160},
  url = {https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00160/124234/The-Limitations-of-Large-Language-Models-for},
  note = {Critical philosophical perspective on LLMs as models of human cognition. Argues LLMs' potential as cognitive models can only be realized where specifically designed or adapted to model particular cognitive phenomena. Smaller, more targeted models often provide more efficient solutions with greater explanatory power. Chemero (2023) cited: LLMs differ from human cognition because not embodied.}
}

@article{Nature2025Encoding,
  author = {Authors, Multiple},
  title = {How Large Language Models Encode Theory-of-Mind: A Study on Sparse Parameter Patterns},
  journal = {npj Artificial Intelligence},
  year = {2025},
  doi = {10.1038/s44387-025-00031-9},
  url = {https://www.nature.com/articles/s44387-025-00031-9},
  note = {Mechanistic research on how LLMs internally represent ToM. Found LLMs use small, specialized set of internal connections to handle social reasoning. Abilities depend strongly on how model represents word positions, especially through rotary positional encoding (RoPE). November 2025. Reveals cognitive architecture of AI reasoning.}
}

@article{Chen2025Survey,
  author = {Chen, TBD and Jiang, TBD and Qin, TBD and Tan, TBD},
  title = {Theory of Mind in Large Language Models: Assessment and Enhancement},
  journal = {ACL 2025},
  year = {2025},
  url = {https://aclanthology.org/2025.acl-long.1522.pdf},
  note = {First broad survey addressing both evaluation and enhancement of LLMs' ToM capabilities. Comprehensive review of benchmarks (FANToM, ATOMS, ToMi, HI-TOM). Found: 1) LLMs can match human performance on specific ToM tasks, 2) LLM ToM remains limited and non-robust, 3) internal ToM representations suggest emerging cognitive capabilities. Identifies safety implications. ACL 2025 proceedings.}
}

@article{PNAS2024Evaluating,
  title = {Evaluating Large Language Models in Theory of Mind Tasks},
  journal = {PNAS},
  year = {2024},
  doi = {10.1073/pnas.2405460121},
  url = {https://www.pnas.org/doi/10.1073/pnas.2405460121},
  note = {Assessed eleven LLMs using 40 false-belief tasks. Found ChatGPT-4 solved 75% of false-belief tasks. As model size and training sophistication increased, performance on ToM tasks improved dramatically, with sharp jumps in ability for advanced models. Ullman (2023) cited: successes might be brittle - small logically irrelevant modifications cause models like GPT-3.5 to fail questions previously answered correctly.}
}

@article{PhilPsych2024ChatGPT,
  title = {ChatGPT and the Technology-Education Tension: Applying Contextual Virtue Epistemology to a Cognitive Artifact},
  journal = {Philosophy \& Technology},
  year = {2024},
  doi = {10.1007/s13347-024-00701-7},
  url = {https://link.springer.com/article/10.1007/s13347-024-00701-7},
  note = {Applies virtue epistemology to ChatGPT as cognitive artifact. Discusses how educators might appreciate students' 'epistemic humility and metacognitive capacities' when understanding it's 'epistemically preferable to use digital calculator' rather than relying solely on own abilities. Educational practices should incorporate LLMs without undermining students' cognitive character.}
}

@article{MedPhil2025Humility,
  title = {The Need for Epistemic Humility in AI-Assisted Pain Assessment},
  journal = {Medicine, Health Care and Philosophy},
  year = {2025},
  doi = {10.1007/s11019-025-10264-9},
  url = {https://link.springer.com/article/10.1007/s11019-025-10264-9},
  note = {Medical context. Proposes that 'adoption of epistemic humility on part of both AI tool developers and clinicians can contribute to climate of trust'. Notes 'calls for epistemic humility in AI development and AI ethics more broadly'. Healthcare application of epistemic virtue concepts to AI systems.}
}

@article{Frontiers2024Metacognition,
  title = {AI-Empowered Imagery Writing: Integrating AI-Generated Imagery into Digital Mental Health Service},
  journal = {Frontiers in Psychiatry},
  year = {2024},
  doi = {10.3389/fpsyt.2024.1434172},
  url = {https://www.frontiersin.org/journals/psychiatry/articles/10.3389/fpsyt.2024.1434172/full},
  note = {Describes perspectival metacognition (PMC) as 'nuanced form of metacognition that encompasses non-propositional elements like epistemic humility and consideration of diverse perspectives'. Discusses how 'AI chatbots such as ChatGPT have been proposed as first aid for young adults with mental health issues'. Clinical application of AI engaging metacognition.}
}

@article{Wulff2024Advancing,
  author = {Wulff, Dirk U. and Mata, Rui},
  title = {Advancing Cognitive Science with LLMs},
  journal = {arXiv preprint},
  year = {2024},
  note = {arXiv:2511.00206},
  url = {https://arxiv.org/abs/2511.00206},
  note = {Second major 2024 review on LLMs and cognitive science. Explores how LLMs can advance cognitive science research while acknowledging limitations. LLMs have shown capabilities resembling human cognition, including GPT-3 exhibiting priming, distance, SNARC, and size congruity effects, as well as content effects in logical reasoning tasks similar to humans.}
}

@article{PhilRoySoc2024Reevaluating,
  title = {Re-evaluating Theory of Mind Evaluation in Large Language Models},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  year = {2024},
  doi = {10.1098/rstb.2023.0499},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2023.0499},
  note = {Philosophical reevaluation of ToM testing in LLMs. Critical examination of methodologies and what ToM tests actually reveal about AI cognitive capacities. Question of whether LLMs possess Theory of Mind has sparked significant scientific and public interest. Published in prestigious Royal Society journal.}
}

@article{PhilSci2025Psychology,
  title = {The Psychology of LLM Interactions: The Uncanny Valley and Other Minds},
  journal = {Philosophy of Science (journal)},
  year = {2025},
  doi = {10.1080/29974100.2025.2457627},
  url = {https://www.tandfonline.com/doi/full/10.1080/29974100.2025.2457627},
  note = {Explores psychological and philosophical dimensions of human-LLM interaction. Applies 'other minds' problem and uncanny valley concepts to AI. How humans attribute mental states to LLMs and epistemic implications of these attributions. Philosophy of science meets psychology of AI interaction.}
}
