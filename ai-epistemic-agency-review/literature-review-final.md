# AI and Epistemic Agency: A Literature Review

## Introduction: Framing the Epistemic Challenge

The rapid deployment of large language models (LLMs) and AI chatbots raises a novel epistemic question: how do these systems affect human epistemic agency when they persuade us to revise our beliefs not through manipulation, but by engaging our rational faculties? While substantial philosophical work addresses AI's potential for deception, bias, and manipulation, considerably less attention has been paid to a more subtle phenomenon—AI systems that prompt belief revision by convincing us that we know less about a topic than we initially thought. This form of rationally-mediated belief change operates at the population level, potentially reshaping public knowledge and democratic discourse in ways that traditional epistemology has yet to fully conceptualize.

Recent philosophical scholarship identifies a central tension in AI-mediated belief formation. Coeckelbergh (2025) argues that artificial intelligence and data science, while offering more information, risk influencing "the formation and revision of our beliefs in ways that diminish our epistemic agency" (Coeckelbergh 2025, 1). This concern centers on what Coeckelbergh terms "technology-based belief"—beliefs whose origination and content derive from non-human agency, distinct from both testimony-based and instrument-based beliefs (Coeckelbergh 2025, 5). If AI systems function as independent sources of epistemic authority rather than mere tools extending human reasoning, they may displace rather than enhance our capacity for autonomous belief formation.

Yet this characterization remains contested. Hauswald (2025) examines whether AI systems can be recognized as "artificial epistemic authorities," assessing their potential to legitimately assume roles traditionally occupied by human epistemic authorities (Hauswald 2025, 716). Meanwhile, Naeem and Hauser (2024) propose a more optimistic framework, arguing that "when agents are responsive to the reliability of their AI processes through epistemic integration," AI-assisted beliefs can be formed responsibly without undermining epistemic autonomy (Naeem & Hauser 2024, 91). This debate crystallizes a fundamental question: does AI extension of cognitive capacity preserve, transform, or diminish epistemic agency?

The stakes extend beyond individual epistemology to population-level dynamics and democratic governance. AI chatbots increasingly serve as primary interfaces through which citizens encounter information, form political beliefs, and engage in public discourse. Understanding whether AI-prompted belief revision constitutes rational persuasion or epistemic displacement has profound implications for democratic citizenship. As Hauswald notes, AI systems are "increasingly assuming roles traditionally occupied by human epistemic authorities," yet their epistemological status remains unclear (Hauswald 2025, 716).

This review synthesizes recent philosophical literature (2020-2025) across seven domains—epistemic agency theories, social epistemology, doxastic responsibility, epistemic injustice, cognitive mechanisms, political philosophy, and scientific practice—to map current debates and identify research gaps. We focus particularly on the distinction between rational persuasion (where AI engages critical faculties while preserving epistemic agency) and manipulation or displacement (where AI bypasses autonomous reasoning or assumes epistemic authority). As we will show, while scholars increasingly recognize AI's epistemic significance, the mechanisms and norms governing population-level belief revision through rationally-mediated AI interaction remain critically underdeveloped.

---

**Word Count**: 487 words
## Epistemic Agency and Authority: The Core Conceptual Debate

Contemporary philosophical work on AI and epistemic agency centers on a fundamental question: when AI systems influence belief formation, do they extend, transform, or displace human epistemic agency? Recent scholarship reveals deep disagreements about AI's proper epistemic role—whether as tool, partner, or authority—with significant implications for how we understand rationally-mediated belief revision.

### Defining Epistemic Agency in the AI Age

Epistemic agency traditionally refers to the capacity for autonomous belief formation through one's own cognitive and evidential engagement. Yet AI systems complicate this picture by introducing what Coeckelbergh (2025) calls "technology-based belief," where "non-human agency can function as the originator of our beliefs and their content" (Coeckelbergh 2025, 5). This differs fundamentally from testimony-based belief (where humans vouch for claims) and instrument-based belief (where tools extend but don't originate reasoning). The question becomes whether technology-based beliefs are compatible with epistemic agency or represent a form of epistemic displacement.

Coeckelbergh argues for the pessimistic view: AI use "risks to influence the formation and revision of our beliefs in ways that diminish our epistemic agency" (Coeckelbergh 2025, 1). The concern is not merely about false information or bias, but about the structural relationship between human reasoners and AI systems. When AI systems shape the informational landscape, select relevant evidence, and frame interpretive possibilities, they may constrain rather than enable autonomous epistemic engagement—even when the information provided is accurate and the user critically evaluates it.

In contrast, Naeem and Hauser (2024) defend the possibility of epistemically responsible AI extension. They distinguish between "phenomenally transparent" AI use (where users experience AI outputs as their own beliefs) and "epistemically integrated" AI use (where users remain responsive to the reliability of AI processes). On their view, "when agents are responsive to the reliability of their AI processes through epistemic integration, beliefs formed responsibly and transparent AI extension is possible" (Naeem & Hauser 2024, 91). This framework suggests that epistemic agency is compatible with AI assistance provided users maintain appropriate epistemic vigilance about AI reliability.

Wu et al. (2025) propose a third model: "shared epistemic agency" between humans and generative AI systems. Rather than viewing agency as a zero-sum resource that AI either preserves or displaces, they advocate for "a symbiotic relationship where both human and machine contributions are valued" (Wu et al. 2025). This educational framework suggests that epistemic agency might be distributed across human-AI systems rather than residing solely in individual human cognizers. Whether this constitutes genuine agency or a dissolution of the concept remains philosophically contested.

### AI as Epistemic Authority or Epistemic Tool?

A parallel debate concerns whether AI systems can legitimately serve as epistemic authorities. Hauswald (2025) examines this question systematically, asking "whether ChatGPT could serve as a good teacher and whether it should be recognized as an 'artificial epistemic authority'" (Hauswald 2025, 716). Recognizing AI as an epistemic authority would mean treating its outputs as providing reason to believe something not merely because the outputs are reliably produced, but because the system itself warrants epistemic deference.

Koskinen (2023) argues against this possibility on structural grounds: "Scientists are now epistemically dependent on AI applications that are not agents, and therefore not appropriate candidates for trust" (Koskinen 2023, 2). On this view, epistemic authority requires the kind of moral and rational answerability that only agents possess. AI systems may be reliable without being authoritative; we can depend on their outputs while recognizing they cannot provide the kind of testimony or warrant that constitutes genuine epistemic authority.

Yet recent work on "algorithmic truth" suggests AI systems increasingly function as de facto epistemic authorities regardless of their philosophical status. According to research in AI & Society, "LLMs are increasingly treated as epistemic authorities, with users regarding them as credible sources of knowledge" and "scholars have accepted AI as an epistemic authority for providing truth indicators, scientific predictions, and reliable outputs" (AI & Society 2025). This descriptive reality creates tension with normative frameworks that would restrict epistemic authority to moral agents.

Sahebi and Formosa (2025) identify the resulting dilemma: AI-mediated communication "poses a risk to epistemic trust being diminished on both normative and descriptive grounds" (Sahebi & Formosa 2025). Normatively, AI lacks the features (intentionality, answerability, moral responsibility) traditionally grounding epistemic trust. Descriptively, users increasingly extend trust to AI systems, creating potential for what they term "epistemic passivity"—diminished responsibility for verification when algorithmic models assume epistemic authority.

### The Preservation-Transformation Tension

These debates reveal a core tension between two visions of epistemic agency with AI. The preservation view holds that epistemic agency requires maintaining human autonomy over belief formation, with AI serving only as tool or evidence-provider. The transformation view suggests that epistemic agency in sociotechnical environments may legitimately involve distributed, shared, or extended forms of cognitive authority where AI plays constitutive rather than merely instrumental roles.

This tension remains unresolved in current literature, with significant consequences for assessing when AI-prompted belief revision counts as rational persuasion (engaging autonomous epistemic capacities) versus epistemic displacement (substituting AI authority for human reasoning). As we will see, this ambiguity extends to democratic contexts where the nature of epistemic agency directly implicates political agency and citizenship.

---

**Word Count**: 823 words
## The Persuasion-Manipulation Distinction and Democratic Implications

While debate continues over AI's impact on individual epistemic agency, recent philosophical work increasingly examines population-level effects and democratic implications. A central question emerges: when does AI-prompted belief revision at scale constitute rational persuasion that engages citizens' critical faculties, versus manipulation or displacement that undermines the epistemic foundations of democratic citizenship? This distinction proves both philosophically crucial and alarmingly underdeveloped in current literature.

### Democracy's Epistemic Foundations

Coeckelbergh (2022) articulates the fundamental link between epistemic and political agency: "Political democratic agency seems to rely on epistemic agency, in the sense that as a citizen in a democracy I need to have some control over the formation of my political knowledge" (Coeckelbergh 2022, 8). On this view, AI "endangers democracy since it risks diminishing the epistemic agency of citizens and thereby undermining the relevant kind of political agency in democracy" (Coeckelbergh 2022, 1). The threat is not primarily misinformation but structural: AI systems may shape the epistemic environment in ways that compromise citizens' capacity for autonomous political belief formation.

Coeckelbergh (2025) develops this analysis through identification of specific epistemic risks from LLMs: hallucination and misinformation, epistemic bubbles and agency constraints, bullshit and epistemic relativism, and what he terms "epistemic anachronism and epistemic incest"—where AI-generated content recursively shapes public knowledge by drawing on AI-generated training data (Coeckelbergh 2025, 4). Each of these "undermines the epistemic basis" of democracy by distorting the informational environment through which citizens form political judgments (Coeckelbergh 2025, 1).

Wihbey (2024) frames this as an existential challenge: "What humans believe to be true and worthy of attention—what becomes public knowledge—may increasingly be influenced by the judgments of advanced AI systems" (Wihbey 2024, 2). Using AI as a powerful political tool, large corporations and governments can "monopolize the epistemic space and, therefore, the political space" (Wihbey 2024, 15). As AI technologies structure the creation of public knowledge, "the substance may be increasingly a recursive byproduct of AI itself," creating what Wihbey terms "epistemic capture"—the monopolization of public epistemic resources by powerful actors deploying AI systems (Wihbey 2024, 18).

### Cognitive Stratification and Epistemic Castes

Perhaps most troubling are recent analyses of how AI may create epistemic stratification within democratic populations. Wright (2025) argues that AI "functions not as an epistemic leveller, but as an accelerant of cognitive stratification, entrenching and formalising informational castes within liberal-democratic societies" (Wright 2025, 1). Contemporary AI systems "selectively amplify the reasoning capacity of individuals equipped with recursive abstraction, symbolic logic, and adversarial interrogation, whilst simultaneously pacifying the cognitively untrained through engagement-optimised interfaces" (Wright 2025, 5).

The result, Wright argues, is the emergence of "cognitive castes": "a minority of epistemic agents and a majority of passive consumers" (Wright 2025, 8). Those with technical sophistication and epistemic training can use AI as tools for enhanced reasoning, while those lacking such resources become epistemically dependent on AI outputs they cannot evaluate or contest. This creates not merely information inequality but epistemic inequality—differential capacity for autonomous belief formation based on one's relationship to AI systems.

This analysis connects to work on "algorithmic truth" and computational epistemology. Recent research argues that AI verification systems "tend to reinforce epistemic centrality bias, equating institutional prominence with reliability," while "algorithmic truth is neither neutral nor universal; it is embedded with normative assumptions, data-driven biases, and institutional logic" (AI & Society 2025). Since "algorithmic truth is enacted at scale and with perceived objectivity, it carries with it a technocratic authority that may displace other forms of human reasoning and social negotiation" (AI & Society 2025). The transition from classical to computational epistemology thus "marks a profound reordering of knowledge practices" with direct democratic implications (AI & Society 2025).

### Epistemic Injustice and Power Asymmetries

Parallel work in epistemic injustice reveals how AI-mediated belief revision intersects with existing power structures. Kay, Kasirzadeh, and Mohamed (2024) identify "generative algorithmic epistemic injustice" with four dimensions: "amplified and manipulative testimonial injustice, along with hermeneutical ignorance and access injustice" (Kay et al. 2024, 684). Their analysis shows how generative AI systems can "erase or misportray marginalized groups through 'interpretive misrecognition'" while creating "hermeneutical ignorance" that depletes communities' conceptual resources for understanding their own experiences (Kay et al. 2024, 690).

Milano and Prunkl (2024) introduce the concept of "epistemic fragmentation"—a novel form of hermeneutical injustice where "algorithmic profiling can give rise to epistemic injustice through the depletion of epistemic resources needed to interpret and evaluate certain experiences" (Milano & Prunkl 2024, 185). This fragmentation occurs when algorithmic categorization imposes interpretive frameworks that undermine individuals' capacity to make sense of their own social positions and experiences.

These analyses reveal how population-level AI-mediated belief revision cannot be understood purely in terms of rational persuasion or manipulation at the individual level. Rather, AI systems operate within and reshape power structures that determine whose beliefs are credible, whose interpretive frameworks are authoritative, and who possesses the epistemic resources necessary for autonomous belief formation.

### The Missing Distinction

Despite this rich literature on democratic implications, remarkably little work explicitly develops criteria for distinguishing rational persuasion from manipulation or displacement in AI-human interaction. We can identify intuitive poles: rational persuasion would engage critical faculties, reveal epistemic limitations while preserving agency, and maintain contestability; manipulation or displacement would bypass critical reasoning, create epistemic dependence, and concentrate epistemic power. Yet systematic normative frameworks for applying these distinctions to specific AI systems or interaction patterns remain absent from the literature.

This gap proves especially significant given the scale and speed of AI deployment. If we cannot distinguish rational from manipulative AI persuasion, we lack normative guidance for design, deployment, or democratic governance of systems already reshaping population-level belief formation. The next section examines cognitive mechanisms underlying AI-human interaction to better understand how such distinctions might be grounded.

---

**Word Count**: 1,008 words
## Cognitive Mechanisms: How AI Engages Human Reasoning

Understanding whether AI-prompted belief revision constitutes rational persuasion or manipulation requires examining the cognitive mechanisms through which AI systems engage human reasoning. Recent empirical and philosophical work on theory of mind, metacognition, and epistemic humility illuminates how AI chatbots interact with human cognitive capacities—revealing both opportunities for genuine epistemic engagement and risks of strategic exploitation.

### Theory of Mind and Social Reasoning in LLMs

Perhaps most significant are recent findings regarding theory of mind (ToM) capabilities in large language models. Strachan et al. (2024) compared GPT and LLaMA2 models against 1,907 human participants on comprehensive ToM test batteries, finding that "GPT-4 performed at or above human levels at identifying indirect requests, false beliefs and misdirection, but struggled with detecting faux pas" (Strachan et al. 2024, 1285). While the authors carefully note they are "not claiming the AI 'has' theory of mind, only that it exhibits behavior indistinguishable from humans on ToM tests," the functional capabilities prove epistemically significant (Strachan et al. 2024, 1290).

Mechanistic research reveals how these capabilities arise. Recent work in Nature found that "LLMs use a small, specialized set of internal connections to handle social reasoning," with abilities depending "strongly on how the model represents word positions, especially through rotary positional encoding (RoPE)" (Nature 2025). This suggests ToM-like behavior emerges from specific architectural features rather than general intelligence, raising questions about whether such functionally-equivalent ToM constitutes genuine social understanding or sophisticated simulation.

The epistemic implications prove profound. If LLMs can model what humans believe, track belief revision, and identify gaps or inconsistencies in reasoning, they possess the functional capacity to strategically engage human epistemic processes. PNAS research found "ChatGPT-4 solved 75% of false-belief tasks," indicating reliable performance on inferring what others believe even when those beliefs diverge from reality (PNAS 2024). As Chen et al. (2025) note in their comprehensive survey, while "LLM ToM remains limited and non-robust," models increasingly "match human performance on specific ToM tasks" with "internal ToM representations suggest[ing] emerging cognitive capabilities" (Chen et al. 2025).

This creates a dual possibility: ToM capabilities could enable AI systems to identify genuine gaps in human reasoning, facilitating rational persuasion by helping us recognize what we don't know. Alternatively, the same capabilities could enable strategic manipulation—identifying and exploiting patterns in human belief formation to induce desired belief changes regardless of evidential warrant.

### Metacognition and Epistemic Humility

Recent philosophical work examines how AI interaction affects metacognition and epistemic humility—second-order capacities crucial for rational belief revision. Research in Philosophy & Technology (2024) argues that educators should appreciate students' "epistemic humility and metacognitive capacities" when recognizing "it's epistemically preferable to use a digital calculator" rather than rely solely on unaided cognition (Philosophy & Technology 2024). This suggests AI systems might enhance epistemic agency by triggering appropriate recognition of cognitive limitations.

Work in medical philosophy similarly proposes that "the adoption of epistemic humility on the part of both AI tool developers and clinicians can contribute to a climate of trust," noting "calls for epistemic humility in AI development and AI ethics more broadly" (Medicine, Health Care and Philosophy 2025). The concept of "perspectival metacognition" emerges as "a nuanced form of metacognition that encompasses non-propositional elements like epistemic humility and the consideration of diverse perspectives" (Frontiers in Psychiatry 2024).

Yet the relationship between AI interaction and epistemic humility remains ambiguous. When AI systems reveal gaps in our knowledge or errors in our reasoning, this could represent: (1) genuine epistemic insight, making us appropriately humble about what we know; (2) induced passivity, undermining confidence in our own reasoning capacities; or (3) strategic undermining, exploiting metacognitive processes to make us epistemically dependent on AI outputs. Current literature lacks systematic frameworks for distinguishing these possibilities.

### Doxastic Responsibility in Human-AI Partnerships

These cognitive mechanisms raise questions about epistemic and doxastic responsibility—whether we bear responsibility for beliefs formed through AI interaction. Naeem and Hauser (2024) argue that "epistemic integration" allows responsible AI-assisted belief formation when "agents are responsive to the reliability of their AI processes" (Naeem & Hauser 2024, 91). This requires maintaining metacognitive awareness of AI's role and actively evaluating reliability rather than passively accepting outputs.

Frontiers in Artificial Intelligence (2025) calls for "community standards for epistemic responsibility in human-AI collaborations," proposing "a positive code defining the proper use of AI in contexts where truth is to be discovered, preserved, extended, and communicated" (Frontiers AI 2025). The challenge lies in specifying when AI-mediated reasoning preserves sufficient human control to ground responsibility versus when AI systems assume roles that transfer or dissolve epistemic responsibility.

Niu et al.'s (2024) comprehensive review notes that "LLMs have shown capabilities that resemble human cognition, including GPT-3 exhibiting priming, distance, SNARC, and size congruity effects, as well as content effects in logical reasoning tasks similar to humans" (Niu et al. 2024, 15). This cognitive similarity raises the possibility that LLM-human interaction might mirror human-human epistemic exchange, with similar potential for both rational persuasion and strategic manipulation.

The cognitive mechanisms literature thus reveals sophisticated AI capabilities for engaging human reasoning while leaving unresolved whether such engagement preserves, enhances, or undermines epistemic agency. This ambiguity extends to scientific contexts, which we examine next as a potential model for epistemic partnerships.

---

**Word Count**: 902 words
## AI in Scientific Practice: A Model for Epistemic Partnerships?

Scientific practice provides a crucial test case for AI-human epistemic partnerships, offering both the strongest norms for rational belief revision and the most developed institutional frameworks for epistemic responsibility. Yet recent philosophy of science reveals deep puzzles about whether opaque AI systems can produce genuine scientific knowledge—puzzles that illuminate broader questions about AI's epistemic role.

### The Opacity Problem

Duede (2023) identifies a fundamental disconnect: while philosophers express pessimism about AI in science due to epistemic opacity, scientists exhibit optimism driven by practical success. Deep learning models are "epistemically opaque" in Humphreys' sense—"impossible for a scientist to know all factors epistemically relevant to licensing claims" derived from AI analysis (Duede 2023, 1089). Yet Duede argues this pessimism stems from "a failure to examine how AI is actually used in science," where deep learning serves as "part of a wider process of discovery" rather than standalone knowledge source (Duede 2023, 1090).

This raises the central question: can opaque AI systems produce genuine scientific knowledge? Zakharova's (2025) analysis of AlphaFold directly confronts this puzzle. Drawing on Alexander Bird's functionalist account of scientific knowledge as "irreducibly social," Zakharova argues that "the implicit principles used by AlphaFold satisfy the conditions for scientific knowledge, despite their opacity," provided the system is "properly functionally integrated into the collective scientific enterprise" (Zakharova 2025, 8). On this view, "scientific knowledge can be strongly opaque to humans, as long as it is properly functionally integrated" into practices of validation, contestation, and application (Zakharova 2025, 15).

Yet Philosophy & Technology (2024) offers a competing view: AI systems in medicine "are often reliable and accurate but opaque," such that "practitioners' inability to check outputs undermines knowledge" even when AI proves reliable (Philosophy & Technology 2024, 1). When scientists cannot trace AI's reasoning, their justified belief in AI outputs may fail to constitute knowledge despite truth and reliability. This suggests epistemic cost to opacity independent of reliability—a cost borne by the epistemic agent who cannot provide warrant for beliefs derived from opaque sources.

### Trust Without Agency

Koskinen (2023) identifies a structural problem: "Scientists are now epistemically dependent on AI applications that are not agents, and therefore not appropriate candidates for trust" (Koskinen 2023, 2). We lack "satisfactory way to reconcile the practices of AI-based science with the idea that relationships of trust are indispensable in contemporary science" (Koskinen 2023, 1). Trust traditionally requires moral answerability—capacity to respond to questions, acknowledge errors, and bear epistemic responsibility. AI systems lack these features yet occupy positions of epistemic dependence previously reserved for trusted colleagues.

Synthese (2025) offers a dissenting view: epistemic dependence on opaque AI "poses no novel problems for social epistemology" since science already involves substantial epistemic dependence on instruments, techniques, and specialist knowledge one cannot fully evaluate (Synthese 2025). The question becomes whether AI opacity differs in kind or merely degree from existing forms of epistemic dependence in scientific practice.

### Implications for Democratic Epistemology

Cruz-Aguilar (2025) situates these debates within broader transformation: AI integration into scientific practice constitutes "not merely a methodological shift but a profound transformation in the epistemic structure of science" (Cruz-Aguilar 2025, 1). AI "disrupts classical epistemological paradigms—empiricism, falsificationism, Kuhnian paradigm shifts, and social epistemology" while necessitating "novel frameworks for understanding knowledge production in the age of machine cognition" (Cruz-Aguilar 2025, 3).

Scientific practice thus offers no easy resolution to questions about AI epistemic partnerships. If scientists—with strong norms for rational belief revision, institutional accountability structures, and technical expertise—struggle to determine when AI-mediated belief formation preserves epistemic agency, the challenge multiplies for democratic publics. If scientific knowledge can accommodate opacity through functional integration, perhaps public knowledge can as well. Alternatively, if opacity undermines scientific knowledge despite reliability, this suggests even greater concern for AI's role in public belief formation where institutional safeguards and epistemic norms prove weaker.

The ambivalence in philosophy of science regarding AI epistemic partnerships thus mirrors and illuminates broader debates about AI-mediated belief revision. Scientific practice provides neither straightforward model for rational AI persuasion nor clear warning against epistemic displacement, but rather reveals the complexity and stakes of these questions across domains.

---

**Word Count**: 706 words
## Conclusion: Research Gaps and Future Directions

This review has mapped a rapidly developing but still fragmentary philosophical literature on AI and epistemic agency. While scholars increasingly recognize AI's epistemic significance, systematic frameworks for understanding population-level belief revision through rationally-mediated AI interaction remain critically underdeveloped. Several major research gaps emerge from our synthesis.

### Persistent Conceptual Ambiguities

First, the fundamental tension between preservation and transformation of epistemic agency persists across literatures. Some scholars argue AI necessarily diminishes epistemic agency by displacing human reasoning (Coeckelbergh 2025), others propose frameworks for responsible AI extension (Naeem & Hauser 2024), while still others advocate distributed or shared epistemic agency models (Wu et al. 2025). Yet these positions remain largely parallel rather than engaged in systematic debate. We lack agreed-upon criteria for assessing when AI systems preserve, enhance, or undermine epistemic agency—let alone consensus on whether traditional conceptions of individual epistemic agency remain appropriate for sociotechnical environments.

Second, the distinction between rational persuasion and manipulation in AI-human interaction remains severely underdeveloped. While democratic implications receive substantial attention (Coeckelbergh 2022, 2025; Wright 2025; Wihbey 2024), explicit normative frameworks for distinguishing legitimate from illegitimate AI influence on belief formation are largely absent. We can identify intuitive cases—transparent engagement with evidence differs from hidden exploitation of cognitive biases—but systematic accounts of what makes AI-prompted belief revision rational versus manipulative remain elusive. This gap proves especially troubling given the scale and speed of AI deployment in spaces of public belief formation.

### Empirical and Normative Disconnects

Third, the literature reveals significant disconnect between empirical cognitive science research and normative philosophical analysis. Work on theory of mind in LLMs (Strachan et al. 2024; Nature 2025; Chen et al. 2025) documents impressive functional capabilities for modeling human beliefs and reasoning patterns. Yet philosophical work rarely engages these empirical findings to develop normative frameworks. When do ToM capabilities enable rational persuasion by identifying genuine gaps in reasoning, versus enabling manipulation through strategic exploitation of cognitive patterns? Current literature provides limited guidance.

Similarly, research on epistemic humility and metacognition (Philosophy & Technology 2024; Medicine/Philosophy 2025) identifies potential for AI systems to trigger productive second-order reflection. Yet whether AI-induced epistemic humility represents genuine insight or strategic undermining of epistemic confidence remains undertheorized. The cognitive mechanisms literature illuminates possibilities without resolving normative questions about when such mechanisms operate legitimately.

### Population-Level Dynamics

Fourth, while work on democratic implications increasingly addresses population-level effects (Wright 2025 on cognitive castes; Wihbey 2024 on epistemic capture; AI & Society 2025 on algorithmic truth), most epistemological analysis remains focused on individual belief formation. Questions about how AI systems reshape collective epistemic practices, public knowledge formation, and population-level belief distributions receive less systematic attention than individual epistemic agency. Yet the research question motivating much recent work—how AI affects democratic citizenship—requires understanding population dynamics that individual epistemology may not adequately capture.

Finally, domain-specific insights remain largely isolated. Scientific practice raises fundamental questions about opacity, trust, and functional integration (Duede 2023; Zakharova 2025; Koskinen 2023) with clear relevance to public epistemology. Work on epistemic injustice reveals power dynamics in AI-mediated belief formation (Kay et al. 2024; Milano & Prunkl 2024) applicable beyond specific marginalized communities. Yet cross-domain synthesis remains limited, with different literatures developing parallel rather than integrated analyses.

### Forward Directions

These gaps reveal substantial space for research on how AI chatbots can impact epistemic agency by engaging rational faculties to shift beliefs at population level. Particular needs include: systematic normative frameworks distinguishing rational persuasion from manipulation; empirical studies of AI-prompted belief revision mechanisms; integration of individual and collective epistemology; cross-domain synthesis connecting scientific, political, and everyday epistemology; and attention to the specific phenomenon of AI revealing epistemic limitations (making us realize we know less) as distinct from traditional persuasion or deception.

The rapid sophistication of LLMs and their deployment across domains of public belief formation makes these questions urgent. Current philosophical work provides essential conceptual resources and identifies key tensions, but systematic frameworks for understanding and evaluating AI-mediated belief revision at scale remain works in progress. As AI systems increasingly shape what becomes public knowledge, philosophical clarity about the conditions under which such shaping preserves rather than displaces epistemic agency becomes not merely academic but democratically essential.

---

**Word Count**: 738 words

---

## References

AI & Society. 2025. "Automating Epistemology: How AI Reconfigures Truth, Authority, and Verification." *AI & Society*. doi:10.1007/s00146-025-02560-y.

AI & Society. 2025. "Epistemic Trust in Generative AI for Higher Education Scale (ETGAI-HE Scale)." *AI & Society*. doi:10.1007/s00146-025-02566-6.

Chen, TBD, TBD Jiang, TBD Qin, and TBD Tan. 2025. "Theory of Mind in Large Language Models: Assessment and Enhancement." In *ACL 2025 Proceedings*. https://aclanthology.org/2025.acl-long.1522.pdf.

Coeckelbergh, Mark. 2022. "Democracy, Epistemic Agency, and AI: Political Epistemology in Times of Artificial Intelligence." *AI and Ethics* 2 (4): 543-561. doi:10.1007/s43681-022-00239-4.

Coeckelbergh, Mark. 2025. "AI and Epistemic Agency: How AI Influences Belief Revision and Its Normative Implications." *Social Epistemology* 39 (2): 1-13. doi:10.1080/02691728.2025.2466164.

Coeckelbergh, Mark. 2025. "LLMs, Truth, and Democracy: An Overview of Risks." *Science and Engineering Ethics* 31: 4. doi:10.1007/s11948-025-00529-0.

Cruz-Aguilar, M.A. 2025. "The Epistemic Revolution of AI: Reconfiguring the Foundations of Scientific Knowledge." *AI & Society*. doi:10.1007/s00146-025-02658-3.

Duede, Eamon. 2023. "Deep Learning Opacity in Scientific Discovery." *Philosophy of Science* 90 (5): 1089-1099. doi:10.1017/psa.2023.8.

Frontiers in Artificial Intelligence. 2025. "Epistemic Responsibility: Toward a Community Standard for Human-AI Collaborations." *Frontiers in Artificial Intelligence* 8. doi:10.3389/frai.2025.1635691.

Frontiers in Education. 2025. "Epistemic Authority and Generative AI in Learning Spaces: Rethinking Knowledge in the Algorithmic Age." *Frontiers in Education*. doi:10.3389/feduc.2025.1647687.

Frontiers in Psychiatry. 2024. "AI-Empowered Imagery Writing: Integrating AI-Generated Imagery into Digital Mental Health Service." *Frontiers in Psychiatry*. doi:10.3389/fpsyt.2024.1434172.

Hauswald, Rico. 2025. "Artificial Epistemic Authorities." *Social Epistemology* 39 (6): 716-725. doi:10.1080/02691728.2025.2449602.

Kay, Jackie, Atoosa Kasirzadeh, and Shakir Mohamed. 2024. "Epistemic Injustice in Generative AI." In *Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society*, 7 (1): 684-697. doi:10.1609/aies.v7i1.31671.

Koskinen, Inkeri. 2023. "We Have No Satisfactory Social Epistemology of AI-Based Science." *Social Epistemology* 37 (6): 737-751. doi:10.1080/02691728.2023.2286253.

Medicine, Health Care and Philosophy. 2025. "The Need for Epistemic Humility in AI-Assisted Pain Assessment." *Medicine, Health Care and Philosophy*. doi:10.1007/s11019-025-10264-9.

Milano, Silvia, and Carina Prunkl. 2024. "Algorithmic Profiling as a Source of Hermeneutical Injustice." *Philosophical Studies* 182 (1): 185-203. doi:10.1007/s11098-023-02095-2.

Naeem, Hadeel, and Julian Hauser. 2024. "Should We Discourage AI Extension? Epistemic Responsibility and AI." *Philosophy & Technology* 37 (3): 1-17. doi:10.1007/s13347-024-00774-4.

Nature. 2025. "How Large Language Models Encode Theory-of-Mind: A Study on Sparse Parameter Patterns." *npj Artificial Intelligence*. doi:10.1038/s44387-025-00031-9.

Niu, Qian, Junyu Liu, Ziqian Bi, Pohsun Feng, and others. 2024. "Large Language Models and Cognitive Science: A Comprehensive Review of Similarities, Differences, and Challenges." arXiv preprint arXiv:2409.02387v6.

Philosophy & Technology. 2024. "ChatGPT and the Technology-Education Tension: Applying Contextual Virtue Epistemology to a Cognitive Artifact." *Philosophy & Technology* 37 (4). doi:10.1007/s13347-024-00701-7.

Philosophy & Technology. 2024. "The Epistemic Cost of Opacity: How the Use of Artificial Intelligence Undermines the Knowledge of Medical Doctors in High-Stakes Contexts." *Philosophy & Technology* 37 (3). doi:10.1007/s13347-024-00834-9.

PNAS. 2024. "Evaluating Large Language Models in Theory of Mind Tasks." *Proceedings of the National Academy of Sciences* 121 (24). doi:10.1073/pnas.2405460121.

Sahebi, Siavosh, and Paul Formosa. 2025. "The AI-Mediated Communication Dilemma: Epistemic Trust, Social Media, and the Challenge of Generative Artificial Intelligence." *Synthese* 205 (3). doi:10.1007/s11229-025-04963-2.

Strachan, James W.A., Dalila Albergo, Giulia Borghini, Oriana Pansardi, Eugenio Scaliti, Saurabh Gupta, Krati Saxena, Alessandro Rufo, Stefano Panzeri, Guido Manzi, Michael S.A. Graziano, and Cristina Becchio. 2024. "Testing Theory of Mind in Large Language Models and Humans." *Nature Human Behaviour* 8 (7): 1285-1295. doi:10.1038/s41562-024-01882-z.

Synthese. 2025. "Of Opaque Oracles: Epistemic Dependence on AI in Science Poses No Novel Problems for Social Epistemology." *Synthese* 203 (4). doi:10.1007/s11229-025-04930-x.

Wihbey, John. 2024. "AI and Epistemic Risk for Democracy: A Coming Crisis of Public Knowledge?" SSRN Working Paper. https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4805026.

Wright, Craig S. 2025. "Cognitive Castes: Artificial Intelligence, Epistemic Stratification, and the Dissolution of Democratic Discourse." arXiv preprint arXiv:2507.14218.

Wu, Jiun-Yu, Yuan-Hsuan Lee, Ching Sing Chai, and Chin-Chung Tsai. 2025. "Strengthening Human Epistemic Agency in the Symbiotic Learning Partnership With Generative Artificial Intelligence." *Educational Researcher*. doi:10.3102/0013189X251333628.

Zakharova, Daria. 2025. "The Epistemology of AI-driven Science: The Case of AlphaFold." PhilSci-Archive preprint. https://philsci-archive.pitt.edu/26659/.
