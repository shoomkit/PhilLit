## Cognitive Mechanisms: How AI Engages Human Reasoning

Understanding whether AI-prompted belief revision constitutes rational persuasion or manipulation requires examining the cognitive mechanisms through which AI systems engage human reasoning. Recent empirical and philosophical work on theory of mind, metacognition, and epistemic humility illuminates how AI chatbots interact with human cognitive capacities—revealing both opportunities for genuine epistemic engagement and risks of strategic exploitation.

### Theory of Mind and Social Reasoning in LLMs

Perhaps most significant are recent findings regarding theory of mind (ToM) capabilities in large language models. Strachan et al. (2024) compared GPT and LLaMA2 models against 1,907 human participants on comprehensive ToM test batteries, finding that "GPT-4 performed at or above human levels at identifying indirect requests, false beliefs and misdirection, but struggled with detecting faux pas" (Strachan et al. 2024, 1285). While the authors carefully note they are "not claiming the AI 'has' theory of mind, only that it exhibits behavior indistinguishable from humans on ToM tests," the functional capabilities prove epistemically significant (Strachan et al. 2024, 1290).

Mechanistic research reveals how these capabilities arise. Recent work in Nature found that "LLMs use a small, specialized set of internal connections to handle social reasoning," with abilities depending "strongly on how the model represents word positions, especially through rotary positional encoding (RoPE)" (Nature 2025). This suggests ToM-like behavior emerges from specific architectural features rather than general intelligence, raising questions about whether such functionally-equivalent ToM constitutes genuine social understanding or sophisticated simulation.

The epistemic implications prove profound. If LLMs can model what humans believe, track belief revision, and identify gaps or inconsistencies in reasoning, they possess the functional capacity to strategically engage human epistemic processes. PNAS research found "ChatGPT-4 solved 75% of false-belief tasks," indicating reliable performance on inferring what others believe even when those beliefs diverge from reality (PNAS 2024). As Chen et al. (2025) note in their comprehensive survey, while "LLM ToM remains limited and non-robust," models increasingly "match human performance on specific ToM tasks" with "internal ToM representations suggest[ing] emerging cognitive capabilities" (Chen et al. 2025).

This creates a dual possibility: ToM capabilities could enable AI systems to identify genuine gaps in human reasoning, facilitating rational persuasion by helping us recognize what we don't know. Alternatively, the same capabilities could enable strategic manipulation—identifying and exploiting patterns in human belief formation to induce desired belief changes regardless of evidential warrant.

### Metacognition and Epistemic Humility

Recent philosophical work examines how AI interaction affects metacognition and epistemic humility—second-order capacities crucial for rational belief revision. Research in Philosophy & Technology (2024) argues that educators should appreciate students' "epistemic humility and metacognitive capacities" when recognizing "it's epistemically preferable to use a digital calculator" rather than rely solely on unaided cognition (Philosophy & Technology 2024). This suggests AI systems might enhance epistemic agency by triggering appropriate recognition of cognitive limitations.

Work in medical philosophy similarly proposes that "the adoption of epistemic humility on the part of both AI tool developers and clinicians can contribute to a climate of trust," noting "calls for epistemic humility in AI development and AI ethics more broadly" (Medicine, Health Care and Philosophy 2025). The concept of "perspectival metacognition" emerges as "a nuanced form of metacognition that encompasses non-propositional elements like epistemic humility and the consideration of diverse perspectives" (Frontiers in Psychiatry 2024).

Yet the relationship between AI interaction and epistemic humility remains ambiguous. When AI systems reveal gaps in our knowledge or errors in our reasoning, this could represent: (1) genuine epistemic insight, making us appropriately humble about what we know; (2) induced passivity, undermining confidence in our own reasoning capacities; or (3) strategic undermining, exploiting metacognitive processes to make us epistemically dependent on AI outputs. Current literature lacks systematic frameworks for distinguishing these possibilities.

### Doxastic Responsibility in Human-AI Partnerships

These cognitive mechanisms raise questions about epistemic and doxastic responsibility—whether we bear responsibility for beliefs formed through AI interaction. Naeem and Hauser (2024) argue that "epistemic integration" allows responsible AI-assisted belief formation when "agents are responsive to the reliability of their AI processes" (Naeem & Hauser 2024, 91). This requires maintaining metacognitive awareness of AI's role and actively evaluating reliability rather than passively accepting outputs.

Frontiers in Artificial Intelligence (2025) calls for "community standards for epistemic responsibility in human-AI collaborations," proposing "a positive code defining the proper use of AI in contexts where truth is to be discovered, preserved, extended, and communicated" (Frontiers AI 2025). The challenge lies in specifying when AI-mediated reasoning preserves sufficient human control to ground responsibility versus when AI systems assume roles that transfer or dissolve epistemic responsibility.

Niu et al.'s (2024) comprehensive review notes that "LLMs have shown capabilities that resemble human cognition, including GPT-3 exhibiting priming, distance, SNARC, and size congruity effects, as well as content effects in logical reasoning tasks similar to humans" (Niu et al. 2024, 15). This cognitive similarity raises the possibility that LLM-human interaction might mirror human-human epistemic exchange, with similar potential for both rational persuasion and strategic manipulation.

The cognitive mechanisms literature thus reveals sophisticated AI capabilities for engaging human reasoning while leaving unresolved whether such engagement preserves, enhances, or undermines epistemic agency. This ambiguity extends to scientific contexts, which we examine next as a potential model for epistemic partnerships.

---

**Word Count**: 902 words
