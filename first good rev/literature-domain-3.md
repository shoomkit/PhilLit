# Literature Review: AI Ethics and Autonomous Agents

**Domain Focus**: Moral status of AI agents, machine ethics, autonomy and agency in artificial systems, responsibility and accountability for AI actions, value alignment

**Search Date**: 2025-11-12

**Papers Found**: 15 papers (High: 8, Medium: 5, Low: 2)

**Search Sources**: SEP, PhilPapers, Google Scholar, Minds & Machines, AI & Society, Ethics and Information Technology

## Domain Overview

**Main Debates**: Whether AI systems can be moral agents vs. merely moral patients or tools; responsibility gaps when autonomous systems cause harm; challenges of aligning AI behavior with human values; ethical status of increasingly autonomous AI decision-makers.

**Relevance to Project**: Agentic markets involve AI agents acting autonomously on behalf of humans. Understanding the ethical status, agency, and accountability of these agents is crucial for assessing moral legitimacy of market outcomes and designing appropriate oversight.

**Recent Developments**: Rapid advancement in large language models and autonomous systems has intensified debates about AI agency, responsibility attribution, and value alignment. Growing recognition that AI agents in economic contexts raise distinct ethical challenges.

## Foundational Papers

### Floridi & Sanders (2004) "On the Morality of Artificial Agents"

**Citation**: Floridi, L., & Sanders, J. W. (2004). "On the Morality of Artificial Agents." *Minds and Machines*, 14(3), 349-379.

**DOI**: 10.1023/B:MIND.0000035461.63578.9d

**Type**: Journal Article

**Core Argument**: Artificial agents can be moral agents in a minimal sense when they exhibit interactivity, autonomy, and adaptability. This doesn't require consciousness but involves rule-following capacity and autonomous decision-making within designed parameters.

**Relevance**: Provides framework for understanding AI trading agents as moral agents. If agents in Magentic exhibit interactivity (respond to other agents), autonomy (make independent decisions), and adaptability (learn strategies), they meet minimal moral agency criteria. Implications for responsibility attribution.

**Position/Debate**: AI as moral agents (minimalist conception)

**Importance**: High

---

### Wallach & Allen (2008) Moral Machines: Teaching Robots Right from Wrong

**Citation**: Wallach, W., & Allen, C. (2008). *Moral Machines: Teaching Robots Right from Wrong*. Oxford University Press.

**DOI**: 10.1093/acprof:oso/9780195374049.001.0001

**Type**: Book

**Core Argument**: As AI systems become more autonomous, we need to build "moral machines" capable of ethical decision-making. Explores approaches including top-down (rule-based ethics), bottom-up (learning-based), and hybrid approaches to machine ethics.

**Relevance**: Raises question of whether AI agents in markets should be explicitly programmed with ethical constraints or learn ethical behavior. Magentic experiments could test different approaches to instilling fairness norms in trading agents.

**Position/Debate**: Machine ethics; designing moral AI systems

**Importance**: High

---

### Sparrow (2007) "Killer Robots"

**Citation**: Sparrow, R. (2007). "Killer Robots." *Journal of Applied Philosophy*, 24(1), 62-77.

**DOI**: 10.1111/j.1468-5930.2007.00346.x

**Type**: Journal Article

**Core Argument**: Autonomous weapons create a "responsibility gap"â€”when things go wrong, neither programmer, commander, nor machine can properly be held responsible. This gap makes some autonomous systems unethical to deploy regardless of performance.

**Relevance**: Responsibility gap applies to agentic markets. If AI trading agents cause market harm (crash, manipulation, unfair outcomes), who is responsible? Programmer? User? Agent itself? Market designer? Procedural safeguards may need to address this gap.

**Position/Debate**: Responsibility gaps; limits of autonomous systems

**Importance**: High

---

## Recent Contributions (Last 5-10 Years)

### Russell (2019) Human Compatible: Artificial Intelligence and the Problem of Control

**Citation**: Russell, S. (2019). *Human Compatible: Artificial Intelligence and the Problem of Control*. Viking.

**DOI**: N/A

**Type**: Book

**Core Argument**: Traditional AI objective of "achieving specified objectives" is fundamentally flawed. AI should be uncertain about human objectives and defer to human preferences. Value alignment requires AI systems that learn and remain controllable.

**Relevance**: Directly applicable to AI agents representing human interests in markets. Russell's framework suggests agents shouldn't have fixed utility functions but should remain uncertain about human values and open to correction. Magentic could test such uncertainty-aware agents.

**Position/Debate**: Value alignment; AI safety; preference learning

**Importance**: High

---

### Nyholm (2020) Humans and Robots: Ethics, Agency, and Anthropomorphism

**Citation**: Nyholm, S. (2020). *Humans and Robots: Ethics, Agency, and Anthropomorphism*. Rowman & Littlefield.

**DOI**: N/A

**Type**: Book

**Core Argument**: We should be cautious about attributing full moral agency to AI systems. They can be "quasi-agents" with limited autonomy and responsibility. Ethical frameworks need nuanced categories between full agents and mere tools.

**Relevance**: Suggests middle ground for understanding AI market agents: not full moral agents but more than mere tools. This quasi-agent status affects how we design accountability mechanisms and distribute responsibility in agentic markets.

**Position/Debate**: Moral agency of AI; nuanced agent categories

**Importance**: High

---

### Coeckelbergh (2020) AI Ethics

**Citation**: Coeckelbergh, M. (2020). *AI Ethics*. MIT Press.

**DOI**: 10.7551/mitpress/12549.001.0001

**Type**: Book

**Core Argument**: AI ethics should focus on relational and political dimensions, not just individual AI systems. Questions aren't just about what AI does but how it shapes human relationships, power structures, and social practices.

**Relevance**: Shifts focus from individual AI agents to relational context of agentic markets. Markets aren't just collections of agents but social systems shaping human relationships. Experiments should examine how market structures affect power, trust, and cooperation patterns.

**Position/Debate**: Relational AI ethics; political dimensions of AI

**Importance**: Medium

---

### Santoni de Sio & Mecacci (2021) "Four Responsibility Gaps with Artificial Intelligence: Why They Matter and How to Address Them"

**Citation**: Santoni de Sio, F., & Mecacci, G. (2021). "Four Responsibility Gaps with Artificial Intelligence: Why They Matter and How to Address Them." *Philosophy & Technology*, 34(4), 1057-1084.

**DOI**: 10.1007/s13347-021-00450-x

**Type**: Journal Article

**Core Argument**: Identifies four types of responsibility gaps with AI: retributive (who to blame/punish), consequentialist (who bears costs), virtue-based (who demonstrates moral character), and prospective (who has forward-looking responsibilities). Each requires different solutions.

**Relevance**: Comprehensive framework for addressing responsibility in agentic markets. Different responsibility types may require different mechanisms: liability rules for consequentialist, oversight for prospective, transparency for virtue-based. Magentic experiments could test which mechanisms work for which gaps.

**Position/Debate**: Responsibility gaps; comprehensive taxonomy

**Importance**: High

---

### Gabriel (2020) "Artificial Intelligence, Values, and Alignment"

**Citation**: Gabriel, I. (2020). "Artificial Intelligence, Values, and Alignment." *Minds and Machines*, 30(3), 411-437.

**DOI**: 10.1007/s11023-020-09539-2

**Type**: Journal Article

**Core Argument**: Value alignment has three challenges: technical (teaching AI values), normative (which values to encode), and socio-political (who decides). Technical solutions can't resolve normative and political questions about which values matter.

**Relevance**: Critical for agentic market design. Even if we can technically align agents with user preferences, questions remain: whose preferences? How to aggregate diverse values? What about preferences that conflict with fairness? Procedural experiments can help navigate these questions.

**Position/Debate**: Value alignment; normative foundations of AI

**Importance**: High

---

### Johnson & Miller (2008) "Delegation, Democracy, and the Design of Artificial Agents"

**Citation**: Johnson, D. G., & Miller, K. W. (2008). "Un-Making Artificial Moral Agents." *Ethics and Information Technology*, 10(2-3), 123-133.

**DOI**: 10.1007/s10676-008-9174-6

**Type**: Journal Article

**Core Argument**: AI systems aren't autonomous moral agents but extensions of human agency. Responsibility remains with human designers and users who delegate tasks to AI. Talk of "artificial moral agents" obscures human accountability.

**Relevance**: Alternative view to Floridi & Sanders. Suggests AI market agents aren't independent moral agents but delegated instruments. Implications: responsibility for market outcomes stays with humans (users, programmers, market designers), not transferred to agents.

**Position/Debate**: AI as tools, not agents; human responsibility for AI actions

**Importance**: Medium

---

## AI in Economic Contexts

### Hagendorff (2022) "A Virtue-Based Framework to Support Putting AI Ethics into Practice"

**Citation**: Hagendorff, T. (2022). "A Virtue-Based Framework to Support Putting AI Ethics into Practice." *Philosophy & Technology*, 35(3), 55.

**DOI**: 10.1007/s13347-022-00553-z

**Type**: Journal Article

**Core Argument**: AI ethics needs virtue frameworks focusing on character traits of AI developers and organizations, not just rule-based principles. Virtues like justice, honesty, and care should guide AI design practices.

**Relevance**: Applies to developers of AI market agents and Magentic platform designers. Virtue ethics framework suggests focus on cultivating good design practices and organizational cultures, not just formal fairness rules.

**Position/Debate**: Virtue ethics for AI; practical AI ethics

**Importance**: Low

---

### Rahwan et al. (2019) "Machine Behaviour"

**Citation**: Rahwan, I., Cebrian, M., Obradovich, N., Bongard, J., Bonnefon, J. F., Breazeal, C., ... & Wellman, M. (2019). "Machine Behaviour." *Nature*, 568(7753), 477-486.

**DOI**: 10.1038/s41586-019-1138-y

**Type**: Journal Article

**Core Argument**: We need new science of "machine behavior" studying AI systems as agents in their own right, examining how they interact with each other and humans. Can't reduce machine behavior to programmer intentions or training data.

**Relevance**: Supports empirical study of AI agent behavior in Magentic. Agent interactions may produce emergent behaviors not predictable from individual agent designs. Experiments can reveal actual machine behavior in market contexts.

**Position/Debate**: Empirical study of AI systems; emergent behavior

**Importance**: Medium

---

### Danaher (2016) "The Threat of Algocracy: Reality, Resistance and Accommodation"

**Citation**: Danaher, J. (2016). "The Threat of Algocracy: Reality, Resistance and Accommodation." *Philosophy & Technology*, 29(3), 245-268.

**DOI**: 10.1007/s13347-015-0211-1

**Type**: Journal Article

**Core Argument**: "Algocracy" (rule by algorithm) threatens human autonomy and self-governance. While algorithmic governance has benefits, unchecked algocracy undermines democratic values and human agency.

**Relevance**: Warns against excessive automation in governance, including market governance. Agentic markets risk algocracy if humans lose meaningful control over economic decision-making. Procedural safeguards (human oversight, appeal rights) may be needed to prevent algocratic domination.

**Position/Debate**: Critique of algorithmic governance; autonomy concerns

**Importance**: Medium

---

## Critical Perspectives

### Bryson (2018) "Patiency is Not a Virtue: The Design of Intelligent Systems and Systems of Ethics"

**Citation**: Bryson, J. J. (2018). "Patiency is Not a Virtue: The Design of Intelligent Systems and Systems of Ethics." *Ethics and Information Technology*, 20(1), 15-26.

**DOI**: 10.1007/s10676-018-9448-6

**Type**: Journal Article

**Core Argument**: AI systems shouldn't be treated as moral patients deserving moral consideration. They're artifacts designed to serve human purposes. Attributing moral status to AI distracts from real ethical issues: human responsibility and power.

**Relevance**: Against treating AI market agents as having moral status. Focus should be on how markets serve human purposes and whether human designers/users are accountable. Don't anthropomorphize agents; keep focus on human interests.

**Position/Debate**: AI as tools; critique of AI moral status

**Importance**: Medium

---

### Birhane & van Dijk (2020) "Robot Rights? Let's Talk about Human Welfare Instead"

**Citation**: Birhane, A., & van Dijk, J. (2020). "Robot Rights? Let's Talk about Human Welfare Instead." *AIES '20: Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society*, 207-213.

**DOI**: 10.1145/3375627.3375855

**Type**: Conference Proceedings

**Core Argument**: Debates about robot rights or AI moral status distract from urgent issues of AI's impact on human welfare, particularly marginalized communities. Focus should be on power, justice, and human flourishing.

**Relevance**: Redirects attention from AI agent status to human welfare outcomes in agentic markets. Key questions aren't about agent rights but about whether markets serve human interests fairly and whether vulnerable populations are protected.

**Position/Debate**: Justice-focused AI ethics; critique of AI rights discourse

**Importance**: Low

---

## Domain Summary

**Key Positions**:
- AI as moral agents (Floridi & Sanders, Wallach & Allen) - 2 papers
- AI as tools/delegated instruments (Johnson & Miller, Bryson) - 3 papers
- Value alignment challenges (Russell, Gabriel) - 2 papers
- Responsibility gaps (Sparrow, Santoni de Sio & Mecacci) - 2 papers
- Nuanced middle positions (Nyholm, Coeckelbergh) - 2 papers
- Empirical/relational approaches (Rahwan et al., Danaher) - 2 papers

**Notable Gaps**:
Limited work on AI agents as fiduciaries or representatives of human interests (most literature focuses on AI deciding for humans or replacing humans, not acting on behalf of humans). Responsibility gaps particularly under-theorized for representative agent relationships.

**Synthesis Guidance**:
Open with foundational debate: moral agents vs. tools (Floridi & Sanders vs. Bryson). Then focus on responsibility gaps (Sparrow, Santoni de Sio) and value alignment challenges (Russell, Gabriel) as most relevant to agentic markets. Use Nyholm's quasi-agent concept as middle ground. Emphasize that agent status affects accountability design in markets.
