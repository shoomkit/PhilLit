{
  "status": "success",
  "source": "semantic_scholar",
  "query": "weak to strong generalization",
  "results": [
    {
      "paperId": "6b97aa78bcdb88548c44e7e1671c0ed37ed37976",
      "title": "Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision",
      "authors": [
        {
          "name": "Collin Burns",
          "authorId": "90909974"
        },
        {
          "name": "Pavel Izmailov",
          "authorId": "7991830"
        },
        {
          "name": "J. Kirchner",
          "authorId": "2274766352"
        },
        {
          "name": "Bowen Baker",
          "authorId": "2274766907"
        },
        {
          "name": "Leo Gao",
          "authorId": "2027599537"
        },
        {
          "name": "Leopold Aschenbrenner",
          "authorId": "2274764580"
        },
        {
          "name": "Yining Chen",
          "authorId": "2274810643"
        },
        {
          "name": "Adrien Ecoffet",
          "authorId": "66821245"
        },
        {
          "name": "Manas R. Joglekar",
          "authorId": "2185778"
        },
        {
          "name": "Jan Leike",
          "authorId": "2990741"
        },
        {
          "name": "I. Sutskever",
          "authorId": "1701686"
        },
        {
          "name": "Jeff Wu",
          "authorId": "2274911253"
        },
        {
          "name": "OpenAI",
          "authorId": "2274764845"
        }
      ],
      "year": 2023,
      "abstract": "Widely used alignment techniques, such as reinforcement learning from human feedback (RLHF), rely on the ability of humans to supervise model behavior - for example, to evaluate whether a model faithfully followed instructions or generated safe outputs. However, future superhuman models will behave in complex ways too difficult for humans to reliably evaluate; humans will only be able to weakly supervise superhuman models. We study an analogy to this problem: can weak model supervision elicit the full capabilities of a much stronger model? We test this using a range of pretrained language models in the GPT-4 family on natural language processing (NLP), chess, and reward modeling tasks. We find that when we naively finetune strong pretrained models on labels generated by a weak model, they consistently perform better than their weak supervisors, a phenomenon we call weak-to-strong generalization. However, we are still far from recovering the full capabilities of strong models with naive finetuning alone, suggesting that techniques like RLHF may scale poorly to superhuman models without further work. We find that simple methods can often significantly improve weak-to-strong generalization: for example, when finetuning GPT-4 with a GPT-2-level supervisor and an auxiliary confidence loss, we can recover close to GPT-3.5-level performance on NLP tasks. Our results suggest that it is feasible to make empirical progress today on a fundamental challenge of aligning superhuman models.",
      "citationCount": 386,
      "doi": "10.48550/arXiv.2312.09390",
      "arxivId": "2312.09390",
      "url": "https://www.semanticscholar.org/paper/6b97aa78bcdb88548c44e7e1671c0ed37ed37976",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2312.09390"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "b7c6f23623661e64cb3ff0c7aaebdcbf92156622",
      "title": "Weak to Strong Generalization for Large Language Models with Multi-capabilities",
      "authors": [
        {
          "name": "Yucheng Zhou",
          "authorId": "2110348767"
        },
        {
          "name": "Jianbing Shen",
          "authorId": "2266802577"
        },
        {
          "name": "Yu Cheng",
          "authorId": "2361825415"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 67,
      "doi": null,
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/b7c6f23623661e64cb3ff0c7aaebdcbf92156622",
      "venue": "International Conference on Learning Representations",
      "journal": null,
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "50d5ef4d95aa4127f982812fe108298f54eaea01",
      "title": "Theoretical Analysis of Weak-to-Strong Generalization",
      "authors": [
        {
          "name": "Hunter Lang",
          "authorId": "30155522"
        },
        {
          "name": "David Sontag",
          "authorId": "2266751797"
        },
        {
          "name": "Aravindan Vijayaraghavan",
          "authorId": "2285908399"
        }
      ],
      "year": 2024,
      "abstract": "Strong student models can learn from weaker teachers: when trained on the predictions of a weaker model, a strong pretrained student can learn to correct the weak model's errors and generalize to examples where the teacher is not confident, even when these examples are excluded from training. This enables learning from cheap, incomplete, and possibly incorrect label information, such as coarse logical rules or the generations of a language model. We show that existing weak supervision theory fails to account for both of these effects, which we call pseudolabel correction and coverage expansion, respectively. We give a new bound based on expansion properties of the data distribution and student hypothesis class that directly accounts for pseudolabel correction and coverage expansion. Our bounds capture the intuition that weak-to-strong generalization occurs when the strong model is unable to fit the mistakes of the weak teacher without incurring additional error. We show that these expansion properties can be checked from finite data and give empirical evidence that they hold in practice.",
      "citationCount": 38,
      "doi": "10.48550/arXiv.2405.16043",
      "arxivId": "2405.16043",
      "url": "https://www.semanticscholar.org/paper/50d5ef4d95aa4127f982812fe108298f54eaea01",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2405.16043"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "60f066b3d7391dc5da3e3638970fd00f1aadebdf",
      "title": "Co-Supervised Learning: Improving Weak-to-Strong Generalization with Hierarchical Mixture of Experts",
      "authors": [
        {
          "name": "Yuejiang Liu",
          "authorId": "14772333"
        },
        {
          "name": "Alexandre Alahi",
          "authorId": "2266840641"
        }
      ],
      "year": 2024,
      "abstract": "Steering the behavior of a strong model pre-trained on internet-scale data can be difficult due to the scarcity of competent supervisors. Recent studies reveal that, despite supervisory noises, a strong student model may surpass its weak teacher when fine-tuned on specific objectives. Yet, the effectiveness of such weak-to-strong generalization remains limited, especially in the presence of large capability gaps. In this paper, we propose to address this challenge by harnessing a diverse set of specialized teachers, instead of a single generalist one, that collectively supervises the strong student. Our approach resembles the classical hierarchical mixture of experts, with two components tailored for co-supervision: (i) we progressively alternate student training and teacher assignment, leveraging the growth of the strong student to identify plausible supervisions; (ii) we conservatively enforce teacher-student and local-global consistency, leveraging their dependencies to reject potential annotation noises. We validate the proposed method through visual recognition tasks on the OpenAI weak-to-strong benchmark and additional multi-domain datasets. Our code is available at \\url{https://github.com/yuejiangliu/csl}.",
      "citationCount": 29,
      "doi": "10.48550/arXiv.2402.15505",
      "arxivId": "2402.15505",
      "url": "https://www.semanticscholar.org/paper/60f066b3d7391dc5da3e3638970fd00f1aadebdf",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.15505"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "098eda60ecd7b951d50d85280d629c00c949e125",
      "title": "Quantifying the Gain in Weak-to-Strong Generalization",
      "authors": [
        {
          "name": "Moses Charikar",
          "authorId": "2249049090"
        },
        {
          "name": "Chirag Pabbaraju",
          "authorId": "52210179"
        },
        {
          "name": "Kirankumar Shiragur",
          "authorId": "2581932"
        }
      ],
      "year": 2024,
      "abstract": "Recent advances in large language models have shown capabilities that are extraordinary and near-superhuman. These models operate with such complexity that reliably evaluating and aligning them proves challenging for humans. This leads to the natural question: can guidance from weak models (like humans) adequately direct the capabilities of strong models? In a recent and somewhat surprising work, Burns et al. (2023) empirically demonstrated that when strong models (like GPT-4) are finetuned using labels generated by weak supervisors (like GPT-2), the strong models outperform their weaker counterparts -- a phenomenon they term weak-to-strong generalization. In this work, we present a theoretical framework for understanding weak-to-strong generalization. Specifically, we show that the improvement in performance achieved by strong models over their weaker counterparts is quantified by the misfit error incurred by the strong model on labels generated by the weaker model. Our theory reveals several curious algorithmic insights. For instance, we can predict the amount by which the strong model will improve over the weak model, and also choose among different weak models to train the strong model, based on its misfit error. We validate our theoretical findings through various empirical assessments.",
      "citationCount": 28,
      "doi": "10.48550/arXiv.2405.15116",
      "arxivId": "2405.15116",
      "url": "https://www.semanticscholar.org/paper/098eda60ecd7b951d50d85280d629c00c949e125",
      "venue": "Neural Information Processing Systems",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2405.15116"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "55eaf0237c60428e59abb02366cbea3dd9070672",
      "title": "Vision Superalignment: Weak-to-Strong Generalization for Vision Foundation Models",
      "authors": [
        {
          "name": "Jianyuan Guo",
          "authorId": "2148899357"
        },
        {
          "name": "Hanting Chen",
          "authorId": "2118023932"
        },
        {
          "name": "Chengcheng Wang",
          "authorId": "2243437549"
        },
        {
          "name": "Kai Han",
          "authorId": "2277423285"
        },
        {
          "name": "Chang Xu",
          "authorId": "2271675319"
        },
        {
          "name": "Yunhe Wang",
          "authorId": "2354961469"
        }
      ],
      "year": 2024,
      "abstract": "Recent advancements in large language models have sparked interest in their extraordinary and near-superhuman capabilities, leading researchers to explore methods for evaluating and optimizing these abilities, which is called superalignment. In this context, our paper delves into the realm of vision foundation models, focusing on the concept of weak-to-strong generalization, which involves using a weaker model to supervise a stronger one, aiming to enhance the latter's capabilities beyond the former's limits. We introduce a novel and adaptively adjustable loss function for weak-to-strong supervision. Our comprehensive experiments span various scenarios, including few-shot learning, transfer learning, noisy label learning, and common knowledge distillation settings. The results are striking: our approach not only exceeds the performance benchmarks set by strong-to-strong generalization but also surpasses the outcomes of fine-tuning strong models with whole datasets. This compelling evidence underscores the significant potential of weak-to-strong generalization, showcasing its capability to substantially elevate the performance of vision foundation models. The code is available at https://github.com/ggjy/vision_weak_to_strong.",
      "citationCount": 26,
      "doi": "10.48550/arXiv.2402.03749",
      "arxivId": "2402.03749",
      "url": "https://www.semanticscholar.org/paper/55eaf0237c60428e59abb02366cbea3dd9070672",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.03749"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "76fc00df28701f3d9d683157352b9ef6b0776c54",
      "title": "Understanding the Capabilities and Limitations of Weak-to-Strong Generalization",
      "authors": [
        {
          "name": "Wei Yao",
          "authorId": "2279748096"
        },
        {
          "name": "Wenkai Yang",
          "authorId": "2394712981"
        },
        {
          "name": "Ziqiao Wang",
          "authorId": "2364644096"
        },
        {
          "name": "Yankai Lin",
          "authorId": "2307480156"
        },
        {
          "name": "Yong Liu",
          "authorId": "2344265275"
        }
      ],
      "year": 2025,
      "abstract": null,
      "citationCount": 6,
      "doi": "10.48550/arXiv.2502.01458",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/76fc00df28701f3d9d683157352b9ef6b0776c54",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.01458"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "21f080943d2a4775bf807962da09b0aaeeaa93e4",
      "title": "Discrepancies are Virtue: Weak-to-Strong Generalization through Lens of Intrinsic Dimension",
      "authors": [
        {
          "name": "Yijun Dong",
          "authorId": "2115906260"
        },
        {
          "name": "Yicheng Li",
          "authorId": "2344689361"
        },
        {
          "name": "Yunai Li",
          "authorId": "2344647923"
        },
        {
          "name": "Jason D. Lee",
          "authorId": "2281752823"
        },
        {
          "name": "Qi Lei",
          "authorId": "2281745016"
        }
      ],
      "year": 2025,
      "abstract": "Weak-to-strong (W2S) generalization is a type of finetuning (FT) where a strong (large) student model is trained on pseudo-labels generated by a weak teacher. Surprisingly, W2S FT often outperforms the weak teacher. We seek to understand this phenomenon through the observation that FT often occurs in intrinsically low-dimensional spaces. Leveraging the low intrinsic dimensionality of FT, we analyze W2S in the ridgeless regression setting from a variance reduction perspective. For a strong student-weak teacher pair with sufficiently expressive low-dimensional feature subspaces $\\mathcal{V}_s, \\mathcal{V}_w$, we provide an exact characterization of the variance that dominates the generalization error of W2S. This unveils a virtue of discrepancy between the strong and weak models in W2S: the variance of the weak teacher is inherited by the strong student in $\\mathcal{V}_s \\cap \\mathcal{V}_w$, while reduced by a factor of $\\mathrm{dim}(\\mathcal{V}_s)/N$ in the subspace of discrepancy $\\mathcal{V}_w \\setminus \\mathcal{V}_s$ with $N$ pseudo-labels for W2S. Our analysis further casts light on the sample complexities and the scaling of performance gap recovery in W2S. The analysis is supported by experiments on synthetic regression problems, as well as real vision and NLP tasks.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2502.05075",
      "arxivId": "2502.05075",
      "url": "https://www.semanticscholar.org/paper/21f080943d2a4775bf807962da09b0aaeeaa93e4",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.05075"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "c27bd0acf8bd09575360d746eaa5fba476cace9e",
      "title": "Relating Misfit to Gain in Weak-to-Strong Generalization Beyond the Squared Loss",
      "authors": [
        {
          "name": "Abhijeet Mulgund",
          "authorId": "2260862806"
        },
        {
          "name": "Chirag Pabbaraju",
          "authorId": "52210179"
        }
      ],
      "year": 2025,
      "abstract": "The paradigm of weak-to-strong generalization constitutes the training of a strong AI model on data labeled by a weak AI model, with the goal that the strong model nevertheless outperforms its weak supervisor on the target task of interest. For the setting of real-valued regression with the squared loss, recent work quantitatively characterizes the gain in performance of the strong model over the weak model in terms of the misfit between the strong and weak model. We generalize such a characterization to learning tasks whose loss functions correspond to arbitrary Bregman divergences when the strong class is convex. This extends the misfit-based characterization of performance gain in weak-to-strong generalization to classification tasks, as the cross-entropy loss can be expressed in terms of a Bregman divergence. In most practical scenarios, however, the strong model class may not be convex. We therefore weaken this assumption and study weak-to-strong generalization for convex combinations of $k$ strong models in the strong class, in the concrete setting of classification. This allows us to obtain a similar misfit-based characterization of performance gain, upto an additional error term that vanishes as $k$ gets large. Our theoretical findings are supported by thorough experiments on synthetic as well as real-world datasets.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2501.19105",
      "arxivId": "2501.19105",
      "url": "https://www.semanticscholar.org/paper/c27bd0acf8bd09575360d746eaa5fba476cace9e",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2501.19105"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "f713b439266e6cc1415025da056f1304f2b78b8a",
      "title": "Debate Helps Weak-to-Strong Generalization",
      "authors": [
        {
          "name": "Hao Lang",
          "authorId": "98256342"
        },
        {
          "name": "Fei Huang",
          "authorId": "2257407873"
        },
        {
          "name": "Yongbin Li",
          "authorId": "1527090216"
        }
      ],
      "year": 2025,
      "abstract": "Common methods for aligning already-capable models with desired behavior rely on the ability of humans to provide supervision.\nHowever, future superhuman models will surpass the capability of humans.\nTherefore, humans will only be able to weakly supervise superhuman models.\nThis expected deficiency of human evaluation would weaken the safety of future AI systems.\nScalable oversight and weak-to-strong generalization are two complementary approaches to tackle this issue.\nIn this paper, we attempt to combine the strengths of these two approaches to further improve alignment.\nSpecifically, we investigate ways of improving human supervision with a strong pretrained model and then supervise the strong model with enhanced weak human supervision.\nTo make iterative empirical progress, we consider an analogy: can we use a strong model to improve weak model supervision and then use it to supervise the strong model?\nWe empirically test it by finetuning a small weak model on ground truth labels with the additional help from a large strong model, and then finetuning the strong model on labels generated by the weak model.\nWe find that debate can assist a weak model in extracting trustworthy information from an untrustworthy strong model, which provides leverage as context on samples when training a weak model.\nWe also show that an ensemble of weak models helps exploit long arguments generated by strong model debaters and obtain a more robust supervision estimate.\nExtensive experiments on the OpenAI weak-to-strong NLP benchmarks show that the combination approach leads to better alignment, which indicates that debate has the potential to help weak-to-strong generalization.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2501.13124",
      "arxivId": "2501.13124",
      "url": "https://www.semanticscholar.org/paper/f713b439266e6cc1415025da056f1304f2b78b8a",
      "venue": "AAAI Conference on Artificial Intelligence",
      "journal": {
        "pages": "27410-27418"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "8642fabac14df193315483ca40cc06c06a9bd15f",
      "title": "Weak-to-Strong Generalization Even in Random Feature Networks, Provably",
      "authors": [
        {
          "name": "Marko Medvedev",
          "authorId": "2320150991"
        },
        {
          "name": "Kaifeng Lyu",
          "authorId": "41049476"
        },
        {
          "name": "Dingli Yu",
          "authorId": "2141026731"
        },
        {
          "name": "Sanjeev Arora",
          "authorId": "2261737783"
        },
        {
          "name": "Zhiyuan Li",
          "authorId": "46947755"
        },
        {
          "name": "Nathan Srebro",
          "authorId": "2288280083"
        }
      ],
      "year": 2025,
      "abstract": "Weak-to-Strong Generalization (Burns et al., 2024) is the phenomenon whereby a strong student, say GPT-4, learns a task from a weak teacher, say GPT-2, and ends up significantly outperforming the teacher. We show that this phenomenon does not require a strong learner like GPT-4. We consider student and teacher that are random feature models, described by two-layer networks with a random and fixed bottom layer and a trained top layer. A\"weak\"teacher, with a small number of units (i.e. random features), is trained on the population, and a\"strong\"student, with a much larger number of units (i.e. random features), is trained only on labels generated by the weak teacher. We demonstrate, prove, and understand how the student can outperform the teacher, even though trained only on data labeled by the teacher. We also explain how such weak-to-strong generalization is enabled by early stopping. Importantly, we also show the quantitative limits of weak-to-strong generalization in this model.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2503.02877",
      "arxivId": "2503.02877",
      "url": "https://www.semanticscholar.org/paper/8642fabac14df193315483ca40cc06c06a9bd15f",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.02877"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "82ee8994ccbbf2db5866258cdb83c1626dedf0c0",
      "title": "Representations Shape Weak-to-Strong Generalization: Theoretical Insights and Empirical Predictions",
      "authors": [
        {
          "name": "Yihao Xue",
          "authorId": "150353219"
        },
        {
          "name": "Jiping Li",
          "authorId": "2343667005"
        },
        {
          "name": "Baharan Mirzasoleiman",
          "authorId": "2389094"
        }
      ],
      "year": 2025,
      "abstract": "Weak-to-Strong Generalization (W2SG), where a weak model supervises a stronger one, serves as an important analogy for understanding how humans might guide superhuman intelligence in the future. Promising empirical results revealed that a strong model can surpass its weak supervisor. While recent work has offered theoretical insights into this phenomenon, a clear understanding of the interactions between weak and strong models that drive W2SG remains elusive. We investigate W2SG through a theoretical lens and show that it can be characterized using kernels derived from the principal components of weak and strong models' internal representations. These kernels can be used to define a space that, at a high level, captures what the weak model is unable to learn but is learnable by the strong model. The projection of labels onto this space quantifies how much the strong model falls short of its full potential due to weak supervision. This characterization also provides insights into how certain errors in weak supervision can be corrected by the strong model, regardless of overfitting. Our theory has significant practical implications, providing a representation-based metric that predicts W2SG performance trends without requiring labels, as shown in experiments on molecular predictions with transformers and 5 NLP tasks involving 52 LLMs.",
      "citationCount": 4,
      "doi": "10.48550/arXiv.2502.00620",
      "arxivId": "2502.00620",
      "url": "https://www.semanticscholar.org/paper/82ee8994ccbbf2db5866258cdb83c1626dedf0c0",
      "venue": "International Conference on Machine Learning",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2502.00620"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "89481e9bc2eb0396003c26b8f5640718f99894fb",
      "title": "Revisiting Weak-to-Strong Generalization in Theory and Practice: Reverse KL vs. Forward KL",
      "authors": [
        {
          "name": "Wei Yao",
          "authorId": "2279748096"
        },
        {
          "name": "Wenkai Yang",
          "authorId": "2394712981"
        },
        {
          "name": "Ziqiao Wang",
          "authorId": "2117428102"
        },
        {
          "name": "Yankai Lin",
          "authorId": "2307480156"
        },
        {
          "name": "Yong Liu",
          "authorId": "2344265275"
        }
      ],
      "year": 2025,
      "abstract": "As large language models advance toward superhuman performance, ensuring their alignment with human values and abilities grows increasingly complex. Weak-to-strong generalization offers a promising approach by leveraging predictions from weaker models to guide stronger systems, but its effectiveness could be constrained by the inherent noise and inaccuracies in these weak predictions. To address this, we propose a theoretically grounded approach that replaces forward KL divergence-whose mass-covering behavior risks overfitting to imperfect weak signals-with reverse KL divergence. Reverse KL divergence's zero-forcing effect prioritizes high-confidence predictions, effectively mitigating the influence of unreliable weak supervision. Theoretically, we extend existing bounds and derive tighter lower bounds for both forward and reverse KL divergence, establishing that reverse KL achieves at least comparable guarantees to forward KL. Notably, when a sufficiently pre-trained strong model is fine-tuned on the last linear layer, reverse KL guarantees that it outperforms its weak supervisor by the magnitude of their disagreement. Empirically, we demonstrate that reverse KL and reverse cross-entropy enable strong models to successfully outperform those trained with forward KL and standard cross-entropy across most settings, highlighting the practical advantages of these reverse losses.",
      "citationCount": 3,
      "doi": "10.48550/arXiv.2502.11107",
      "arxivId": "2502.11107",
      "url": "https://www.semanticscholar.org/paper/89481e9bc2eb0396003c26b8f5640718f99894fb",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "pages": "2860-2888"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "58e527b45daa80191083937d8da2cfd4be9f00b3",
      "title": "Improving Weak-to-Strong Generalization with Scalable Oversight and Ensemble Learning",
      "authors": [
        {
          "name": "Jitao Sang",
          "authorId": "2266388216"
        },
        {
          "name": "Yuhang Wang",
          "authorId": "2266722675"
        },
        {
          "name": "Jing Zhang",
          "authorId": "2266777463"
        },
        {
          "name": "Yanxu Zhu",
          "authorId": "2268499891"
        },
        {
          "name": "Chao Kong",
          "authorId": "2218906158"
        },
        {
          "name": "Junhong Ye",
          "authorId": "2282448323"
        },
        {
          "name": "Shuyu Wei",
          "authorId": "2268518561"
        },
        {
          "name": "Jinlin Xiao",
          "authorId": "2282252674"
        }
      ],
      "year": 2024,
      "abstract": "This paper presents a follow-up study to OpenAI's recent superalignment work on Weak-to-Strong Generalization (W2SG). Superalignment focuses on ensuring that high-level AI systems remain consistent with human values and intentions when dealing with complex, high-risk tasks. The W2SG framework has opened new possibilities for empirical research in this evolving field. Our study simulates two phases of superalignment under the W2SG framework: the development of general superhuman models and the progression towards superintelligence. In the first phase, based on human supervision, the quality of weak supervision is enhanced through a combination of scalable oversight and ensemble learning, reducing the capability gap between weak teachers and strong students. In the second phase, an automatic alignment evaluator is employed as the weak supervisor. By recursively updating this auto aligner, the capabilities of the weak teacher models are synchronously enhanced, achieving weak-to-strong supervision over stronger student models.We also provide an initial validation of the proposed approach for the first phase. Using the SciQ task as example, we explore ensemble learning for weak teacher models through bagging and boosting. Scalable oversight is explored through two auxiliary settings: human-AI interaction and AI-AI debate. Additionally, the paper discusses the impact of improved weak supervision on enhancing weak-to-strong generalization based on in-context learning. Experiment code and dataset will be released at https://github.com/ADaM-BJTU/W2SG.",
      "citationCount": 17,
      "doi": "10.48550/arXiv.2402.00667",
      "arxivId": "2402.00667",
      "url": "https://www.semanticscholar.org/paper/58e527b45daa80191083937d8da2cfd4be9f00b3",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2402.00667"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "a477ee76ed952429e106a871ae0790b6a2f5acd8",
      "title": "Super(ficial)-alignment: Strong Models May Deceive Weak Models in Weak-to-Strong Generalization",
      "authors": [
        {
          "name": "Wenkai Yang",
          "authorId": "2120801160"
        },
        {
          "name": "Shiqi Shen",
          "authorId": "2241038698"
        },
        {
          "name": "Guangyao Shen",
          "authorId": "2306953653"
        },
        {
          "name": "Zhi Gong",
          "authorId": "2240547021"
        },
        {
          "name": "Yankai Lin",
          "authorId": "2307480156"
        }
      ],
      "year": 2024,
      "abstract": "Superalignment, where humans act as weak supervisors for superhuman models, has become a crucial problem with the rapid development of Large Language Models (LLMs). Recent work has preliminarily studied this problem by using weak models to supervise strong models, and discovered that weakly supervised strong students can consistently outperform weak teachers towards the alignment target, leading to a weak-to-strong generalization phenomenon. However, we are concerned that behind such a promising phenomenon, whether there exists an issue of weak-to-strong deception, where strong models deceive weak models by exhibiting well-aligned in areas known to weak models but producing misaligned behaviors in cases weak models do not know. We take an initial step towards exploring this security issue in a specific but realistic multi-objective alignment case, where there may be some alignment targets conflicting with each other (e.g., helpfulness v.s. harmlessness). We aim to explore whether, in such cases, strong models might deliberately make mistakes in areas known to them but unknown to weak models within one alignment dimension, in exchange for a higher reward in another dimension. Through extensive experiments in both the reward modeling and preference optimization scenarios, we find: (1) The weak-to-strong deception phenomenon exists across all settings. (2) The deception intensifies as the capability gap between weak and strong models increases. (3) Bootstrapping with an intermediate model can mitigate the deception to some extent, though its effectiveness remains limited. Our work highlights the urgent need to pay more attention to the true reliability of superalignment.",
      "citationCount": 20,
      "doi": "10.48550/arXiv.2406.11431",
      "arxivId": "2406.11431",
      "url": "https://www.semanticscholar.org/paper/a477ee76ed952429e106a871ae0790b6a2f5acd8",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.11431"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "b708b1d3956d5a46a6c7d6da2782ee6dedefa611",
      "title": "On the Mechanisms of Weak-to-Strong Generalization: A Theoretical Perspective",
      "authors": [
        {
          "name": "Behrad Moniri",
          "authorId": "2090616450"
        },
        {
          "name": "Hamed Hassani",
          "authorId": "2333895132"
        }
      ],
      "year": 2025,
      "abstract": "Weak-to-strong generalization, where a student model trained on imperfect labels generated by a weaker teacher nonetheless surpasses that teacher, has been widely observed but the mechanisms that enable it have remained poorly understood. In this paper, through a theoretical analysis of simple models, we uncover three core mechanisms that can drive this phenomenon. First, by analyzing ridge regression, we study the interplay between the teacher and student regularization and prove that a student can compensate for a teacher's under-regularization and achieve lower test error. We also analyze the role of the parameterization regime of the models. Second, by analyzing weighted ridge regression, we show that a student model with a regularization structure more aligned to the target, can outperform its teacher. Third, in a nonlinear multi-index setting, we demonstrate that a student can learn easy, task-specific features from the teacher while leveraging its own broader pre-training to learn hard-to-learn features that the teacher cannot capture.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2505.18346",
      "arxivId": "2505.18346",
      "url": "https://www.semanticscholar.org/paper/b708b1d3956d5a46a6c7d6da2782ee6dedefa611",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.18346"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ba85cd62e7e3fec44589078d1192ebf445d39dd3",
      "title": "From Linear to Nonlinear: Provable Weak-to-Strong Generalization through Feature Learning",
      "authors": [
        {
          "name": "Junsoo Oh",
          "authorId": "2329533086"
        },
        {
          "name": "Jerry Song",
          "authorId": "2389351063"
        },
        {
          "name": "Chulhee Yun",
          "authorId": "2328409982"
        }
      ],
      "year": 2025,
      "abstract": "Weak-to-strong generalization refers to the phenomenon where a stronger model trained under supervision from a weaker one can outperform its teacher. While prior studies aim to explain this effect, most theoretical insights are limited to abstract frameworks or linear/random feature models. In this paper, we provide a formal analysis of weak-to-strong generalization from a linear CNN (weak) to a two-layer ReLU CNN (strong). We consider structured data composed of label-dependent signals of varying difficulty and label-independent noise, and analyze gradient descent dynamics when the strong model is trained on data labeled by the pretrained weak model. Our analysis identifies two regimes -- data-scarce and data-abundant -- based on the signal-to-noise characteristics of the dataset, and reveals distinct mechanisms of weak-to-strong generalization. In the data-scarce regime, generalization occurs via benign overfitting or fails via harmful overfitting, depending on the amount of data, and we characterize the transition boundary. In the data-abundant regime, generalization emerges in the early phase through label correction, but we observe that overtraining can subsequently degrade performance.",
      "citationCount": 2,
      "doi": "10.48550/arXiv.2510.24812",
      "arxivId": "2510.24812",
      "url": "https://www.semanticscholar.org/paper/ba85cd62e7e3fec44589078d1192ebf445d39dd3",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2510.24812"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "2775d04270f92bcaa597d6010649bfff1a834f47",
      "title": "Strong Empowered and Aligned Weak Mastered Annotation for Weak-to-Strong Generalization",
      "authors": [
        {
          "name": "Yongqi Li",
          "authorId": "2243469274"
        },
        {
          "name": "Xin Miao",
          "authorId": "2215546811"
        },
        {
          "name": "Mayi Xu",
          "authorId": "2243655194"
        },
        {
          "name": "Tieyun Qian",
          "authorId": "2243466068"
        }
      ],
      "year": 2025,
      "abstract": "The super-alignment problem of how humans can effectively supervise super-human AI has garnered increasing attention. Recent research has focused on investigating the weak-to-strong generalization (W2SG) scenario as an analogy for super-alignment. This scenario examines how a pre-trained strong model, supervised by an aligned weak model, can outperform its weak supervisor. Despite good progress, current W2SG methods face two main issues: 1) The annotation quality is limited by the knowledge scope of the weak model; 2) It is risky to position the strong model as the final corrector.\nTo tackle these issues, we propose a ``Strong Empowered and Aligned Weak Mastered'' (SEAM) framework for weak annotations in W2SG. This framework can leverage the vast intrinsic knowledge of the pre-trained strong model to empower the annotation and position the aligned weak model as the annotation master. Specifically, the pre-trained strong model first generates principle fast-and-frugal trees for samples to be annotated, encapsulating rich sample-related knowledge. Then, the aligned weak model picks informative nodes based on the tree's information distribution for final annotations. Experiments on six datasets for preference tasks in W2SG scenarios validate the effectiveness of our proposed method.",
      "citationCount": 2,
      "doi": "10.1609/aaai.v39i26.34955",
      "arxivId": null,
      "url": "https://www.semanticscholar.org/paper/2775d04270f92bcaa597d6010649bfff1a834f47",
      "venue": "AAAI Conference on Artificial Intelligence",
      "journal": {
        "pages": "27437-27445"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "1f7fdff94dfc5fc0c8433c14f85ededed6a8531b",
      "title": "On the Emergence of Weak-to-Strong Generalization: A Bias-Variance Perspective",
      "authors": [
        {
          "name": "Gengze Xu",
          "authorId": "2324058881"
        },
        {
          "name": "Wei Yao",
          "authorId": "2279748096"
        },
        {
          "name": "Ziqiao Wang",
          "authorId": "2364644096"
        },
        {
          "name": "Yong Liu",
          "authorId": "2344265275"
        }
      ],
      "year": 2025,
      "abstract": "Weak-to-strong generalization (W2SG) refers to the phenomenon where a strong student model, trained on a dataset labeled by a weak teacher, ultimately outperforms the teacher on the target task. Recent studies attribute this performance gain to the prediction misfit between the student and teacher models. In this work, we theoretically investigate the emergence of W2SG through a generalized bias-variance decomposition of Bregman divergence. Specifically, we show that the expected population risk gap between the student and teacher is quantified by the expected misfit between the two models. While this aligns with previous results, our analysis removes several restrictive assumptions, most notably, the convexity of the student's hypothesis class, required in earlier works. Moreover, we show that W2SG is more likely to emerge when the student model approximates its posterior mean teacher, rather than mimicking an individual teacher. Using a concrete example, we demonstrate that if the student model size is sufficiently large, it can indeed converge to the posterior mean teacher in expectation. Our analysis also suggests that avoiding overfitting to the teacher's supervision and reducing the entropy of student's prediction further facilitate W2SG. In addition, we show that the reverse cross-entropy loss, unlike the standard forward cross-entropy, is less sensitive to the predictive uncertainty of the teacher. Finally, we empirically verify our theoretical insights and demonstrate that incorporating the reverse cross-entropy loss consistently improves student performance.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2505.24313",
      "arxivId": "2505.24313",
      "url": "https://www.semanticscholar.org/paper/1f7fdff94dfc5fc0c8433c14f85ededed6a8531b",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.24313"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "4edc2381bc31042d40c1ee9251e324cf1c10e611",
      "title": "How to Mitigate Overfitting in Weak-to-strong Generalization?",
      "authors": [
        {
          "name": "Junhao Shi",
          "authorId": "2348888995"
        },
        {
          "name": "Qinyuan Cheng",
          "authorId": "1834133"
        },
        {
          "name": "Zhaoye Fei",
          "authorId": "2132200788"
        },
        {
          "name": "Y. Zheng",
          "authorId": "3337238"
        },
        {
          "name": "Qipeng Guo",
          "authorId": "153683057"
        },
        {
          "name": "Xipeng Qiu",
          "authorId": "2350155100"
        }
      ],
      "year": 2025,
      "abstract": "Aligning powerful AI models on tasks that surpass human evaluation capabilities is the central problem of \\textbf{superalignment}. To address this problem, weak-to-strong generalization aims to elicit the capabilities of strong models through weak supervisors and ensure that the behavior of strong models aligns with the intentions of weak supervisors without unsafe behaviors such as deception. Although weak-to-strong generalization exhibiting certain generalization capabilities, strong models exhibit significant overfitting in weak-to-strong generalization: Due to the strong fit ability of strong models, erroneous labels from weak supervisors may lead to overfitting in strong models. In addition, simply filtering out incorrect labels may lead to a degeneration in question quality, resulting in a weak generalization ability of strong models on hard questions. To mitigate overfitting in weak-to-strong generalization, we propose a two-stage framework that simultaneously improves the quality of supervision signals and the quality of input questions. Experimental results in three series of large language models and two mathematical benchmarks demonstrate that our framework significantly improves PGR compared to naive weak-to-strong generalization, even achieving up to 100\\% PGR on some models.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2503.04249",
      "arxivId": "2503.04249",
      "url": "https://www.semanticscholar.org/paper/4edc2381bc31042d40c1ee9251e324cf1c10e611",
      "venue": "Annual Meeting of the Association for Computational Linguistics",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2503.04249"
      },
      "publicationTypes": [
        "JournalArticle",
        "Conference"
      ]
    },
    {
      "paperId": "06a6418abcc2ca0595971a6cfdebc9272d18bcf4",
      "title": "Limitations of refinement methods for weak to strong generalization",
      "authors": [
        {
          "name": "Seamus Somerstep",
          "authorId": "102596550"
        },
        {
          "name": "Ya'acov Ritov",
          "authorId": "2240337985"
        },
        {
          "name": "Mikhail Yurochkin",
          "authorId": "2345005591"
        },
        {
          "name": "Subha Maity",
          "authorId": "1471072023"
        },
        {
          "name": "Yuekai Sun",
          "authorId": "2247880508"
        }
      ],
      "year": 2025,
      "abstract": "Standard techniques for aligning large language models (LLMs) utilize human-produced data, which could limit the capability of any aligned LLM to human level. Label refinement and weak training have emerged as promising strategies to address this superalignment problem. In this work, we adopt probabilistic assumptions commonly used to study label refinement and analyze whether refinement can be outperformed by alternative approaches, including computationally intractable oracle methods. We show that both weak training and label refinement suffer from irreducible error, leaving a performance gap between label refinement and the oracle. These results motivate future research into developing alternative methods for weak to strong generalization that synthesize the practicality of label refinement or weak training and the optimality of the oracle procedure.",
      "citationCount": 1,
      "doi": "10.48550/arXiv.2508.17018",
      "arxivId": "2508.17018",
      "url": "https://www.semanticscholar.org/paper/06a6418abcc2ca0595971a6cfdebc9272d18bcf4",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2508.17018"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "692aabc07a61bf8fcb5195fda7733abc9bc516c9",
      "title": "The Capabilities and Limitations of Weak-to-Strong Generalization: Generalization and Calibration",
      "authors": [
        {
          "name": "Wei Yao",
          "authorId": "2279748096"
        },
        {
          "name": "Wenkai Yang",
          "authorId": "2394712981"
        },
        {
          "name": "Ziqiao Wang",
          "authorId": "2117428102"
        },
        {
          "name": "Yankai Lin",
          "authorId": "2307480156"
        },
        {
          "name": "Yong Liu",
          "authorId": "2344265275"
        }
      ],
      "year": 2025,
      "abstract": "Weak-to-strong generalization, where weakly supervised strong models outperform their weaker teachers, offers a promising approach to aligning superhuman models with human values. To deepen the understanding of this approach, we provide theoretical insights into its capabilities and limitations. First, in the classification setting, we establish upper and lower generalization error bounds for the strong model, identifying the primary limitations as stemming from the weak model's generalization error and the optimization objective itself. Additionally, we derive lower and upper bounds on the calibration error of the strong model. These theoretical bounds reveal two critical insights: (1) the weak model should demonstrate strong generalization performance and maintain well-calibrated predictions, and (2) the strong model's training process must strike a careful balance, as excessive optimization could undermine its generalization capability by over-relying on the weak supervision signals. Finally, in the regression setting, we extend the work of Charikar et al. (2024) to a loss function based on Kullback-Leibler (KL) divergence, offering guarantees that the strong student can outperform its weak teacher by at least the magnitude of their disagreement. We conduct sufficient experiments to validate our theory.",
      "citationCount": 1,
      "doi": null,
      "arxivId": "2502.01458",
      "url": "https://www.semanticscholar.org/paper/692aabc07a61bf8fcb5195fda7733abc9bc516c9",
      "venue": "",
      "journal": null,
      "publicationTypes": null
    },
    {
      "paperId": "9366d2f3884a0a9197d2d2838c81319be3bd0197",
      "title": "Improving Weak-to-Strong Generalization with Reliability-Aware Alignment",
      "authors": [
        {
          "name": "Yue Guo",
          "authorId": "2260849905"
        },
        {
          "name": "Yi Yang",
          "authorId": "2260444973"
        }
      ],
      "year": 2024,
      "abstract": "Large language models (LLMs) are now rapidly advancing and surpassing human abilities on many natural language tasks. However, aligning these super-human LLMs with human knowledge remains challenging because the supervision signals from human annotators may be wrong. This issue, known as the\"super-alignment\"problem, requires enhancing weak-to-strong generalization, where a strong LLM must generalize from imperfect supervision provided by a weaker source. To address this issue, we propose an approach to improve weak-to-strong generalization by involving the reliability of weak supervision signals in the alignment process. In our method, we query the weak supervisor for multiple answers, estimate the answer reliability, and enhance the alignment process by filtering out uncertain data or re-weighting reliable data. Experiments on four datasets demonstrate that our methods effectively identify the quality of weak labels and significantly enhance weak-to-strong generalization. Our work presents effective techniques for error-robust model alignment, reducing error propagation from noisy supervision and enhancing the accuracy and reliability of LLMs. Codes are publicly available at http://github.com/Irenehere/ReliableAlignment.",
      "citationCount": 14,
      "doi": "10.48550/arXiv.2406.19032",
      "arxivId": "2406.19032",
      "url": "https://www.semanticscholar.org/paper/9366d2f3884a0a9197d2d2838c81319be3bd0197",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2406.19032"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "24bf816ede9ca91657346c529cd2fef5e69aacb4",
      "title": "Provable Weak-to-Strong Generalization via Benign Overfitting",
      "authors": [
        {
          "name": "David X. Wu",
          "authorId": "2233115473"
        },
        {
          "name": "Anant Sahai",
          "authorId": "1724120"
        }
      ],
      "year": 2024,
      "abstract": "The classic teacher-student model in machine learning posits that a strong teacher supervises a weak student to improve the student's capabilities. We instead consider the inverted situation, where a weak teacher supervises a strong student with imperfect pseudolabels. This paradigm was recently brought forth by Burns et al.'23 and termed \\emph{weak-to-strong generalization}. We theoretically investigate weak-to-strong generalization for binary and multilabel classification in a stylized overparameterized spiked covariance model with Gaussian covariates where the weak teacher's pseudolabels are asymptotically like random guessing. Under these assumptions, we provably identify two asymptotic phases of the strong student's generalization after weak supervision: (1) successful generalization and (2) random guessing. Our techniques should eventually extend to weak-to-strong multiclass classification. Towards doing so, we prove a tight lower tail inequality for the maximum of correlated Gaussians, which may be of independent interest. Understanding the multilabel setting reinforces the value of using logits for weak supervision when they are available.",
      "citationCount": 14,
      "doi": "10.48550/arXiv.2410.04638",
      "arxivId": "2410.04638",
      "url": "https://www.semanticscholar.org/paper/24bf816ede9ca91657346c529cd2fef5e69aacb4",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2410.04638"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "e6357780e72f4c8e239823b61f014229d3f6c9a6",
      "title": "Weak-to-Strong Generalization Through the Data-Centric Lens",
      "authors": [
        {
          "name": "Changho Shin",
          "authorId": "2054260304"
        },
        {
          "name": "John Cooper",
          "authorId": "2319150466"
        },
        {
          "name": "Frederic Sala",
          "authorId": "2186982588"
        }
      ],
      "year": 2024,
      "abstract": "The weak-to-strong generalization phenomenon is the driver for important machine learning applications including highly data-efficient learning and, most recently, performing superalignment. While decades of research have resulted in numerous algorithms that produce strong empirical performance, understanding what aspects of data enable weak-to-strong generalization has been understudied. We propose a simple data-centric mechanism that characterizes weak-to-strong generalization: the overlap density. Intuitively, generalization tracks the number of points that contain overlaps, i.e., both easy patterns (learnable by a weak model) and challenging patterns (only learnable by a stronger model), as with such points, weak predictions can be used to learn challenging patterns by stronger models. We provide a practical overlap detection algorithm to find such points in datasets and leverage them to learn, among multiple sources of data, which to query when seeking to maximize overlap density and thereby enhance weak-to-strong generalization. We present a theoretical result showing that the generalization benefit is a function of the overlap density and a regret bound for our data selection algorithm. Empirically, we validate the mechanism and the overlap detection algorithm on a wide array of settings.",
      "citationCount": 14,
      "doi": "10.48550/arXiv.2412.03881",
      "arxivId": "2412.03881",
      "url": "https://www.semanticscholar.org/paper/e6357780e72f4c8e239823b61f014229d3f6c9a6",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2412.03881"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "ee88abd7af355fdde827b5656ab59b2cef81c65c",
      "title": "High-dimensional Analysis of Knowledge Distillation: Weak-to-Strong Generalization and Scaling Laws",
      "authors": [
        {
          "name": "M. E. Ildiz",
          "authorId": "46214352"
        },
        {
          "name": "Halil Alperen Gozeten",
          "authorId": "2327335132"
        },
        {
          "name": "Ege Onur Taga",
          "authorId": "2297769735"
        },
        {
          "name": "Marco Mondelli",
          "authorId": "2302797572"
        },
        {
          "name": "Samet Oymak",
          "authorId": "3103394"
        }
      ],
      "year": 2024,
      "abstract": "A growing number of machine learning scenarios rely on knowledge distillation where one uses the output of a surrogate model as labels to supervise the training of a target model. In this work, we provide a sharp characterization of this process for ridgeless, high-dimensional regression, under two settings: (i) model shift, where the surrogate model is arbitrary, and (ii) distribution shift, where the surrogate model is the solution of empirical risk minimization with out-of-distribution data. In both cases, we characterize the precise risk of the target model through non-asymptotic bounds in terms of sample size and data distribution under mild conditions. As a consequence, we identify the form of the optimal surrogate model, which reveals the benefits and limitations of discarding weak features in a data-dependent fashion. In the context of weak-to-strong (W2S) generalization, this has the interpretation that (i) W2S training, with the surrogate as the weak model, can provably outperform training with strong labels under the same data budget, but (ii) it is unable to improve the data scaling law. We validate our results on numerical experiments both on ridgeless regression and on neural network architectures.",
      "citationCount": 13,
      "doi": "10.48550/arXiv.2410.18837",
      "arxivId": "2410.18837",
      "url": "https://www.semanticscholar.org/paper/ee88abd7af355fdde827b5656ab59b2cef81c65c",
      "venue": "International Conference on Learning Representations",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2410.18837"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "058adcbc13eb61020b0e6395d0b77a009339d84e",
      "title": "EnsemW2S: Enhancing Weak-to-Strong Generalization with Large Language Model Ensembles",
      "authors": [
        {
          "name": "Aakriti Agrawal",
          "authorId": "2258553912"
        },
        {
          "name": "Mucong Ding",
          "authorId": "52184822"
        },
        {
          "name": "Zora Che",
          "authorId": "2173680156"
        },
        {
          "name": "Chenghao Deng",
          "authorId": "2238209214"
        },
        {
          "name": "Anirudh Satheesh",
          "authorId": "2211810584"
        },
        {
          "name": "John Langford",
          "authorId": "2283766631"
        },
        {
          "name": "Furong Huang",
          "authorId": "2303418156"
        }
      ],
      "year": 2024,
      "abstract": "With Large Language Models (LLMs) rapidly approaching and potentially surpassing human-level performance, it has become imperative to develop approaches capable of effectively supervising and enhancing these powerful models using smaller, human-level models exposed to only human-level data. We address this critical weak-to-strong (W2S) generalization challenge by proposing a novel method aimed at improving weak experts, by training on the same limited human-level data, enabling them to generalize to complex, super-human-level tasks. Our approach, called **EnsemW2S**, employs a token-level ensemble strategy that iteratively combines multiple weak experts, systematically addressing the shortcomings identified in preceding iterations. By continuously refining these weak models, we significantly enhance their collective ability to supervise stronger student models. We extensively evaluate the generalization performance of both the ensemble of weak experts and the subsequent strong student model across in-distribution (ID) and out-of-distribution (OOD) datasets. For OOD, we specifically introduce question difficulty as an additional dimension for defining distributional shifts. Our empirical results demonstrate notable improvements, achieving 4%, and 3.2% improvements on ID datasets and, upto 6% and 2.28% on OOD datasets for experts and student models respectively, underscoring the effectiveness of our proposed method in advancing W2S generalization.",
      "citationCount": 7,
      "doi": "10.48550/arXiv.2505.21959",
      "arxivId": "2505.21959",
      "url": "https://www.semanticscholar.org/paper/058adcbc13eb61020b0e6395d0b77a009339d84e",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2505.21959"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "8880022a7d461c8c51266fa2908a8669d770772e",
      "title": "A transfer learning framework for weak to strong generalization",
      "authors": [
        {
          "name": "Seamus Somerstep",
          "authorId": "102596550"
        },
        {
          "name": "Felipe Maia Polo",
          "authorId": "1490941219"
        },
        {
          "name": "Moulinath Banerjee",
          "authorId": "2262444444"
        },
        {
          "name": "Ya'acov Ritov",
          "authorId": "2240337985"
        },
        {
          "name": "M. Yurochkin",
          "authorId": "8202372"
        },
        {
          "name": "Yuekai Sun",
          "authorId": "2247880508"
        }
      ],
      "year": 2024,
      "abstract": "Modern large language model (LLM) alignment techniques rely on human feedback, but it is unclear whether these techniques fundamentally limit the capabilities of aligned LLMs. In particular, it is unknown if it is possible to align (stronger) LLMs with superhuman capabilities with (weaker) human feedback without degrading their capabilities. This is an instance of the weak-to-strong generalization problem: using feedback from a weaker (less capable) model to train a stronger (more capable) model. We prove that weak-to-strong generalization is possible by eliciting latent knowledge from pre-trained LLMs. In particular, we cast the weak-to-strong generalization problem as a transfer learning problem in which we wish to transfer a latent concept prior from a weak model to a strong pre-trained model. We prove that a naive fine-tuning approach suffers from fundamental limitations, but an alternative refinement-based approach suggested by the problem structure provably overcomes the limitations of fine-tuning. Finally, we demonstrate the practical applicability of the refinement approach in multiple LLM alignment tasks.",
      "citationCount": 8,
      "doi": null,
      "arxivId": "2405.16236",
      "url": "https://www.semanticscholar.org/paper/8880022a7d461c8c51266fa2908a8669d770772e",
      "venue": "International Conference on Learning Representations",
      "journal": null,
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "509a85b7194c4d4b055135ac545ed478b2320214",
      "title": "Weak-to-Strong Generalization beyond Accuracy: a Pilot Study in Safety, Toxicity, and Legal Reasoning",
      "authors": [
        {
          "name": "Ruimeng Ye",
          "authorId": "2290490354"
        },
        {
          "name": "Yang Xiao",
          "authorId": "2290756492"
        },
        {
          "name": "Bo Hui",
          "authorId": "2326118022"
        }
      ],
      "year": 2024,
      "abstract": "As large language models (LLMs) continue to advance, ensuring their alignment with human values becomes increasingly critical. Traditional alignment methods heavily rely on human feedback to fine-tune models. With the emergence of superhuman models whose outputs may surpass human understanding, evaluating and aligning these models using human judgments poses significant challenges. To address the challenges, recent works use weak supervisors to elicit knowledge from much stronger models. However, there are important disanalogies between the empirical setup in the existing works and the genuine goal of alignment. We remark that existing works investigate the phenomenon of weak-to-strong generation in analogous setup (i.e., binary classification), rather than practical alignment-relevant tasks (e.g., safety). In this paper, we bridge this gap by extending weak-to-strong generation to the context of practical alignment. We empirically demonstrate the widespread phenomenon of weak-to-strong generation in three complicated alignment tasks: safety, toxicity, and legal reasoning}. Furthermore, we explore efficient strategies for improving alignment performance to enhance the quality of model outcomes. Lastly, we summarize and analyze the challenges and potential solutions in regard to specific alignment tasks, which we hope to catalyze the research progress on the topic of weak-to-strong generalization. Our code is released at https://github.com/yeruimeng/WTS.git.",
      "citationCount": 5,
      "doi": "10.48550/arXiv.2410.12621",
      "arxivId": "2410.12621",
      "url": "https://www.semanticscholar.org/paper/509a85b7194c4d4b055135ac545ed478b2320214",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2410.12621"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    },
    {
      "paperId": "06050dca1a9da67bfb0a9d22646aa2a92b32460c",
      "title": "Weak-to-Strong Generalization with Failure Trajectories: A Tree-based Approach to Elicit Optimal Policy in Strong Models",
      "authors": [
        {
          "name": "Ruimeng Ye",
          "authorId": "2290490354"
        },
        {
          "name": "Zihan Wang",
          "authorId": "2243360876"
        },
        {
          "name": "Yang Xiao",
          "authorId": "2290756492"
        },
        {
          "name": "Zinan Ling",
          "authorId": "2342406723"
        },
        {
          "name": "Manling Li",
          "authorId": "2374218635"
        },
        {
          "name": "Bo Hui",
          "authorId": "2371993573"
        }
      ],
      "year": 2025,
      "abstract": "Weak-to-Strong generalization (W2SG) is a new trend to elicit the full capabilities of a strong model with supervision from a weak model. While existing W2SG studies focus on simple tasks like binary classification, we extend this paradigm to complex interactive decision-making environments. Specifically, we fine-tune a strong model with trajectories of intermediate actions generated by a weak model. Motivated by the human learning process, we propose to generalize not only success knowledge but also failure experience so that the strong model can learn from failed trajectories accumulated by weak models. To effectively and efficiently elicit the potential of strong agents, we further construct ``trajectory trees,\"a hierarchical representation that organizes weak model-generated action trajectories, coupled with Monte Carlo Tree Search (MCTS) to optimize the strong model. Through theoretical analysis, we provide formal guarantees for the effectiveness of our method in improving W2SG performance. Our empirical evaluations demonstrate substantial improvements in reasoning and decision-making capabilities across diverse task domains, validating the scalability and robustness of our proposed framework.",
      "citationCount": 0,
      "doi": "10.48550/arXiv.2507.18858",
      "arxivId": "2507.18858",
      "url": "https://www.semanticscholar.org/paper/06050dca1a9da67bfb0a9d22646aa2a92b32460c",
      "venue": "arXiv.org",
      "journal": {
        "name": "ArXiv",
        "volume": "abs/2507.18858"
      },
      "publicationTypes": [
        "JournalArticle"
      ]
    }
  ],
  "count": 30,
  "errors": []
}
