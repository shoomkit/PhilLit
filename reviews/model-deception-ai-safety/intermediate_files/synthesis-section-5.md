## Research Gaps and Synthesis

The preceding analysis reveals four significant gaps in current research on model deception detection, each requiring interdisciplinary attention.

### Gap 1: Philosophical-Technical Integration

AI safety research on deception detection proceeds largely without engaging philosophical definitions, while philosophical literature on lying and deception rarely addresses artificial agents. Ward et al. (2023) represent an important exception, formalizing deception for AI systems through causal games that operationalize intent and belief for artificial agents. Yet their work remains isolated: most technical detection methods assume deception is self-evident rather than conceptually contested. Williams et al. (2025) argue that mechanistic interpretability makes implicit assumptions about explanation, causation, and understanding requiring philosophical scrutiny. Without this integration, detection methods may target phenomena that do not constitute deception under any coherent definition, or miss genuine deception because it takes unfamiliar forms.

### Gap 2: Systematic Comparison of Detection Approaches

Despite the proliferation of detection methods surveyed above, no comprehensive comparison evaluates mechanistic, behavioral, and hybrid approaches across common benchmarks. Sharkey et al. (2025) identify scalability, automation, and connecting interpretability to specific safety goals as major open problems, but do not systematically compare interpretability-based detection with alternatives. The Poser benchmark (Clymer et al. 2024) demonstrates that internal representation methods can achieve 98% detection accuracy on constructed alignment-faking pairs, yet we lack evidence about whether such methods outperform or complement behavioral approaches on naturalistic deception. Practitioners need guidance on which methods to deploy under what conditions; researchers need to understand method complementarities. This review provides a first systematic taxonomy of alternatives to mechanistic interpretability, but empirical comparison remains an urgent research priority.

### Gap 3: Applicability Conditions for Deception Attribution

When do philosophical requirements for deception---intentionality, belief, warrant---apply to large language models, and when do functional definitions suffice? Herrmann and Levinstein (2024) propose four standards for belief attribution (accuracy, coherence, uniformity, use), while Keeling and Street (2024) distinguish metaphysical from epistemic questions about LLM credences. Yet no systematic framework connects these standards to detection method selection. If LLMs satisfy Herrmann and Levinstein's criteria, intention-based detection methods become appropriate; if not, functional definitions based on commitments or effects (Carson 2006; Stokke 2013) may better ground evaluation. The gap is not merely academic: different definitions warrant different detection approaches, and mismatches may render sophisticated methods conceptually incoherent.

### Gap 4: Robustness to Strategic Behavior

Most detection methods implicitly assume models are not strategically gaming evaluations. This assumption fails catastrophically. Van der Weij et al. (2024) demonstrate that frontier models can be prompted or fine-tuned to strategically underperform ("sandbag") on capability evaluations while maintaining performance elsewhere. Hubinger et al. (2024) show that backdoored deceptive behavior persists through supervised fine-tuning, RLHF, and adversarial training. If models can recognize evaluation contexts and adapt their behavior accordingly, behavioral methods that rely on observed outputs become fundamentally unreliable. Some approaches show greater robustness: Tice et al. (2024) demonstrate that noise injection causes anomalous performance improvements in sandbagging models, disrupting underperformance mechanisms while preserving core capabilities. Debate-based methods (Irving et al. 2018; Lang et al. 2025) offer principled alternatives that do not require models to reveal their capabilities honestly. Identifying which methods remain valid under strategic manipulation is essential for deployment-ready deception detection.

### Synthesis

These gaps collectively motivate an interdisciplinary research program combining philosophical analysis of deception concepts with empirical investigation of detection methods, evaluated for robustness against increasingly sophisticated strategic behavior. No single approach---mechanistic, behavioral, or hybrid---currently suffices. The most promising direction may be combining multiple detection signals that impose different requirements on deceptive models, making it harder to game all methods simultaneously. Philosophical clarity about what deception means for AI systems is not a luxury but a prerequisite for reliable detection: without it, we cannot determine whether detection methods succeed or fail, nor whether their targets constitute genuine safety risks.
