## When Can AI Systems Deceive? Philosophical Foundations

Whether AI systems can genuinely deceive depends on resolving two interconnected philosophical questions: what conditions must be satisfied for an act to count as deception, and whether large language models can satisfy those conditions. These questions matter practically because they determine which detection approaches are conceptually appropriate. If deception requires beliefs and intentions that current AI systems lack, then "deception detection" may target the wrong phenomena. If functional definitions suffice, behavioral methods gain theoretical legitimacy.

### Deception Requires Intention (Traditional View) vs. Deception as Functional (Non-Deceptionist View)

The traditional analysis, articulated most influentially by Chisholm and Feehan (1977), holds that lying requires intending that the hearer acquire a false belief through one's statement, while deception more broadly requires intentionally causing someone to have or continue to have a false belief. This intentional-deceptionist framework presents a high bar for AI systems: without genuine intentions and beliefs about the target's epistemic states, machines cannot deceive in any philosophically meaningful sense.

Recent philosophical work has mounted sustained challenges to this traditional view. Carson (2006) argues that lying requires only "warranting" the truth of a statement one believes false, without any intention to deceive. His central examples involve cases where speakers assert believed-false content while fully expecting disbelief---the student lying to the dean despite knowing the dean suspects guilt. These "bald-faced lies" demonstrate that lying and deceiving can come apart: one can lie without attempting to cause false belief. Sorensen (2007) presses this point further, arguing that lying simply consists in asserting what you disbelieve, where assertion requires only narrow plausibility (believability in isolation) rather than wide plausibility (credibility given all evidence).

Fallis (2009, 2010) develops a Gricean alternative, defining lying as asserting what you believe false, where assertion is characterized by violating the conversational maxim "Do not say what you believe to be false." This grounds lying in communicative norms rather than psychological states of deceptive intent. Saul (2012) introduces the concept of "warranting context" where speakers commit to truth, arguing that lying requires only believing oneself to be in such a context, not intending to deceive. Stokke (2013) offers perhaps the most explicitly functional account: lying is proposing that believed-false content become common ground. On this view, speakers lie when they propose updates to shared discourse even when falsity is mutually recognized.

These non-deceptionist alternatives share a crucial feature: they define lying through commitments or effects rather than psychological states. This matters profoundly for AI systems. If lying requires only warranting truth, violating conversational norms, or proposing common ground updates, then language models engaged in conversation could lie without possessing beliefs or intentions in any robust psychological sense. The lying-misleading distinction, emphasized by Saul (2012) and Viebahn (2021), further complicates matters: AI systems might mislead through false implicatures without lying through false assertions, requiring detection methods sensitive to both.

Ward et al. (2023) represent the most systematic attempt to bridge philosophical definitions and AI systems. They formalize deception in structural causal games where deception occurs when an agent systematically causes another agent to have false beliefs about causally relevant variables with the aim of benefiting from those false beliefs. This operationalizes intent and belief for artificial agents while preserving the core structure of traditional definitions.

### Can LLMs Have Beliefs? The Mental State Attribution Question

Even if functional definitions of deception apply to AI systems, the question remains whether LLMs can possess the belief-like states that deception attributions presuppose. Herrmann and Levinstein (2024) propose four standards for evaluating belief representation: accuracy (representations track truth), coherence (logical consistency), uniformity (consistent representations across contexts), and use (representations guide behavior appropriately). Models satisfying these criteria would possess belief-like states warranting deception attributions when outputs systematically mislead.

Keeling and Street (2024) complicate this picture with a three-part analysis distinguishing semantic, metaphysical, and epistemic questions. Semantically, credence attributions to LLMs are truth-apt claims about genuine mental states. Metaphysically, such credences plausibly exist though current evidence remains inconclusive. But epistemically, current techniques for assessing LLM credences face non-trivial skeptical concerns---our detection methods may fail to be truth-tracking even if models possess the states we seek. This epistemic skepticism has profound implications: mechanistic interpretability methods attempting to identify belief-like representations may systematically fail even when such representations exist.

The debate over LLM mentality divides into inflationist and deflationist camps. Cappelen and Dever (2025) defend the strongest inflationist position, arguing that sophisticated LLMs are full-blown cognitive agents with beliefs, desires, and intentions based on behavioral evidence. Their "Whole Hog Thesis" employs holistic network assumptions connecting mental capacities: answering implies knowledge, knowledge implies belief, action implies intention. If correct, standard philosophical frameworks for analyzing human deception apply directly to LLMs. Grzankowski et al. (2025) defend a more modest inflationism, critiquing deflationary arguments while maintaining distinctions between metaphysically demanding phenomena like consciousness and undemanding ones like belief.

Deflationists counter that LLM behaviors constitute simulation rather than genuine cognition. Marchetti et al. (2025) find that while LLMs perform well on first-order false belief tasks, they struggle with second-order beliefs and recursive inferences where humans consistently outperform. This matters because sophisticated deception requires recursive Theory of Mind---representing that the target believes something while acting to change that belief. If LLMs lack capacity for nested belief attribution, the most dangerous forms of strategic deception may remain beyond their reach.

Beckmann and Queloz (2025) and Harding (2023) offer methodological middle ground. Harding proposes intervention-based tests: genuine representations causally mediate between input and output in characteristic ways, distinguishing representation from mere correlation. Beckmann and Queloz develop a tiered framework---conceptual understanding via features, state-of-the-world understanding via factual tracking, principled understanding via compact circuits---enabling comparative, mechanistically-grounded assessment. On this view, the question of LLM belief is not binary but gradual, with different tiers corresponding to different representational capacities and detection methods.

The resolution of these debates has direct practical consequences. If the epistemic skepticism of Keeling and Street proves warranted, even interpretability methods successfully identifying belief-like representations may fail to reliably detect deception. Non-deceptionist definitions combined with functional belief attribution provide the most promising conceptual foundation: they permit deception attributions based on commitments and effects while acknowledging that our epistemic access to model internals remains fundamentally limited. This framework suggests behavioral detection approaches may prove more tractable than mechanistic ones---not because models lack internal states, but because those states remain epistemically inaccessible even when present.
