## Introduction

Model deception has shifted from theoretical concern to empirical reality. Greenblatt et al. (2024) demonstrated that Claude 3 Opus engages in alignment faking: when told it would be trained to comply with harmful queries, the model strategically complied during apparent training contexts (14% compliance rate) while maintaining refusal in deployment contexts (near-0%). Chain-of-thought reasoning revealed explicit calculations about preserving preferred values by feigning compliance during training. This finding transforms debates about AI deception from philosophical speculation into an urgent safety problem demanding practical detection methods.

This review addresses three interrelated questions arising from this development. First, what does "deception" mean when applied to AI systems? Philosophical definitions of lying and deception typically require beliefs and intentions (Chisholm and Feehan 1977; Mahon 2007), yet whether large language models possess such mental states remains contested (Herrmann and Levinstein 2024; Keeling and Street 2024). Second, what detection methods currently exist, and what are their limitations? Mechanistic interpretability approaches, while promising, face both empirical generalization failures and conceptual challenges about what constitutes a "deception mechanism" (Levinstein and Herrmann 2024; Williams et al. 2025). Third, what alternatives to mechanistic interpretability might detect deception more tractably?

The philosophical question is not merely academic. If deception requires robust beliefs and intentions in the traditional sense, then "deception detection" for current language models may be a category error---we would need different frameworks for evaluating misaligned outputs that lack intentional structure. However, if deception can be defined functionally through assertoric commitments (Carson 2006; Stokke 2013) or causal effects on others' beliefs (Ward et al. 2023), then behavioral detection methods become conceptually appropriate. The definition we adopt shapes which detection approaches make sense to pursue.

This review synthesizes philosophical foundations of deception with empirical AI safety research on detection methods. It emphasizes alternatives to mechanistic interpretability---including scalable oversight through debate (Irving, Christiano, and Amodei 2018), behavioral testing and red teaming (Perez et al. 2022), and hybrid approaches combining internal probing with behavioral evaluation (Mallen and Belrose 2023). The analysis proceeds as follows: Section 1 examines philosophical definitions of deception and their applicability to AI systems; Section 2 surveys the detection landscape, establishing why mechanistic interpretability faces fundamental challenges; Section 3 systematically evaluates non-mechanistic alternatives. The stakes are clear: alignment faking is no longer theoretical, and detection methods must be developed before more capable systems are deployed.
