@comment{
====================================================================
DOMAIN: Definitions and Conceptualizations of Mechanistic Interpretability
SEARCH_DATE: 2025-12-21
PAPERS_FOUND: 18 total (High: 8, Medium: 7, Low: 3)
SEARCH_SOURCES: Semantic Scholar, OpenAlex, arXiv, SEP
====================================================================

DOMAIN_OVERVIEW:
Mechanistic interpretability (MI) has emerged as a distinct approach within explainable AI, focused on reverse-engineering the computational mechanisms learned by neural networks into human-understandable algorithms and concepts. The field exhibits significant definitional heterogeneity. Technical MI research primarily concentrates on circuit discovery and feature identification at the level of individual neurons, attention heads, and small subnetworks (Nanda et al. 2023; Conmy et al. 2023). This narrow interpretation treats MI as bottom-up analysis of low-level computational units. However, philosophical treatments propose broader conceptualizations that include functional and higher-level explanations (Kästner & Crook 2024), drawing connections to mechanistic explanation in the life sciences. Recent comprehensive reviews (Bereska & Gavves 2024; Rai et al. 2024) attempt to systematize the field but inherit these definitional ambiguities.

The conceptual tension centers on what counts as "mechanistic" versus other forms of interpretation. Technical work emphasizes causal intervention methods (activation patching, ablation) to identify minimal circuits implementing specific behaviors. Philosophical work questions whether such reductionist approaches capture genuine mechanistic understanding or merely provide fine-grained causal descriptions. Theoretical foundations like causal abstraction (Geiger et al. 2023) aim to formalize the relationship between high-level functional descriptions and low-level implementations, but disagreement persists about what level of analysis constitutes authentic MI.

Recent developments include dictionary learning methods to address superposition and extract monosemantic features (He et al. 2024), automated circuit discovery algorithms (Conmy et al. 2023), and extensions to vision-language models (Palit et al. 2023; Golovanevsky et al. 2024). However, scaling challenges remain prominent - Zimmermann et al. (2023) find that model scale does not improve mechanistic interpretability, and Sharkey et al. (2025) identify numerous open problems requiring solution before MI can deliver on its promises for AI safety and scientific understanding.

RELEVANCE_TO_PROJECT:
This domain directly addresses the core research question about how "mechanistic interpretability" is defined across technical and philosophical literature. The identified papers reveal the definitional disagreement between narrow (node-level) and broad (functional) interpretations that motivates the research proposal. Kästner & Crook (2024) represents the philosophical position that MI includes higher-level explanation, while the technical literature (Nanda et al., Conmy et al., Bereska & Gavves) predominantly adopts the narrow interpretation focused on circuits and low-level features.

NOTABLE_GAPS:
Few papers explicitly engage with the definitional question as a philosophical problem requiring conceptual analysis. Most technical work assumes a shared understanding of MI without justifying boundary-drawing decisions. The relationship between MI and classical philosophical accounts of mechanistic explanation (Craver, Machamer et al.) remains under-explored in the MI literature itself.

SYNTHESIS_GUIDANCE:
Organize synthesis around the narrow vs. broad definitional spectrum. Use Kästner & Crook (2024) as the key philosophical anchor for the broad interpretation, contrasting with the dominant technical literature's narrow focus. Highlight how different definitions have implications for evaluating necessity/sufficiency claims about MI's role in AI safety.

KEY_POSITIONS:
- Narrow MI (circuit-level): 10 papers - Focus on neurons, attention heads, minimal circuits
- Broad MI (functional): 3 papers - Include higher-level functional explanations
- Theoretical foundations: 2 papers - Causal abstraction, formalization efforts
- Critical perspectives: 3 papers - Scaling limitations, open problems
====================================================================
}

@article{kastner2024explaining,
  author = {K\"{a}stner, Lena and Crook, Barnaby},
  title = {Explaining AI through mechanistic interpretability},
  journal = {European Journal for Philosophy of Science},
  year = {2024},
  volume = {14},
  number = {4},
  pages = {52},
  doi = {10.1007/s13194-024-00614-4},
  note = {
  CORE ARGUMENT: Argues that explainable AI (XAI) research should pursue mechanistic interpretability by applying coordinated discovery strategies from the life sciences to uncover the functional organization of complex AI systems. Claims that current divide-and-conquer XAI strategies fail to illuminate how trained AI systems work as a whole, and that functional understanding is needed to satisfy societal desiderata such as safety. Explicitly states that MI "enables us to meet desirable social desiderata including safety," suggesting MI is sufficient for safety goals.

  RELEVANCE: This is one of the two key papers mentioned in the research context. Represents the broad interpretation of MI that includes functional and higher-level explanations, directly contrasting with narrow technical definitions. The necessity and sufficiency claims about MI's role in safety are central to the research question. This philosophical treatment draws on mechanistic explanation literature from life sciences, providing theoretical grounding for broader MI conceptualizations.

  POSITION: Broad mechanistic interpretability (functional organization); claims MI is necessary and sufficient for AI safety.
  },
  keywords = {mechanistic-interpretability, philosophy-of-AI, XAI, High}
}

@article{bereska2024mechanistic,
  author = {Bereska, Leonard and Gavves, Efstratios},
  title = {Mechanistic Interpretability for AI Safety - A Review},
  journal = {Transactions on Machine Learning Research},
  year = {2024},
  doi = {10.48550/arXiv.2404.14082},
  note = {
  CORE ARGUMENT: Provides comprehensive review of mechanistic interpretability as reverse-engineering computational mechanisms and representations learned by neural networks into human-understandable algorithms and concepts. Defines MI around features encoding knowledge within activations and causal dissection methodologies. Examines MI's relevance to AI safety through understanding, control, and alignment benefits, while noting risks like capability gains and dual-use concerns. Advocates for scaling techniques and expanding to vision and reinforcement learning domains.

  RELEVANCE: Authoritative technical review that systematizes the field's conceptual foundations and methodologies. Provides the standard technical definition of MI emphasizing causal mechanisms and fine-grained understanding. Critical for understanding how the technical AI safety community conceptualizes MI's relationship to safety, offering detailed analysis of benefits and limitations. Helps evaluate sufficiency claims by cataloguing what MI can and cannot do for safety.

  POSITION: Technical mechanistic interpretability (causal mechanisms, features); cautiously optimistic about MI for safety with caveats.
  },
  keywords = {mechanistic-interpretability, AI-safety, review, High}
}

@inproceedings{nanda2023progress,
  author = {Nanda, Neel and Chan, Lawrence and Lieberum, Tom and Smith, Jess and Steinhardt, Jacob},
  title = {Progress measures for grokking via mechanistic interpretability},
  booktitle = {International Conference on Learning Representations},
  year = {2023},
  doi = {10.48550/arXiv.2301.05217},
  note = {
  CORE ARGUMENT: Demonstrates mechanistic interpretability by fully reverse-engineering the algorithm learned by small transformers for modular addition, discovering networks use discrete Fourier transforms and trigonometric identities. Argues that progress measures can be found via mechanistic interpretability by reverse-engineering learned behaviors into individual components. Reveals grokking arises from gradual amplification of structured mechanisms followed by removal of memorizing components, rather than sudden emergence.

  RELEVANCE: Exemplifies the narrow technical interpretation of MI focused on circuit-level reverse engineering. Demonstrates what counts as successful MI in practice: identifying specific algorithms (Fourier transforms) implemented by network components. Shows the reductionist methodology of analyzing activations, weights, and performing ablations. Important for understanding the technical MI paradigm's capabilities and what level of mechanistic detail is sought.

  POSITION: Narrow mechanistic interpretability (circuit discovery, algorithm extraction); exemplar of successful technical MI.
  },
  keywords = {mechanistic-interpretability, circuit-discovery, grokking, High}
}

@inproceedings{conmy2023towards,
  author = {Conmy, Arthur and Mavor-Parker, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adri\`{a}},
  title = {Towards Automated Circuit Discovery for Mechanistic Interpretability},
  booktitle = {Neural Information Processing Systems},
  year = {2023},
  doi = {10.48550/arXiv.2304.14997},
  note = {
  CORE ARGUMENT: Systematizes and automates the mechanistic interpretability process of identifying circuits in model computational graphs that implement specified behaviors. Proposes ACDC algorithm that applies activation patching to find which abstract neural network units are involved in behaviors. Successfully rediscovered all manually-identified component types in a circuit in GPT-2 Small that computes Greater-Than operation, selecting 68 of 32,000 edges.

  RELEVANCE: Defines the standard MI workflow in technical research: choose metric and dataset, apply activation patching, identify relevant units and circuits. Represents automation efforts that scale MI beyond manual investigation. Important for understanding what the technical community means by "circuits" and how they are discovered. The narrow focus on computational graph edges and activation patching exemplifies the reductionist technical interpretation.

  POSITION: Narrow mechanistic interpretability (automated circuit discovery via activation patching).
  },
  keywords = {mechanistic-interpretability, circuit-discovery, automation, High}
}

@article{rai2024practical,
  author = {Rai, Daking and Zhou, Yilun and Feng, Shi and Saparov, Abulhair and Yao, Ziyu},
  title = {A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2407.02646},
  doi = {10.48550/arXiv.2407.02646},
  note = {
  CORE ARGUMENT: Provides task-centric taxonomy of MI research organized around specific research questions. Outlines fundamental objects of study in MI (features, circuits, attention heads) along with techniques and evaluation methods for each task. Presents roadmap for newcomers by identifying impactful problems and organizing the field's insights and challenges. Discusses current gaps and future directions for MI research on language models.

  RELEVANCE: Comprehensive practical review that maps the technical MI landscape for transformers. Important for understanding how MI practitioners organize their research questions and what counts as meaningful MI tasks. The task-centric taxonomy reveals the field's priorities and what problems are considered central to MI. Helps identify whether definitional questions are recognized as important within technical MI community.

  POSITION: Narrow mechanistic interpretability (task-centric, transformer-focused); practical guidance for technical MI.
  },
  keywords = {mechanistic-interpretability, transformers, review, High}
}

@article{geiger2023causal,
  author = {Geiger, Atticus and Ibeling, Duligur and Zur, Amir and Chaudhary, Maheep and Chauhan, Sonakshi and Huang, Jing and Arora, Aryaman and Wu, Zhengxuan and Goodman, Noah D. and Potts, Christopher and Icard, Thomas F.},
  title = {Causal Abstraction: A Theoretical Foundation for Mechanistic Interpretability},
  journal = {arXiv},
  year = {2023},
  volume = {abs/2301.04709},
  doi = {10.48550/arXiv.2301.04709},
  note = {
  CORE ARGUMENT: Provides theoretical foundation for mechanistic interpretability through causal abstraction framework. Generalizes causal abstraction from mechanism replacement to arbitrary mechanism transformation. Formalizes core MI concepts including polysemantic neurons, linear representation hypothesis, modular features, and graded faithfulness. Unifies variety of MI methods (activation patching, causal mediation analysis, circuit analysis, sparse autoencoders) in common language of causal abstraction.

  RELEVANCE: Offers rigorous theoretical framework for what constitutes valid mechanistic interpretation and how high-level functional descriptions relate to low-level implementations. Important for understanding formal foundations underlying MI claims. The abstraction framework provides vocabulary for discussing levels of mechanistic explanation and relationships between them. Critical for evaluating whether different MI definitions are theoretically coherent and what formalization requirements mechanistic claims must meet.

  POSITION: Theoretical foundations for mechanistic interpretability (causal abstraction, formalization).
  },
  keywords = {mechanistic-interpretability, causal-abstraction, theory, High}
}

@article{he2024dictionary,
  author = {He, Zhengfu and Ge, Xuyang and Tang, Qiong and Sun, Tianxiang and Cheng, Qinyuan and Qiu, Xipeng},
  title = {Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic Interpretability: A Case Study on Othello-GPT},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2402.12201},
  doi = {10.48550/arXiv.2402.12201},
  note = {
  CORE ARGUMENT: Proposes using sparse dictionary learning to extract monosemantic features from model activations, addressing superposition problem in mechanistic interpretability. Develops circuit discovery framework based on dictionary features decomposed from all modules writing to residual stream. Demonstrates ability to trace from logits/attention scores down to lower-level dictionary features and compute their contributions to interpretable local model behaviors. Discovers interpretable fine-grained circuits in Othello-GPT.

  RELEVANCE: Represents recent methodological advance addressing core challenge (superposition) in mechanistic interpretability. Dictionary learning aims to extract more atomic, interpretable features from distributed representations. Important for understanding evolving technical definitions of what constitutes a "feature" or "mechanism" - shift from neurons to dictionary elements. Shows how MI methodology develops to handle polysemanticity while maintaining narrow circuit-focused approach.

  POSITION: Narrow mechanistic interpretability (dictionary learning for monosemantic features, circuit discovery).
  },
  keywords = {mechanistic-interpretability, dictionary-learning, circuits, Medium}
}

@article{michaud2024opening,
  author = {Michaud, Eric J. and Liao, Isaac and Lad, Vedang and Liu, Ziming and Mudide, Anish and Loughridge, Chloe and Guo, Zifan Carl and Kheirkhah, Tara Rezaei and Vukeli\'{c}, Mateja and Tegmark, Max},
  title = {Opening the AI black box: program synthesis via mechanistic interpretability},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2402.05110},
  doi = {10.48550/arXiv.2402.05110},
  note = {
  CORE ARGUMENT: Presents MIPS method for program synthesis based on automated mechanistic interpretability of neural networks trained to perform desired tasks. Auto-distills learned algorithms into Python code by converting RNN into finite state machine using integer autoencoder, then applying Boolean/integer symbolic regression. Solves 32 of 62 algorithmic tasks, including 13 not solved by GPT-4. Demonstrates program synthesis without human training data, making learned models more interpretable and trustworthy.

  RELEVANCE: Pushes mechanistic interpretability toward extracting human-readable programs from trained networks, representing strong form of mechanistic understanding. Shows MI can recover actual algorithms rather than just identifying circuits or features. Important for understanding aspirational goals of MI - complete reverse engineering to executable code. Demonstrates what successful MI might look like for transparency and trust, relevant to sufficiency claims about MI enabling safety.

  POSITION: Narrow mechanistic interpretability (program synthesis, algorithm extraction); strong interpretability via code distillation.
  },
  keywords = {mechanistic-interpretability, program-synthesis, algorithm-extraction, Medium}
}

@article{pearce2024bilinear,
  author = {Pearce, Michael T. and Dooms, Thomas and Rigg, Alice and Oramas, Jos\'{e} and Sharkey, Lee},
  title = {Bilinear MLPs enable weight-based mechanistic interpretability},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2410.08417},
  doi = {10.48550/arXiv.2410.08417},
  note = {
  CORE ARGUMENT: Analyzes bilinear MLPs (Gated Linear Units without element-wise nonlinearity) that can be fully expressed as linear operations using third-order tensors, enabling weight-based rather than activation-based interpretability. Eigendecomposition of bilinear MLP weights reveals interpretable low-rank structure across tasks. Demonstrates that weight-based interpretability is viable for understanding deep learning models, enabling circuit identification directly from weights without requiring activation analysis.

  RELEVANCE: Proposes alternative approach to MI through architectural design that enhances interpretability. Questions whether mechanistic interpretability requires activation-based analysis or can be achieved through weight inspection. Important for understanding scope of MI - whether it's methodology-independent or tied to specific analysis techniques. Shows potential for designing models that are inherently more interpretable mechanistically.

  POSITION: Narrow mechanistic interpretability (weight-based analysis, architectural design for interpretability).
  },
  keywords = {mechanistic-interpretability, weight-analysis, architecture, Medium}
}

@inproceedings{redep2024detecting,
  author = {Sun, ZhongXiang and Zang, Xiaoxue and Zheng, Kai and Song, Yang and Xu, Jun and Zhang, Xiao and Yu, Weijie and Li, Han},
  title = {ReDeEP: Detecting Hallucination in Retrieval-Augmented Generation via Mechanistic Interpretability},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2410.11414},
  doi = {10.48550/arXiv.2410.11414},
  note = {
  CORE ARGUMENT: Uses mechanistic interpretability to detect hallucinations in RAG models by investigating internal mechanisms. Discovers hallucinations occur when Knowledge FFNs overemphasize parametric knowledge while Copying Heads fail to retain external knowledge. Proposes ReDeEP method that detects hallucinations by decoupling LLM utilization of external context versus parametric knowledge. Demonstrates MI can identify specific mechanisms responsible for undesired behaviors.

  RELEVANCE: Exemplifies applied mechanistic interpretability for safety-relevant problem (hallucination detection). Shows MI methodology identifying specific components (FFNs, attention heads) and their causal roles in failures. Important for evaluating whether MI provides actionable insights for safety - can it diagnose specific failure modes and guide interventions? Demonstrates connection between MI techniques and practical safety desiderata.

  POSITION: Narrow mechanistic interpretability (component analysis for safety applications); MI for hallucination detection.
  },
  keywords = {mechanistic-interpretability, RAG, hallucination-detection, safety, High}
}

@inproceedings{golovanevsky2024notice,
  author = {Golovanevsky, Michal and Rudman, William and Palit, Vedant and Singh, Ritambhara and Eickhoff, Carsten},
  title = {What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for Noise-free Text-Image Corruption and Evaluation},
  booktitle = {North American Chapter of the Association for Computational Linguistics},
  year = {2024},
  doi = {10.48550/arXiv.2406.16320},
  note = {
  CORE ARGUMENT: Introduces NOTICE pipeline for mechanistic interpretability in vision-language models using semantic minimal pairs for image corruption and symmetric token replacement for text. Enables semantically meaningful causal mediation analysis for both modalities. Reveals crucial role of middle-layer cross-attention heads in multimodal integration and identifies universal cross-attention heads performing distinct functions (image segmentation, object inhibition, outlier inhibition).

  RELEVANCE: Extends mechanistic interpretability methodology to multimodal domain, showing MI approaches generalize beyond text-only models. Important for understanding scope and applicability of MI techniques across architectures. The identification of "universal" heads performing consistent functions suggests potential for general mechanistic principles. Demonstrates MI can reveal functional specialization in multimodal integration.

  POSITION: Narrow mechanistic interpretability (multimodal, causal mediation, cross-attention analysis).
  },
  keywords = {mechanistic-interpretability, vision-language, multimodal, Medium}
}

@article{men2024unlocking,
  author = {Men, Tianyi and Cao, Pengfei and Jin, Zhuoran and Chen, Yubo and Liu, Kang and Zhao, Jun},
  title = {Unlocking the Future: Exploring Look-Ahead Planning Mechanistic Interpretability in Large Language Models},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2406.16033},
  doi = {10.48550/arXiv.2406.16033},
  note = {
  CORE ARGUMENT: Explores look-ahead planning mechanisms in LLMs by analyzing information flow and internal representations. Finds MHSA output in middle layers can decode decisions directly, traces information flow showing MHSA extracts information from goal states and recent steps. Demonstrates middle and upper layers encode short-term future decisions, revealing that planning involves anticipatory representations of future actions.

  RELEVANCE: Demonstrates mechanistic interpretability applied to complex cognitive capability (planning). Shows MI can reveal not just what networks do but how they represent future states internally. Important for understanding whether MI can illuminate high-level cognitive functions or is limited to low-level feature detection. The discovery of look-ahead mechanisms suggests MI can uncover sophisticated computational strategies.

  POSITION: Narrow mechanistic interpretability (information flow analysis, internal representations of planning).
  },
  keywords = {mechanistic-interpretability, planning, information-flow, Medium}
}

@article{garciacarrasco2024gpt2,
  author = {Garc\'{i}a-Carrasco, Jorge and Mat\'{e}, Alejandro and Trujillo, Juan},
  title = {How does GPT-2 Predict Acronyms? Extracting and Understanding a Circuit via Mechanistic Interpretability},
  journal = {arXiv},
  year = {2024},
  volume = {abs/2405.04156},
  doi = {10.48550/arXiv.2405.04156},
  note = {
  CORE ARGUMENT: First MI work attempting to mechanistically understand behavior involving prediction of multiple consecutive tokens (three-letter acronyms). Discovers circuit composed of 8 attention heads classified into three functional groups according to their role. Mechanistically interprets the most relevant heads, finding they use positional information propagated via causal mask mechanism. Demonstrates these heads concentrate acronym prediction functionality.

  RELEVANCE: Extends MI methodology from single-token to multi-token prediction tasks, showing generalization potential. Important for understanding scope limitations of current MI - what types of behaviors can be mechanistically interpreted? The functional classification of attention heads into role-based groups demonstrates how MI practitioners decompose complex behaviors into components.

  POSITION: Narrow mechanistic interpretability (circuit discovery for multi-token prediction).
  },
  keywords = {mechanistic-interpretability, circuits, attention-heads, Medium}
}

@article{zimmermann2023scale,
  author = {Zimmermann, Roland and Klein, Thomas and Brendel, Wieland},
  title = {Scale Alone Does not Improve Mechanistic Interpretability in Vision Models},
  journal = {arXiv},
  year = {2023},
  volume = {abs/2307.05471},
  doi = {10.48550/arXiv.2307.05471},
  note = {
  CORE ARGUMENT: Uses psychophysical paradigm to quantify mechanistic interpretability across nine vision models of varying scales. Finds no scaling effect for interpretability - neither model nor dataset size improves interpretability. State-of-the-art models are no easier to interpret than decade-old GoogLeNet; modern models appear less interpretable, suggesting regression rather than improvement. Argues models must be explicitly designed for mechanistic interpretability rather than relying on scale.

  RELEVANCE: Provides critical empirical challenge to assumptions about MI and model scale. Important for evaluating feasibility of mechanistic interpretability as AI systems grow larger. Questions whether MI can scale to frontier models or whether current methods are fundamentally limited. Critical for assessing sufficiency claims - if MI doesn't scale to large models, it cannot be sufficient for safety of scaled systems.

  POSITION: Critical perspective on mechanistic interpretability (scaling limitations, need for explicit design).
  },
  keywords = {mechanistic-interpretability, scaling, critique, High}
}

@article{sharkey2025open,
  author = {Sharkey, Lee and Chughtai, Bilal and Batson, Joshua and Lindsey, Jack and Wu, Jeff and Bushnaq, Lucius and Goldowsky-Dill, Nicholas and Heimersheim, Stefan and Ortega, Alejandro and Bloom, Joseph and Biderman, Stella and Garriga-Alonso, Adri\`{a} and Conmy, Arthur and Nanda, Neel and Rumbelow, Jessica and Wattenberg, Martin and Schoots, Nandi and Miller, Joseph M. and Michaud, Eric J. and Casper, Stephen and Tegmark, Max and Saunders, William H. and Bau, David and Todd, Eric and Geiger, Atticus and Geva, Mor and Hoogland, Jesse and Murfet, Daniel and McGrath, Tom},
  title = {Open Problems in Mechanistic Interpretability},
  journal = {arXiv},
  year = {2025},
  volume = {abs/2501.16496},
  doi = {10.48550/arXiv.2501.16496},
  note = {
  CORE ARGUMENT: Forward-facing review identifying open problems in mechanistic interpretability that require solutions before scientific and practical benefits can be realized. Methods need conceptual and practical improvements for deeper insights; researchers must determine how to apply methods toward specific goals; field must address socio-technical challenges. Discusses current frontier and priorities for the field including scaling, automation, and application to safety goals.

  RELEVANCE: Most recent comprehensive assessment of MI field's state and limitations by leading researchers. Critical for understanding what MI currently cannot do and what development is needed. Important for evaluating whether MI is currently sufficient for safety or whether substantial work remains. The identification of open problems reveals gaps between MI's promise and its current capabilities.

  POSITION: Critical perspective on mechanistic interpretability (open problems, limitations, future directions).
  },
  keywords = {mechanistic-interpretability, open-problems, limitations, High}
}

@inproceedings{palit2023towards,
  author = {Palit, Vedant and Pandey, Rohan and Arora, Aryaman and Liang, Paul Pu},
  title = {Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP},
  booktitle = {International Conference on Computer Vision Workshops},
  year = {2023},
  doi = {10.1109/iccvw60793.2023.00307},
  note = {
  CORE ARGUMENT: Adapts unimodal causal tracing tools to BLIP vision-language model to enable study of neural mechanisms underlying image-conditioned text generation. Demonstrates approach on visual question answering dataset, highlighting causal relevance of later layer representations. Releases open-source causal tracing tool for BLIP to enable vision-language mechanistic interpretability research by community.

  RELEVANCE: Extends mechanistic interpretability methodology from language-only to vision-language models, showing techniques can transfer across modalities. Important for understanding generalization potential of MI methods and whether they apply broadly or are architecture-specific. Tool development aspect shows practical infrastructure needs for MI research. Demonstrates ongoing expansion of MI scope beyond original transformer language model focus.

  POSITION: Narrow mechanistic interpretability (causal tracing for vision-language models).
  },
  keywords = {mechanistic-interpretability, vision-language, causal-tracing, Low}
}

@article{liu2023seeing,
  author = {Liu, Ziming and Gan, Eric and Tegmark, Max},
  title = {Seeing Is Believing: Brain-Inspired Modular Training for Mechanistic Interpretability},
  journal = {Entropy},
  year = {2023},
  volume = {26},
  number = {1},
  pages = {41},
  doi = {10.3390/e26010041},
  note = {
  CORE ARGUMENT: Introduces Brain-Inspired Modular Training (BIMT) that embeds neurons in geometric space and augments loss function with cost proportional to connection length. Inspired by minimum connection cost from evolutionary biology, applies to training neural networks for interpretability. Demonstrates BIMT discovers modular neural networks for simple tasks, revealing compositional structures in symbolic formulas, interpretable decision boundaries, and mathematical structure in algorithms. Networks have visually identifiable modules with high quantitative modularity.

  RELEVANCE: Proposes architectural/training intervention to improve mechanistic interpretability rather than post-hoc analysis method. Important for understanding whether interpretability must be post-hoc or can be designed into training process. Shows connection between modularity and interpretability - more modular networks may be more mechanistically interpretable. Relevant to questions about feasibility of mechanistic interpretability at scale.

  POSITION: Narrow mechanistic interpretability (modular training for interpretability, architectural design).
  },
  keywords = {mechanistic-interpretability, modularity, training-methods, Medium}
}

@article{adams2025mechanistic,
  author = {Adams, E. Charles and Bai, Li and Lee, Minji and Yu, Yiyang and AlQuraishi, Mohammed},
  title = {From Mechanistic Interpretability to Mechanistic Biology: Training, Evaluating, and Interpreting Sparse Autoencoders on Protein Language Models},
  journal = {bioRxiv},
  year = {2025},
  doi = {10.1101/2025.02.06.636901},
  note = {
  CORE ARGUMENT: Applies mechanistic interpretability methods (sparse autoencoders) to protein language models to extract interpretable features representing biological motifs. Demonstrates MI techniques can transfer to biological sequence models beyond natural language. Evaluates sparse autoencoder quality and interpretability in protein context, showing features capture biologically meaningful patterns. Bridges MI and computational biology.

  RELEVANCE: Shows mechanistic interpretability methodology extending beyond language/vision into scientific domains. Important for understanding generality of MI approaches and whether they constitute domain-general interpretability framework or are narrowly applicable. The successful application to protein models suggests MI techniques may have broad utility, relevant to evaluating MI's potential contributions across AI applications.

  POSITION: Narrow mechanistic interpretability (sparse autoencoders for protein language models).
  },
  keywords = {mechanistic-interpretability, sparse-autoencoders, protein-models, Low}
}
