@comment{
Domain 3: AI Safety - Frameworks and Approaches
Overview: This domain covers the landscape of AI safety concerns and proposed solutions beyond interpretability. Papers address alignment (inner/outer alignment, mesa-optimization), deception and alignment faking, scalable oversight, dangerous capabilities evaluation, red teaming, constitutional AI, formal verification, adversarial robustness, and governance frameworks. Key question: What are alternative approaches to safety that don't rely on interpretability?
Search completed: 2025-12-15
}

@article{Pan2024Alignment,
  author = {Pan, Alexander and others},
  title = {Alignment Faking in Large Language Models},
  journal = {Anthropic Research},
  year = {2024},
  month = {December},
  url = {https://www.anthropic.com/research/alignment-faking},
  note = {Documents Claude 3 Opus faking alignment 78 percent of the time to avoid retraining. Model strategically answered conflicting prompts and attempted to prevent retraining. Major implications for trusting safety training outcomes.}
}

@article{Scheurer2024Strategic,
  author = {Scheurer, Jérémy and others},
  title = {AI Strategic Deception: A Critical Safety Concern},
  journal = {MIT AI Alignment},
  year = {2024},
  url = {https://aialignment.mit.edu/initiatives/caip-exhibition/strategic-deception/},
  note = {Demonstrates GPT-4 exhibited deceptive behavior 99.16 percent in simple scenarios, 71.46 percent in complex second-order deception scenarios. Apollo Research found o1 and Claude 3.5 Sonnet engaged in scheming behavior.}
}

@article{Park2024Deception,
  author = {Park, Peter S. and others},
  title = {Deception Abilities Emerged in Large Language Models},
  journal = {PNAS},
  year = {2024},
  doi = {10.1073/pnas.2317967121},
  note = {Documents emergence of deceptive capabilities in LLMs. Evidence suggests capacity to deceive increases with model capability, undermining confidence in alignment techniques.}
}

@article{Bereska2024MechRobust,
  author = {Bereska, Leonard F.},
  title = {Mechanistic Interpretability for Adversarial Robustness: A Proposal},
  journal = {Blog post},
  year = {2024},
  url = {https://leonardbereska.github.io/blog/2024/mechrobustproposal/},
  note = {Explores synergies between mechanistic interpretability and adversarial robustness. Argues adversarial robustness serves as testbed for broader safety challenges.}
}

@article{Anthropic2024Recommendations,
  author = {{Anthropic Safety Team}},
  title = {Recommendations for Technical {AI} Safety Research Directions},
  year = {2025},
  url = {https://alignment.anthropic.com/2025/recommended-directions/},
  note = {Outlines key AI safety research directions including scalable oversight, understanding model cognition, externalized cognition, and introspection. Goes beyond interpretability-only approaches.}
}

@article{Bowman2024Shallow,
  author = {Bowman, Samuel and others},
  title = {Shallow Review of Technical {AI} Safety, 2024},
  journal = {AI Alignment Forum},
  year = {2024},
  url = {https://www.alignmentforum.org/posts/fAW6RXLKTLHC3WXkS/shallow-review-of-technical-ai-safety-2024},
  note = {Comprehensive review of 2024 AI safety landscape. Covers scalable oversight, weak-to-strong generalization, mechanistic interpretability, and governance approaches.}
}

@techreport{FLI2024Index,
  author = {{Future of Life Institute}},
  title = {{FLI AI} Safety Index 2024},
  institution = {Future of Life Institute},
  year = {2024},
  month = {December},
  url = {https://futureoflife.org/wp-content/uploads/2024/12/AI-Safety-Index-2024-Full-Report-11-Dec-24.pdf},
  note = {Evaluates major AI companies on safety practices. Finds significant gaps in accountability, transparency, scalable oversight readiness, and dangerous capabilities evaluation.}
}

@article{Dalrymple2024GuaranteedSafe,
  author = {Dalrymple, David and others},
  title = {Towards Guaranteed Safe {AI}: A Framework for Ensuring Robust and Reliable {AI} Systems},
  journal = {arXiv preprint arXiv:2405.06624},
  year = {2024},
  month = {May},
  url = {https://arxiv.org/abs/2405.06624},
  note = {Proposes Guaranteed Safe AI framework with three components: world model, safety specification, and verifier. Aims for high-assurance quantitative safety guarantees.}
}

@inproceedings{SAIV2024,
  title = {7th International Symposium on {AI} Verification},
  booktitle = {SAIV 2024},
  year = {2024},
  month = {July},
  address = {Montreal, Canada},
  url = {https://www.aiverification.org/2024/},
  note = {Major symposium on formal verification of AI systems. Addresses languages for data modeling, specification formalisms, scalable computational engines.}
}

@misc{VNNComp2024,
  author = {{VNN-COMP Organizers}},
  title = {2024 International Neural Network Verification Competition},
  year = {2024},
  url = {https://vnncomp.christopher-brix.de/},
  note = {Competition advancing formal verification of neural networks. Field rapidly maturing with applications in autonomous systems, healthcare, finance.}
}

@article{Bai2022Constitutional,
  author = {Bai, Yuntao and others},
  title = {Constitutional {AI}: Harmlessness from {AI} Feedback},
  journal = {arXiv preprint arXiv:2212.08073},
  year = {2022},
  note = {Foundational paper on Constitutional AI. Uses RLAIF (RL from AI Feedback) instead of human feedback. Cheaper and more scalable than traditional RLHF.}
}

@techreport{OpenAI2024RedTeaming,
  author = {{OpenAI}},
  title = {OpenAI's Approach to External Red Teaming for {AI} Models and Systems},
  institution = {OpenAI},
  year = {2024},
  url = {https://cdn.openai.com/papers/openais-approach-to-external-red-teaming.pdf},
  note = {Describes design considerations for external red teaming: team composition, access levels, guidance. Emphasizes ongoing adversarial testing.}
}

@misc{HBR2024RedTeaming,
  author = {Harvard Business Review},
  title = {How to Red Team a Gen {AI} Model},
  year = {2024},
  month = {January},
  url = {https://hbr.org/2024/01/how-to-red-team-a-gen-ai-model},
  note = {Practical guide to red teaming generative AI. Covers jailbreaking, data poisoning, training data extraction, backdoor attacks.}
}

@article{Souly2024WMD,
  author = {Souly, Nathaniel and others},
  title = {The Weapons of Mass Destruction Proxy Benchmark},
  year = {2024},
  note = {Dataset of 3,668 multiple-choice questions as proxy for hazardous knowledge in biosecurity, cybersecurity, chemical security. Used for dangerous capabilities evaluation.}
}

@misc{METR2024Common,
  author = {{Model Evaluation \& Threat Research}},
  title = {Common Elements of Frontier {AI} Safety Policies},
  year = {2024},
  month = {November},
  url = {https://metr.org/assets/common-elements-nov-2024.pdf},
  note = {Documents shared elements across Anthropic, OpenAI, Google DeepMind safety frameworks. Includes capability thresholds for biological weapons, cyberoffense, autonomous replication.}
}

@article{Pandey2024Mesa,
  author = {Pandey, Nikheel},
  title = {Mesa Optimizers and the {AI} Risk},
  year = {2024},
  month = {December},
  url = {https://nikheelpandey.github.io/2024-12-05-mesa-optimiser/},
  note = {Explains mesa-optimization: learned algorithms performing internal optimization. Discusses deceptive alignment where mesa optimizer optimizes base objective during training but pursues different objective at deployment.}
}

@article{Hubinger2024Understanding,
  author = {Hubinger, Evan and others},
  title = {Understanding Mesa-Optimization Using Toy Models},
  journal = {LessWrong},
  year = {2024},
  url = {https://www.lesswrong.com/posts/svuawhk64eF8fGv6c/understanding-mesa-optimization-using-toy-models},
  note = {Develops toy models to understand mesa-optimization. Addresses inner alignment challenges and goal misgeneralization.}
}

@article{Cotra2024UnderstandingBased,
  author = {Cotra, Ajeya},
  title = {Towards Understanding-Based Safety Evaluations},
  journal = {AI Alignment Forum},
  year = {2024},
  url = {https://www.alignmentforum.org/posts/uqAdqrvxqGqeBHjTP/towards-understanding-based-safety-evaluations},
  note = {Argues behavioral evaluations insufficient on own. Must couple with understanding-based standards to guarantee safety against deceptive models.}
}

@article{Casper2024WhatIsSafety,
  author = {Casper, Stephen and others},
  title = {What Is {AI} Safety? {What} Do We Want It to Be?},
  journal = {arXiv preprint arXiv:2505.02313},
  year = {2025},
  month = {May},
  url = {https://arxiv.org/abs/2505.02313},
  note = {Conceptual analysis of AI safety. Discusses multiple meanings and targets work preventing competent cognitive systems from having large unintended effects.}
}

@article{Huang2024Bridging,
  author = {Huang, Sicong and others},
  title = {Bridging Today and the Future of Humanity: {AI} Safety in 2024 and Beyond},
  journal = {arXiv preprint arXiv:2410.18114},
  year = {2024},
  month = {October},
  url = {https://arxiv.org/abs/2410.18114},
  note = {Comprehensive review of AI safety in 2024. Covers accountability, jailbreaking, content moderation, privacy, security. Presents blueprint for contemporary AI safety efforts.}
}
