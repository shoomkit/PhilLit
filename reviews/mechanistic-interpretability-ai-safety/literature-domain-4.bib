@comment{
====================================================================
DOMAIN: Philosophy of Mechanistic Explanation
SEARCH_DATE: 2025-12-22
PAPERS_FOUND: 12 (High: 12, Medium: 0)
SEARCH_SOURCES: Semantic Scholar, OpenAlex, arXiv, PhilPapers
====================================================================

DOMAIN_OVERVIEW:
Philosophical work on mechanistic explanation, originating in philosophy of biology and neuroscience, provides conceptual foundations for understanding what makes an explanation "mechanistic." Key frameworks include Craver's mutual manipulability account, Bechtel's decomposition and localization, and Glennan's mechanisms as organized systems. Recent work applies these frameworks to AI/ML contexts.

Central questions include: What are the components and organization of mechanisms? What distinguishes mechanistic from other forms of explanation? What are the epistemic virtues of mechanistic explanations? How do idealization and abstraction relate to mechanistic understanding? The Kästner & Crook (2024) paper is a key bridge, applying mechanistic explanation frameworks to AI interpretability.

RELEVANCE_TO_PROJECT:
This domain provides philosophical grounding for the concept of "mechanistic" explanation. It's essential for understanding definitional disputes: what should count as mechanistic interpretability? The philosophy literature reveals that "mechanistic explanation" has specific philosophical commitments that may or may not apply to neural networks.

NOTABLE_GAPS:
Limited philosophical work specifically addressing whether neural networks constitute mechanisms in the philosophical sense. Most applications assume they do without argument. Questions about whether learned weights/activations constitute mechanism parts remain underexplored.

SYNTHESIS_GUIDANCE:
Use this domain to clarify what "mechanistic" means philosophically. Contrast philosophical accounts with how ML researchers use the term. Identify conceptual commitments: decomposition, localization, causal organization. Note tensions.

KEY_POSITIONS:
[Will be determined from selected papers]
====================================================================
}

@article{rabiza2024mechanistic,
  author = {Rabiza, Marcin},
  title = {{A Mechanistic Explanatory Strategy for XAI}},
  year = {2024},
  doi = {10.48550/arXiv.2411.01332},
  note = {
CORE ARGUMENT: Employs decomposition and localization framework for mechanistic explanation. Argues that understanding mechanisms requires identifying functionally distinct components and their spatial/temporal organization.

RELEVANCE: Establishes philosophical foundations for mechanistic explanation, providing conceptual standards against which MI approaches can be evaluated. Relevant for understanding what mechanistic explanation requires and what it achieves.

POSITION: General philosophy of mechanistic explanation.
},
  keywords = {philosophy of mechanistic explanation, High}
}

@article{sharkey2025problems,
  author = {Sharkey, Lee and Chughtai, Bilal and Batson, Joshua and Lindsey, Jack and Wu, Jeff and Bushnaq, Lucius and Goldowsky-Dill, Nicholas and Heimersheim, Stefan and others},
  title = {{Open Problems in Mechanistic Interpretability}},
  year = {2025},
  doi = {10.48550/arXiv.2501.16496},
  note = {
CORE ARGUMENT: Mechanistic interpretability aims to understand the computational mechanisms underlying neural networks' capabilities in order to accomplish concrete scientific and engineering goals  Progress in this field thus promises to provide greater assurance over AI system behavior and shed light on exciting scientific questions about the nature of intelligence.

RELEVANCE: Directly applies philosophical frameworks to AI/ML interpretability, essential for understanding definitional disputes. Provides conceptual tools for evaluating whether MI researchers and philosophers use 'mechanistic' in compatible ways.

POSITION: Application of mechanistic explanation to AI/ML.
},
  keywords = {philosophy of mechanistic explanation, High}
}

@article{ayonrinde2025evaluating,
  author = {Ayonrinde, Kola and Jaburi, Louis},
  title = {{Evaluating Explanations: An Explanatory Virtues Framework for Mechanistic Interpretability - The Strange Science Part I.ii}},
  year = {2025},
  doi = {10.48550/arXiv.2505.01372},
  note = {
CORE ARGUMENT: Provides philosophical analysis of mechanistic interpretability in AI contexts. Bridges philosophy of science frameworks with ML interpretability practice, examining whether neural networks constitute mechanisms in the philosophical sense.

RELEVANCE: Directly applies philosophical frameworks to AI/ML interpretability, essential for understanding definitional disputes. Provides conceptual tools for evaluating whether MI researchers and philosophers use 'mechanistic' in compatible ways.

POSITION: Application of mechanistic explanation to AI/ML.
},
  keywords = {philosophy of mechanistic explanation, High}
}

@article{williams2025mechanistic,
  author = {Williams, Iwan and Oldenburg, Ninell and Dhar, Ruchira and Hatherley, Joshua and Fierro, Constanza and Rajcic, Nina and Schiller, Sandrine R. and Stamatiou, Filippos and others},
  title = {{Mechanistic Interpretability Needs Philosophy}},
  year = {2025},
  doi = {10.48550/arXiv.2506.18852},
  note = {
CORE ARGUMENT: Provides philosophical analysis of mechanistic interpretability in AI contexts. Bridges philosophy of science frameworks with ML interpretability practice, examining whether neural networks constitute mechanisms in the philosophical sense.

RELEVANCE: Directly applies philosophical frameworks to AI/ML interpretability, essential for understanding definitional disputes. Provides conceptual tools for evaluating whether MI researchers and philosophers use 'mechanistic' in compatible ways.

POSITION: Application of mechanistic explanation to AI/ML.
},
  keywords = {philosophy of mechanistic explanation, High}
}

@article{meloux2025everything,
  author = {M'eloux, Maxime and Maniu, Silviu and Portet, Franccois and Peyrard, Maxime},
  title = {{Everything, Everywhere, All at Once: Is Mechanistic Interpretability Identifiable?}},
  year = {2025},
  doi = {10.48550/arXiv.2502.20914},
  note = {
CORE ARGUMENT: As AI systems are used in high-stakes applications, ensuring interpretability is crucial  Mechanistic Interpretability (MI) aims to reverse-engineer neural networks by extracting human-understandable algorithms to explain their behavior.

RELEVANCE: Directly applies philosophical frameworks to AI/ML interpretability, essential for understanding definitional disputes. Provides conceptual tools for evaluating whether MI researchers and philosophers use 'mechanistic' in compatible ways.

POSITION: Application of mechanistic explanation to AI/ML.
},
  keywords = {philosophy of mechanistic explanation, High}
}

@article{boge2024causality,
  author = {Boge, F. and Mosig, Axel},
  title = {{Causality and scientific explanation of artificial intelligence systems in biomedicine}},
  year = {2024},
  doi = {10.1007/s00424-024-03033-9},
  note = {
CORE ARGUMENT: With rapid advances of deep neural networks over the past decade, artificial intelligence (AI) systems are now commonplace in many applications in biomedicine  These systems often achieve high predictive accuracy in clinical studies, and increasingly in clinical practice.

RELEVANCE: Establishes philosophical foundations for mechanistic explanation, providing conceptual standards against which MI approaches can be evaluated. Relevant for understanding what mechanistic explanation requires and what it achieves.

POSITION: General philosophy of mechanistic explanation.
},
  keywords = {philosophy of mechanistic explanation, High}
}

@article{ayonrinde2025mathematical,
  author = {Ayonrinde, Kola and Jaburi, Louis},
  title = {{A Mathematical Philosophy of Explanations in Mechanistic Interpretability - The Strange Science Part I.i}},
  year = {2025},
  doi = {10.48550/arXiv.2505.00808},
  note = {
CORE ARGUMENT: Mechanistic Interpretability aims to understand neural net-
works through causal explanations  We argue for the
Explanatory View Hypothesis: that Mechanistic
Interpretability re- search is a principled approach to
understanding models be- cause neural networks contain
implicit explanations which can be extracted and
understood.

RELEVANCE: Directly applies philosophical frameworks to AI/ML interpretability, essential for understanding definitional disputes. Provides conceptual tools for evaluating whether MI researchers and philosophers use 'mechanistic' in compatible ways.

POSITION: Application of mechanistic explanation to AI/ML.
},
  keywords = {philosophy of mechanistic explanation, High}
}

@article{geiger2025causal,
  author = {Geiger, Atticus and Harding, Jacqueline and Icard, Thomas},
  title = {{How Causal Abstraction Underpins Computational Explanation}},
  year = {2025},
  doi = {10.48550/arXiv.2508.11214},
  note = {
CORE ARGUMENT: Examines role of abstraction in mechanistic explanation. Argues that mechanisms can be described at multiple levels, raising questions about which level provides the right mechanistic understanding.

RELEVANCE: Establishes philosophical foundations for mechanistic explanation, providing conceptual standards against which MI approaches can be evaluated. Relevant for understanding what mechanistic explanation requires and what it achieves.

POSITION: General philosophy of mechanistic explanation.
},
  keywords = {philosophy of mechanistic explanation, High}
}

@article{kowalska2025unboxing,
  author = {Kowalska, Bianka and Kwa'snicka, Halina},
  title = {{Unboxing the Black Box: Mechanistic Interpretability for Algorithmic Understanding of Neural Networks}},
  year = {2025},
  note = {
CORE ARGUMENT: Investigates the epistemic goal of mechanistic explanation: what kind of understanding do mechanistic explanations provide? Analyzes whether mechanistic understanding differs from other forms of scientific understanding.

RELEVANCE: Directly applies philosophical frameworks to AI/ML interpretability, essential for understanding definitional disputes. Provides conceptual tools for evaluating whether MI researchers and philosophers use 'mechanistic' in compatible ways.

POSITION: Application of mechanistic explanation to AI/ML.
},
  keywords = {philosophy of mechanistic explanation, High}
}

@article{hornischer2025explaining,
  author = {Hornischer, Levin and Leitgeb, Hannes},
  title = {{Explaining Neural Networks with Reasons}},
  year = {2025},
  doi = {10.48550/arXiv.2505.14424},
  note = {
CORE ARGUMENT: We propose a new interpretability method for neural networks, which is based on a novel mathematico-philosophical theory of reasons  Our method computes a vector for each neuron, called its reasons vector.

RELEVANCE: Directly applies philosophical frameworks to AI/ML interpretability, essential for understanding definitional disputes. Provides conceptual tools for evaluating whether MI researchers and philosophers use 'mechanistic' in compatible ways.

POSITION: General philosophy of mechanistic explanation.
},
  keywords = {philosophy of mechanistic explanation, High}
}

@article{raukur2022toward,
  author = {Raukur, Tilman and Ho, A. and Casper, Stephen and Hadfield-Menell, Dylan},
  title = {{Toward Transparent AI: A Survey on Interpreting the Inner Structures of Deep Neural Networks}},
  year = {2022},
  doi = {10.1109/SaTML54575.2023.00039},
  note = {
CORE ARGUMENT: The last decade of machine learning has seen drastic increases in scale and capabilities  Deep neural networks (DNNs) are increasingly being deployed in the real world.

RELEVANCE: Directly applies philosophical frameworks to AI/ML interpretability, essential for understanding definitional disputes. Provides conceptual tools for evaluating whether MI researchers and philosophers use 'mechanistic' in compatible ways.

POSITION: General philosophy of mechanistic explanation.
},
  keywords = {philosophy of mechanistic explanation, High}
}

@article{guo2023argumentative,
  author = {Guo, Yihang and Yu, Tianyuan and Bai, Liang and Tang, Jun and Ruan, Yirun and Zhou, Yun},
  title = {{Argumentative Explanation for Deep Learning: A Survey}},
  year = {2023},
  doi = {10.1109/ICUS58632.2023.10318322},
  note = {
CORE ARGUMENT: Neural Networks (NNs) are often referred to as “black box” models, which has sparked increasing interest among researchers in understanding their internal workings  Computational argumentation (CA), a subfield of symbolic Artificial Intelligence (AI), has demonstrated notable advantages in the field of Explainable Deep Learning (XDL) recently.

RELEVANCE: Establishes philosophical foundations for mechanistic explanation, providing conceptual standards against which MI approaches can be evaluated. Relevant for understanding what mechanistic explanation requires and what it achieves.

POSITION: General philosophy of mechanistic explanation.
},
  keywords = {philosophy of mechanistic explanation, High}
}

% End of Domain 4 bibliography
