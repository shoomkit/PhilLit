{
  "status": "success",
  "source": "arxiv",
  "query": "all:digital cognition artificial intelligence AND cat:cs.AI",
  "results": [
    {
      "arxiv_id": "2602.10117",
      "title": "Biases in the Blind Spot: Detecting What LLMs Fail to Mention",
      "authors": [
        "Iv\u00e1n Arcuschin",
        "David Chanin",
        "Adri\u00e0 Garriga-Alonso",
        "Oana-Maria Camburu"
      ],
      "abstract": "Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefined categories and hand-crafted datasets. In this work, we introduce a fully automated, black-box pipeline for detecting task-specific unverbalized biases. Given a task dataset, the pipeline uses LLM autoraters to generate candidate bias concepts. It then tests each concept on progressively larger input samples by generating positive and negative variations, and applies statistical techniques for multiple testing and early stopping. A concept is flagged as an unverbalized bias if it yields statistically significant performance differences while not being cited as justification in the model's CoTs. We evaluate our pipeline across six LLMs on three decision tasks (hiring, loan approval, and university admissions). Our technique automatically discovers previously unknown biases in these models (e.g., Spanish fluency, English proficiency, writing formality). In the same run, the pipeline also validates biases that were manually identified by prior work (gender, race, religion, ethnicity). More broadly, our proposed approach provides a practical, scalable path to automatic task-specific bias discovery.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10117v1",
      "url": "https://arxiv.org/abs/2602.10117"
    },
    {
      "arxiv_id": "2602.10104",
      "title": "Olaf-World: Orienting Latent Actions for Video World Modeling",
      "authors": [
        "Yuxin Jiang",
        "Yuchao Gu",
        "Ivor W. Tsang",
        "Mike Zheng Shou"
      ],
      "abstract": "Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq$\u0394$-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10104v1",
      "url": "https://arxiv.org/abs/2602.10104"
    },
    {
      "arxiv_id": "2602.10100",
      "title": "Towards Explainable Federated Learning: Understanding the Impact of Differential Privacy",
      "authors": [
        "J\u00falio Oliveira",
        "Rodrigo Ferreira",
        "Andr\u00e9 Riker",
        "Glaucio H. S. Carvalho",
        "Eirini Eleni Tsilopoulou"
      ],
      "abstract": "Data privacy and eXplainable Artificial Intelligence (XAI) are two important aspects for modern Machine Learning systems. To enhance data privacy, recent machine learning models have been designed as a Federated Learning (FL) system. On top of that, additional privacy layers can be added, via Differential Privacy (DP). On the other hand, to improve explainability, ML must consider more interpretable approaches with reduced number of features and less complex internal architecture. In this context, this paper aims to achieve a machine learning (ML) model that combines enhanced data privacy with explainability. So, we propose a FL solution, called Federated EXplainable Trees with Differential Privacy (FEXT-DP), that: (i) is based on Decision Trees, since they are lightweight and have superior explainability than neural networks-based FL systems; (ii) provides additional layer of data privacy protection applying Differential Privacy (DP) to the Tree-Based model. However, there is a side effect adding DP: it harms the explainability of the system. So, this paper also presents the impact of DP protection on the explainability of the ML model. The carried out performance assessment shows improvements of FEXT-DP in terms of a faster training, i.e., numbers of rounds, Mean Squared Error and explainability.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.CR"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10100v1",
      "url": "https://arxiv.org/abs/2602.10100"
    },
    {
      "arxiv_id": "2602.10097",
      "title": "Step-resolved data attribution for looped transformers",
      "authors": [
        "Georgios Kaissis",
        "David Mildenberger",
        "Juan Felipe Gomez",
        "Martin J. Menten",
        "Eleni Triantafillou"
      ],
      "abstract": "We study how individual training examples shape the internal computation of looped transformers, where a shared block is applied for $\u03c4$ recurrent iterations to enable latent reasoning. Existing training-data influence estimators such as TracIn yield a single scalar score that aggregates over all loop iterations, obscuring when during the recurrent computation a training example matters. We introduce \\textit{Step-Decomposed Influence (SDI)}, which decomposes TracIn into a length-$\u03c4$ influence trajectory by unrolling the recurrent computation graph and attributing influence to specific loop iterations. To make SDI practical at transformer scale, we propose a TensorSketch implementation that never materialises per-example gradients. Experiments on looped GPT-style models and algorithmic reasoning tasks show that SDI scales excellently, matches full-gradient baselines with low error and supports a broad range of data attribution and interpretability tasks with per-step insights into the latent reasoning process.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10097v1",
      "url": "https://arxiv.org/abs/2602.10097"
    },
    {
      "arxiv_id": "2602.10095",
      "title": "Causality in Video Diffusers is Separable from Denoising",
      "authors": [
        "Xingjian Bai",
        "Guande He",
        "Zhengqi Li",
        "Eli Shechtman",
        "Xun Huang",
        "Zongze Wu"
      ],
      "abstract": "Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI",
        "cs.LG"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10095v1",
      "url": "https://arxiv.org/abs/2602.10095"
    },
    {
      "arxiv_id": "2602.10090",
      "title": "Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning",
      "authors": [
        "Zhaoyang Wang",
        "Canwen Xu",
        "Boyi Liu",
        "Yite Wang",
        "Siwei Han",
        "Zhewei Yao",
        "Huaxiu Yao",
        "Yuxiong He"
      ],
      "abstract": "Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.CL",
        "cs.LG"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10090v1",
      "url": "https://arxiv.org/abs/2602.10090"
    },
    {
      "arxiv_id": "2602.10085",
      "title": "CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs",
      "authors": [
        "Richard Bornemann",
        "Pierluigi Vito Amadori",
        "Antoine Cully"
      ],
      "abstract": "Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\\href{https://sites.google.com/view/code-sharp/homepage}{here}$.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10085v1",
      "url": "https://arxiv.org/abs/2602.10085"
    },
    {
      "arxiv_id": "2602.10081",
      "title": "Anagent For Enhancing Scientific Table & Figure Analysis",
      "authors": [
        "Xuehang Guo",
        "Zhiyong Lu",
        "Tom Hope",
        "Qingyun Wang"
      ],
      "abstract": "In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \\& figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \\& figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\\uparrow 13.43\\%$ in training-free settings and $\\uparrow 42.12\\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \\& figure analysis. Our project page: https://xhguo7.github.io/Anagent/.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10081v1",
      "url": "https://arxiv.org/abs/2602.10081"
    },
    {
      "arxiv_id": "2602.10069",
      "title": "Humanoid Factors: Design Principles for AI Humanoids in Human Worlds",
      "authors": [
        "Xinyuan Liu",
        "Eren Sadikoglu",
        "Ransalu Senanayake",
        "Lixiao Huang"
      ],
      "abstract": "Human factors research has long focused on optimizing environments, tools, and systems to account for human performance. Yet, as humanoid robots begin to share our workplaces, homes, and public spaces, the design challenge expands. We must now consider not only factors for humans but also factors for humanoids, since both will coexist and interact within the same environments. Unlike conventional machines, humanoids introduce expectations of human-like behavior, communication, and social presence, which reshape usability, trust, and safety considerations. In this article, we introduce the concept of humanoid factors as a framework structured around four pillars - physical, cognitive, social, and ethical - that shape the development of humanoids to help them effectively coexist and collaborate with humans. This framework characterizes the overlap and divergence between human capabilities and those of general-purpose humanoids powered by AI foundation models. To demonstrate our framework's practical utility, we then apply the framework to evaluate a real-world humanoid control algorithm, illustrating how conventional task completion metrics in robotics overlook key human cognitive and interaction principles. We thus position humanoid factors as a foundational framework for designing, evaluating, and governing sustained human-humanoid coexistence.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10069v1",
      "url": "https://arxiv.org/abs/2602.10069"
    },
    {
      "arxiv_id": "2602.10066",
      "title": "Programmable and nonvolatile computing with composition tuning in thin film lithium niobate",
      "authors": [
        "Abhiram Devata",
        "Axel Maga\u00f1a Ponce",
        "David Barton"
      ],
      "abstract": "Matrix-vector multiplications are fundamental operations in artificial intelligence and high-throughput computations, and are executed repeatedly during training and inference. Their high energy cost in electronic processors motivate scalable photonic computing approaches that reduce the energy required per operation. Thin film lithium niobate (TFLN) is a dominant photonic platform due to its large electro-optic effect. However, it lacks nonvolatile index tuning mechanisms, which promise to pave the way for energy-efficient photonic computing. Here, we explore electrochemical lithiation as a route to nonvolatile matrix-vector multiplications in TFLN. The LiNbO3 phase is stable at room temperature over a 2% Li composition window with an associated composition-dependent refractive index. We computationally demonstrate this as a programmable, low-loss approach to perform matrix-vector multiplications by using composition to control matrix weights. We design Mach-Zehnder interferometers to perform image processing tasks under realistic material loss constraints. We also design microring resonators for iterative weight updates, using gradient descent training to program target matrix operations with matrix-vector multiplication accuracy validated at 1.5% average relative error. These demonstrations show a facile route towards nonvolatile photonic computing in TFLN, addressing a critical requirement for energy-efficient photonic matrix operations at scale.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "physics.optics",
      "categories": [
        "physics.optics"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10066v1",
      "url": "https://arxiv.org/abs/2602.10066"
    },
    {
      "arxiv_id": "2602.10063",
      "title": "Chain of Mindset: Reasoning with Adaptive Cognitive Modes",
      "authors": [
        "Tianyi Jiang",
        "Arctanx An",
        "Hengyi Feng",
        "Naixin Zhai",
        "Haodong Li",
        "Xiaomin Yu",
        "Jiahui Liu",
        "Hanwen Du",
        "Shuo Zhang",
        "Zhi Yang",
        "Jie Huang",
        "Yuhua Li",
        "Yongxin Ni",
        "Huacan Wang",
        "Ronghao Chen"
      ],
      "abstract": "Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\\% and 4.72\\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \\href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10063v1",
      "url": "https://arxiv.org/abs/2602.10063"
    },
    {
      "arxiv_id": "2602.10054",
      "title": "AIDED: Augmenting Interior Design with Human Experience Data for Designer-AI Co-Design",
      "authors": [
        "Yang Chen Lin",
        "Chen-Ying Chen",
        "Kai-Hsin Hou",
        "Hung-Yu Chen",
        "Po-Chih Kuo"
      ],
      "abstract": "Interior design often struggles to capture the subtleties of client experience, leaving gaps between what clients feel and what designers can act upon. We present AIDED, a designer-AI co-design workflow that integrates multimodal client data into generative AI (GAI) design processes. In a within-subjects study with twelve professional designers, we compared four modalities: baseline briefs, gaze heatmaps, questionnaire visualizations, and AI-predicted overlays. Results show that questionnaire data were trusted, creativity-enhancing, and satisfying; gaze heatmaps increased cognitive load; and AI-predicted overlays improved GAI communication but required natural language mediation to establish trust. Interviews confirmed that an authenticity-interpretability trade-off is central to balancing client voices with professional control. Our contributions are: (1) a system that incorporates experiential client signals into GAI design workflows; (2) empirical evidence of how different modalities affect design outcomes; and (3) implications for future AI tools that support human-data interaction in creative practice.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.HC",
      "categories": [
        "cs.HC"
      ],
      "doi": "10.1145/3772318.3791378",
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10054v1",
      "url": "https://arxiv.org/abs/2602.10054"
    },
    {
      "arxiv_id": "2602.10053",
      "title": "The Architecture of Illusion: Network Opacity and Strategic Escalation",
      "authors": [
        "Raman Ebrahimi",
        "Sepehr Ilami",
        "Babak Heydari",
        "Isabel Trevino",
        "Massimo Franceschetti"
      ],
      "abstract": "Standard models of bounded rationality typically assume agents either possess accurate knowledge of the population's reasoning abilities (Cognitive Hierarchy) or hold dogmatic, degenerate beliefs (Level-$k$). We introduce the ``Connected Minds'' model, which unifies these frameworks by integrating iterative reasoning with a parameterized network bias. We posit that agents do not observe the global population; rather, they observe a sample biased by their network position, governed by a locality parameter $p$ representing algorithmic ranking, social homophily, or information disclosure. We show that this parameter acts as a continuous bridge: the model collapses to the myopic Level-$k$ recursion as networks become opaque ($p \\to 0$) and recovers the standard Cognitive Hierarchy model under full transparency ($p=1$). Theoretically, we establish that network opacity induces a \\emph{Sophisticated Bias}, causing agents to systematically overestimate the cognitive depth of their opponents while preserving the log-concavity of belief distributions. This makes $p$ an actionable lever: a planner or platform can tune transparency -- globally or by segment (a personalized $p_k$) -- to shape equilibrium behavior. From a mechanism design perspective, we derive the \\emph{Escalation Principle}: in games of strategic complements, restricting information can maximize aggregate effort by trapping agents in echo chambers where they compete against hallucinated, high-sophistication peers. Conversely, we identify a \\emph{Transparency Reversal} for coordination games, where maximizing network visibility is required to minimize variance and stabilize outcomes. Our results suggest that network topology functions as a cognitive zoom lens, determining whether agents behave as local imitators or global optimizers.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.GT",
      "categories": [
        "cs.GT",
        "econ.TH"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10053v1",
      "url": "https://arxiv.org/abs/2602.10053"
    },
    {
      "arxiv_id": "2602.10048",
      "title": "Long Chain-of-Thought Compression via Fine-Grained Group Policy Optimization",
      "authors": [
        "Xinchen Han",
        "Hossam Afifi",
        "Michel Marot",
        "Xilu Wang",
        "Lu Yin"
      ],
      "abstract": "Large Language Models (LLMs) often generate unnecessarily verbose Chain-of-Thought (CoT) reasoning that increases computational costs and latency without proportional performance gains. In this paper, we propose \\textbf{F}ine-grained \\textbf{G}roup policy \\textbf{O}ptimization (\\textbf{FGO}), a Reinforcement Learning (RL) algorithm that refines group responses by subdividing them and assigning appropriate weights based on length and entropy, thereby enabling effective CoT compression. Meanwhile, as an enhanced variant of Group Relative Policy Optimization (GRPO), FGO successfully addresses two major limitations of the GRPO: inefficient data utilization and entropy collapse. We evaluate FGO on multiple reasoning LLMs and benchmarks, including MATH500, AIME24, AMC23, and Minerva. Experimental results show that FGO achieves efficient CoT compression without degrading performance, and simultaneously resolves the key limitations of GRPO.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10048v1",
      "url": "https://arxiv.org/abs/2602.10048"
    },
    {
      "arxiv_id": "2602.10044",
      "title": "Optimistic World Models: Efficient Exploration in Model-Based Deep Reinforcement Learning",
      "authors": [
        "Akshay Mete",
        "Shahid Aamir Sheikh",
        "Tzu-Hsiang Lin",
        "Dileep Kalathil",
        "P. R. Kumar"
      ],
      "abstract": "Efficient exploration remains a central challenge in reinforcement learning (RL), particularly in sparse-reward environments. We introduce Optimistic World Models (OWMs), a principled and scalable framework for optimistic exploration that brings classical reward-biased maximum likelihood estimation (RBMLE) from adaptive control into deep RL. In contrast to upper confidence bound (UCB)-style exploration methods, OWMs incorporate optimism directly into model learning by augmentation with an optimistic dynamics loss that biases imagined transitions toward higher-reward outcomes. This fully gradient-based loss requires neither uncertainty estimates nor constrained optimization. Our approach is plug-and-play with existing world model frameworks, preserving scalability while requiring only minimal modifications to standard training procedures. We instantiate OWMs within two state-of-the-art world model architectures, leading to Optimistic DreamerV3 and Optimistic STORM, which demonstrate significant improvements in sample efficiency and cumulative return compared to their baseline counterparts.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI",
        "eess.SY"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10044v1",
      "url": "https://arxiv.org/abs/2602.10044"
    },
    {
      "arxiv_id": "2602.10043",
      "title": "Simple Image Processing and Similarity Measures Can Link Data Samples across Databases through Brain MRI",
      "authors": [
        "Gaurang Sharma",
        "Harri Polonen",
        "Juha Pajula",
        "Jutta Suksi",
        "Jussi Tohka"
      ],
      "abstract": "Head Magnetic Resonance Imaging (MRI) is routinely collected and shared for research under strict regulatory frameworks. These frameworks require removing potential identifiers before sharing. But, even after skull stripping, the brain parenchyma contains unique signatures that can match other MRIs from the same participants across databases, posing a privacy risk if additional data features are available. Current regulatory frameworks often mandate evaluating such risks based on the assessment of a certain level of reasonableness. Prior studies have already suggested that a brain MRI could enable participant linkage, but they have relied on training-based or computationally intensive methods.   Here, we demonstrate that linking an individual's skull-stripped T1-weighted MRI, which may lead to re-identification if other identifiers are available, is possible using standard preprocessing followed by image similarity computation. Nearly perfect linkage accuracy was achieved in matching data samples across various time intervals, scanner types, spatial resolutions, and acquisition protocols, despite potential cognitive decline, simulating MRI matching across databases. These results aim to contribute meaningfully to the development of thoughtful, forward-looking policies in medical data sharing.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10043v1",
      "url": "https://arxiv.org/abs/2602.10043"
    },
    {
      "arxiv_id": "2602.10042",
      "title": "Fake-HR1: Rethinking reasoning of vision language model for synthetic image detection",
      "authors": [
        "Changjiang Jiang",
        "Xinkuan Sha",
        "Fengchang Yu",
        "Jingjing Liu",
        "Jian Liu",
        "Mingqi Fang",
        "Chenfeng Zhang",
        "Wei Lu"
      ],
      "abstract": "Recent studies have demonstrated that incorporating Chain-of-Thought (CoT) reasoning into the detection process can enhance a model's ability to detect synthetic images. However, excessively lengthy reasoning incurs substantial resource overhead, including token consumption and latency, which is particularly redundant when handling obviously generated forgeries. To address this issue, we propose Fake-HR1, a large-scale hybrid-reasoning model that, to the best of our knowledge, is the first to adaptively determine whether reasoning is necessary based on the characteristics of the generative detection task. To achieve this, we design a two-stage training framework: we first perform Hybrid Fine-Tuning (HFT) for cold-start initialization, followed by online reinforcement learning with Hybrid-Reasoning Grouped Policy Optimization (HGRPO) to implicitly learn when to select an appropriate reasoning mode. Experimental results show that Fake-HR1 adaptively performs reasoning across different types of queries, surpassing existing LLMs in both reasoning ability and generative detection performance, while significantly improving response efficiency.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.CV",
      "categories": [
        "cs.CV",
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10042v1",
      "url": "https://arxiv.org/abs/2602.10042"
    },
    {
      "arxiv_id": "2602.10031",
      "title": "Position: Message-passing and spectral GNNs are two sides of the same coin",
      "authors": [
        "Antonis Vasileiou",
        "Juan Cervino",
        "Pascal Frossard",
        "Charilaos I. Kanatsoulis",
        "Christopher Morris",
        "Michael T. Schaub",
        "Pierre Vandergheynst",
        "Zhiyang Wang",
        "Guy Wolf",
        "Ron Levie"
      ],
      "abstract": "Graph neural networks (GNNs) are commonly divided into message-passing neural networks (MPNNs) and spectral graph neural networks, reflecting two largely separate research traditions in machine learning and signal processing. This paper argues that this divide is mostly artificial, hindering progress in the field. We propose a viewpoint in which both MPNNs and spectral GNNs are understood as different parametrizations of permutation-equivariant operators acting on graph signals. From this perspective, many popular architectures are equivalent in expressive power, while genuine gaps arise only in specific regimes. We further argue that MPNNs and spectral GNNs offer complementary strengths. That is, MPNNs provide a natural language for discrete structure and expressivity analysis using tools from logic and graph isomorphism research, while the spectral perspective provides principled tools for understanding smoothing, bottlenecks, stability, and community structure. Overall, we posit that progress in graph learning will be accelerated by clearly understanding the key similarities and differences between these two types of GNNs, and by working towards unifying these perspectives within a common theoretical and conceptual framework rather than treating them as competing paradigms.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10031v1",
      "url": "https://arxiv.org/abs/2602.10031"
    },
    {
      "arxiv_id": "2602.10021",
      "title": "Decoupled Reasoning with Implicit Fact Tokens (DRIFT): A Dual-Model Framework for Efficient Long-Context Inference",
      "authors": [
        "Wenxuan Xie",
        "Yujia Wang",
        "Xin Tan",
        "Chaochao Lu",
        "Xia Hu",
        "Xuhong Wang"
      ],
      "abstract": "The integration of extensive, dynamic knowledge into Large Language Models (LLMs) remains a significant challenge due to the inherent entanglement of factual data and reasoning patterns. Existing solutions, ranging from non-parametric Retrieval-Augmented Generation (RAG) to parametric knowledge editing, are often constrained in practice by finite context windows, retriever noise, or the risk of catastrophic forgetting. In this paper, we propose DRIFT, a novel dual-model architecture designed to explicitly decouple knowledge extraction from the reasoning process. Unlike static prompt compression, DRIFT employs a lightweight knowledge model to dynamically compress document chunks into implicit fact tokens conditioned on the query. These dense representations are projected into the reasoning model's embedding space, replacing raw, redundant text while maintaining inference accuracy. Extensive experiments show that DRIFT significantly improves performance on long-context tasks, outperforming strong baselines among comparably sized models. Our approach provides a scalable and efficient paradigm for extending the effective context window and reasoning capabilities of LLMs. Our code is available at https://github.com/Lancelot-Xie/DRIFT.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.CL",
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10021v1",
      "url": "https://arxiv.org/abs/2602.10021"
    },
    {
      "arxiv_id": "2602.10019",
      "title": "ADORA: Training Reasoning Models with Dynamic Advantage Estimation on Reinforcement Learning",
      "authors": [
        "Qingnan Ren",
        "Shiting Huang",
        "Zhen Fang",
        "Zehui Chen",
        "Lin Chen",
        "Lijun Li",
        "Feng Zhao"
      ],
      "abstract": "Reinforcement learning has become a cornerstone technique for developing reasoning models in complex tasks, ranging from mathematical problem-solving to imaginary reasoning. The optimization of these models typically relies on policy gradient methods, whose efficacy hinges on the accurate estimation of an advantage function. However, prevailing methods typically employ static advantage estimation, a practice that leads to inefficient credit assignment by neglecting the dynamic utility of training samples over time. This limitation results in suboptimal policy updates, which in turn manifest as slower convergence rates and increased learning instability, as models fail to adapt to evolving sample utilities effectively. To address this problem, we introduce \\textbf{ADORA} (\\textbf{A}dvantage \\textbf{D}ynamics via \\textbf{O}nline \\textbf{R}ollout \\textbf{A}daptation), a novel framework for policy optimization. ADORA dynamically adjusts the advantage function's weighting by adaptively categorizing training data into temporarily advantageous and disadvantageous samples, based on their evolving utility during online model rollouts. This tailored data differentiation strategy allows ADORA to be seamlessly integrated into existing policy optimization algorithms without significant architectural modifications, enabling the policy to prioritize learning from more informative experiences and thereby achieve more efficient policy updates. Extensive evaluations across diverse model families and varying data scales demonstrate that ADORA is a robust and efficient framework. It significantly enhances long reasoning in both geometric and mathematical tasks, consistently achieving notable performance gains without requiring sensitive hyperparameter tuning.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.LG",
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10019v1",
      "url": "https://arxiv.org/abs/2602.10019"
    },
    {
      "arxiv_id": "2602.10016",
      "title": "Kunlun: Establishing Scaling Laws for Massive-Scale Recommendation Systems through Unified Architecture Design",
      "authors": [
        "Bojian Hou",
        "Xiaolong Liu",
        "Xiaoyi Liu",
        "Jiaqi Xu",
        "Yasmine Badr",
        "Mengyue Hang",
        "Sudhanshu Chanpuriya",
        "Junqing Zhou",
        "Yuhang Yang",
        "Han Xu",
        "Qiuling Suo",
        "Laming Chen",
        "Yuxi Hu",
        "Jiasheng Zhang",
        "Huaqing Xiong",
        "Yuzhen Huang",
        "Chao Chen",
        "Yue Dong",
        "Yi Yang",
        "Shuo Chang",
        "Xiaorui Gan",
        "Wenlin Chen",
        "Santanu Kolay",
        "Darren Liu",
        "Jade Nie",
        "Chunzhi Yang",
        "Jiyan Yang",
        "Huayu Li"
      ],
      "abstract": "Deriving predictable scaling laws that govern the relationship between model performance and computational investment is crucial for designing and allocating resources in massive-scale recommendation systems. While such laws are established for large language models, they remain challenging for recommendation systems, especially those processing both user history and context features. We identify poor scaling efficiency as the main barrier to predictable power-law scaling, stemming from inefficient modules with low Model FLOPs Utilization (MFU) and suboptimal resource allocation. We introduce Kunlun, a scalable architecture that systematically improves model efficiency and resource allocation. Our low-level optimizations include Generalized Dot-Product Attention (GDPA), Hierarchical Seed Pooling (HSP), and Sliding Window Attention. Our high-level innovations feature Computation Skip (CompSkip) and Event-level Personalization. These advances increase MFU from 17% to 37% on NVIDIA B200 GPUs and double scaling efficiency over state-of-the-art methods. Kunlun is now deployed in major Meta Ads models, delivering significant production impact.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.IR",
      "categories": [
        "cs.IR",
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10016v1",
      "url": "https://arxiv.org/abs/2602.10016"
    },
    {
      "arxiv_id": "2602.10015",
      "title": "RoboSubtaskNet: Temporal Sub-task Segmentation for Human-to-Robot Skill Transfer in Real-World Environments",
      "authors": [
        "Dharmendra Sharma",
        "Archit Sharma",
        "John Reberio",
        "Vaibhav Kesharwani",
        "Peeyush Thakur",
        "Narendra Kumar Dhar",
        "Laxmidhar Behera"
      ],
      "abstract": "Temporally locating and classifying fine-grained sub-task segments in long, untrimmed videos is crucial to safe human-robot collaboration. Unlike generic activity recognition, collaborative manipulation requires sub-task labels that are directly robot-executable. We present RoboSubtaskNet, a multi-stage human-to-robot sub-task segmentation framework that couples attention-enhanced I3D features (RGB plus optical flow) with a modified MS-TCN employing a Fibonacci dilation schedule to capture better short-horizon transitions such as reach-pick-place. The network is trained with a composite objective comprising cross-entropy and temporal regularizers (truncated MSE and a transition-aware term) to reduce over-segmentation and to encourage valid sub-task progressions. To close the gap between vision benchmarks and control, we introduce RoboSubtask, a dataset of healthcare and industrial demonstrations annotated at the sub-task level and designed for deterministic mapping to manipulator primitives. Empirically, RoboSubtaskNet outperforms MS-TCN and MS-TCN++ on GTEA and our RoboSubtask benchmark (boundary-sensitive and sequence metrics), while remaining competitive on the long-horizon Breakfast benchmark. Specifically, RoboSubtaskNet attains F1 @ 50 = 79.5%, Edit = 88.6%, Acc = 78.9% on GTEA; F1 @ 50 = 30.4%, Edit = 52.0%, Acc = 53.5% on Breakfast; and F1 @ 50 = 94.2%, Edit = 95.6%, Acc = 92.2% on RoboSubtask. We further validate the full perception-to-execution pipeline on a 7-DoF Kinova Gen3 manipulator, achieving reliable end-to-end behavior in physical trials (overall task success approx 91.25%). These results demonstrate a practical path from sub-task level video understanding to deployed robotic manipulation in real-world settings.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10015v1",
      "url": "https://arxiv.org/abs/2602.10015"
    },
    {
      "arxiv_id": "2602.10009",
      "title": "Discovering High Level Patterns from Simulation Traces",
      "authors": [
        "Sean Memery",
        "Kartic Subr"
      ],
      "abstract": "Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with tasks involving physics. The LM's capability for physical reasoning is learned from observational data, rather than being grounded in simulation. A common approach is to include simulation traces as context, but this suffers from poor scalability as simulation traces contain larger volumes of fine-grained numerical and semantic data. In this paper, we propose a natural language guided method to discover coarse-grained patterns (e.g., 'rigid-body collision', 'stable support', etc.) from detailed simulation logs. Specifically, we synthesize programs that operate on simulation logs and map them to a series of high level activated patterns. We show, through two physics benchmarks, that this annotated representation of the simulation log is more amenable to natural language reasoning about physical systems. We demonstrate how this method enables LMs to generate effective reward programs from goals specified in natural language, which may be used within the context of planning or supervised learning.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI",
        "cs.HC"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10009v1",
      "url": "https://arxiv.org/abs/2602.10009"
    },
    {
      "arxiv_id": "2602.10007",
      "title": "A Collaborative Safety Shield for Safe and Efficient CAV Lane Changes in Congested On-Ramp Merging",
      "authors": [
        "Bharathkumar Hegde",
        "Melanie Bouroche"
      ],
      "abstract": "Lane changing in dense traffic is a significant challenge for Connected and Autonomous Vehicles (CAVs). Existing lane change controllers primarily either ensure safety or collaboratively improve traffic efficiency, but do not consider these conflicting objectives together. To address this, we propose the Multi-Agent Safety Shield (MASS), designed using Control Barrier Functions (CBFs) to enable safe and collaborative lane changes. The MASS enables collaboration by capturing multi-agent interactions among CAVs through interaction topologies constructed as a graph using a simple algorithm. Further, a state-of-the-art Multi-Agent Reinforcement Learning (MARL) lane change controller is extended by integrating MASS to ensure safety and defining a customised reward function to prioritise efficiency improvements. As a result, we propose a lane change controller, known as MARL-MASS, and evaluate it in a congested on-ramp merging simulation. The results demonstrate that MASS enables collaborative lane changes with safety guarantees by strictly respecting the safety constraints. Moreover, the proposed custom reward function improves the stability of MARL policies trained with a safety shield. Overall, by encouraging the exploration of a collaborative lane change policy while respecting safety constraints, MARL-MASS effectively balances the trade-off between ensuring safety and improving traffic efficiency in congested traffic. The code for MARL-MASS is available with an open-source licence at https://github.com/hkbharath/MARL-MASS",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.RO",
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.MA",
        "eess.SY"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10007v1",
      "url": "https://arxiv.org/abs/2602.10007"
    },
    {
      "arxiv_id": "2602.10004",
      "title": "ESTAR: Early-Stopping Token-Aware Reasoning For Efficient Inference",
      "authors": [
        "Junda Wang",
        "Zhichao Yang",
        "Dongxu Zhang",
        "Sanjit Singh Batra",
        "Robert E. Tillman"
      ],
      "abstract": "Large reasoning models (LRMs) achieve state-of-the-art performance by generating long chains-of-thought, but often waste computation on redundant reasoning after the correct answer has already been reached. We introduce Early-Stopping for Token-Aware Reasoning (ESTAR), which detects and reduces such reasoning redundancy to improve efficiency without sacrificing accuracy. Our method combines (i) a trajectory-based classifier that identifies when reasoning can be safely stopped, (ii) supervised fine-tuning to teach LRMs to propose self-generated <stop> signals, and (iii) <stop>-aware reinforcement learning that truncates rollouts at self-generated stop points with compute-aware rewards. Experiments on four reasoning datasets show that ESTAR reduces reasoning length by about 3.7x (from 4,799 to 1,290) while preserving accuracy (74.9% vs. 74.2%), with strong cross-domain generalization. These results highlight early stopping as a simple yet powerful mechanism for improving reasoning efficiency in LRMs.",
      "published": "2026-02-10",
      "updated": "2026-02-10",
      "primary_category": "cs.AI",
      "categories": [
        "cs.AI"
      ],
      "doi": null,
      "journal_ref": null,
      "pdf_url": "https://arxiv.org/pdf/2602.10004v1",
      "url": "https://arxiv.org/abs/2602.10004"
    }
  ],
  "count": 25,
  "errors": []
}
